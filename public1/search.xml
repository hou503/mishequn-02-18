<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI a must for the new era</title>
    <url>/2020/09/05/AI%20a%20must%20for%20the%20new%20era/</url>
    <content><![CDATA[<p>In 2016, the birth of AlphaGo kicked off a new wave of artificial intelligence. After nearly three decades of silence, artificial intelligence has finally ushered in a new spring.</p>
<p>Popularize the basics of Artificial Intelligence and take the first step to understanding it. Artificial intelligence is complex, but not mysterious. It is built on basic mathematics with linear algebra and probability theory as its backbone, and it performs complex functions through the combination of simple models. In engineering terms, deep neural networks are often prohibitive with their Ganges-like parameters; in theory, however, their mathematics is much more interpretable. While it takes an extraordinary mind to do high-end, multi-million-dollar-a-year research, understanding the fundamentals of AI is far from an unattainable dream.</p>
<p>The early development of AI followed the path of the semiotic school of thought, but its narrow field of application quickly led it to silence after a brief period of glory. Learning from the lessons of the symbolist school, the connectionist school simulated human intelligence by engineering technical means to simulate the structure and function of the human brain’s nervous system. This line of thinking, to some extent, enables the function of the human brain’s image mind and has become the core technology of today’s artificial intelligence.</p>
<ol>
<li><p>Mathematical Fundamentals. Mathematical fundamentals contain the basic ideas and methods for dealing with intelligence problems, and are essential elements for understanding complex algorithms. In this module I will introduce the mathematical fundamentals essential to the study of artificial intelligence, including linear algebra, probability theory, and optimization methods.</p>
</li>
<li><p>Machine Learning. The role of machine learning is to learn algorithms from data and then to solve practical application problems, which is one of the core goals of artificial intelligence. In this module I will introduce the main methods in machine learning, including linear regression, decision trees, support vector machines, and clustering.</p>
</li>
<li><p>Artificial Neural Networks. As a branch of machine learning, neural networks bring cognitive science into machine learning to simulate the interaction of the biological nervous system to the real world, with good results. In this module I will introduce the basic concepts in neural networks, including multilayer neural networks, feedforward and backpropagation, and self-organizing neural networks.</p>
</li>
<li><p>Deep Learning. In short, deep learning is a neural network containing multiple intermediate layers, and the rise of deep learning has been driven by the explosion of data and soaring computing power. In this module I will introduce the concept and implementation of deep learning, including deep feedforward networks, regularization in deep learning, autoencoders, and more.</p>
</li>
<li><p>Neural Network Examples. A number of neural networks have been used in a variety of application scenarios within the framework of deep learning and have achieved remarkable results. In this module I will introduce several neural network examples, including convolutional neural networks, recurrent neural networks, and deep belief networks.</p>
</li>
<li><p>Artificial Intelligence Beyond Deep Learning. Deep learning has both advantages and limitations, and other directions are useful additions to AI research. In this module I will introduce several learning methods that are not related to deep learning, including Markov random fields, migration learning, and cluster intelligence.</p>
</li>
<li><p>Application Examples. In addition to performing repetitive labor in place of humans, AI has provided meaningful attempts at solving many practical problems. In this module I will describe the application of AI technology to several types of practical tasks, including computer vision, speech recognition, and conversational systems.</p>
</li>
</ol>
<p>At the turn of the century it was popular to say that people who did not understand computers, foreign languages, and driving skills would be the illiterate people of the 21st century. And in the near future, AI is likely to become the new test of illiteracy.</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Backward feed-back neural network algorithm (BP algorithm)</title>
    <url>/2020/11/21/Backward%20feed-back%20neural%20network%20algorithm/</url>
    <content><![CDATA[<h2 id="Principle-of-BP-algorithm"><a href="#Principle-of-BP-algorithm" class="headerlink" title="Principle of BP algorithm"></a>Principle of BP algorithm</h2><p>The BP algorithm consists of an input layer → hidden layer → output layer.</p>
<p><img src="http://56ssc.cc/2020/11/21/Backward feed-back neural network algorithm/WX20210104-083124.png" alt="WX20210104-083124.png"></p>
<p>Input-output layer: Here you can see that this layer can have multiple entries or only 1 entry.</p>
<p>For example, to make a prediction of the next period’s result based on the results of each welfare lottery.</p>
<p>Then the input layer should be the result of the previous period (N numbers). The output layer is this period (N numbers).</p>
<p>Hidden layer: The hidden layer can have N nodes or N levels.</p>
<p>Forward process.</p>
<p>The input layer is accumulated to each node of the hidden layer separately </p>
<p>Suppose the input is x and the node of the hidden layer is s</p>
<p>then</p>
<p>s1 = x1+x2+x3+x4+……</p>
<p>s2 = x1+x2+x3+x4+……</p>
<p>s3 = x1+x2+x3+x4+……</p>
<p>Then in order to be able to dynamically adjust the weights added to each node w1 w2 w3 ……</p>
<p>it becomes</p>
<p>s1 = w1<em>x1+w2</em>x2+w3<em>x3+w4</em>x4+……</p>
<p>s2 = w1<em>x1+w2</em>x2+w3<em>x3+w4</em>x4+……</p>
<p>s3 = w1<em>x1+w2</em>x2+w3<em>x3+w4</em>x4+……</p>
<p>Adding an offset value b by the way the mathematical formula becomes</p>
<p><img src="http://56ssc.cc/2020/11/21/Backward feed-back neural network algorithm/194179-20161121165054581-1673560744.png" alt="194179-20161121165054581-1673560744.png"></p>
<p>Sj also has to go through the transfer function f() to calculate the value of the hidden node</p>
<p>The f() transfer function corresponds to the process by which the</p>
<p>neuron accumulation of biocurrent reaches a certain level triggers the discharge. If the que value is not reached, the accumulated bioelectricity is lost.</p>
<p>As an example </p>
<p>A commonly used excitation function is the sigmoid function diagram as follows</p>
<p>Equation f= 1/(1+e^-x)</p>
<p><img src="http://56ssc.cc/2020/11/21/Backward feed-back neural network algorithm/194179-20161121170525425-1105984505.png" alt="194179-20161121170525425-1105984505.png"></p>
<p>The main significance of adding the sigmod function to the BP algorithm is to add a nonlinear function to solve the nonlinear problem.</p>
<p>The advantage of choosing sigmod is that it is convenient to find the derivative of sigmod = 1 - f </p>
<p>The derivative needs to be computed for the backward feedbacks</p>
<p>The process is the same for the hidden layer to the output layer.</p>
<p>But the focus of the BP algorithm is on the backward feedbacks.</p>
<p>Reverse.</p>
<p>After the forward transfer is finished we are able to get the result once.</p>
<p>This result can then be compared with the expected value, and the variance E is generally calculated.</p>
<p>Then this error can be passed backwards to the upper layer and used to adjust the weights w of the nodes in the upper layer. This process is repeated until the variance E is less than the desired minimum error. (Because it is impossible to achieve 0 error in reality, the program cannot be ended without setting the minimum desired error.)</p>
<p>As for the specific weight adjustment formula has a complete derivation process, too complex to discuss here.</p>
<p>Translated with <a href="http://www.DeepL.com/Translator" target="_blank" rel="noopener">www.DeepL.com/Translator</a> (free version)</p>
<p><img src="http://56ssc.cc/2020/11/21/Backward feed-back neural network algorithm/194179-20161121173615159-1847785388.png" alt="194179-20161121173615159-1847785388.png"></p>
<p>The core concept is to calculate the partial differentiation of the overall variance E on the weight variable Wj to derive whether the current weight should be increased or decreased, and the letter n in the above equation is the learning rate. It is the basic unit used here to adjust the weights at a time.</p>
<p>Partial differentiation.</p>
<p>For example, a formula with N variables y = aX +bY+cZ; where XYZ is a variable. abc is a constant.</p>
<p>A partial differential is the effect of a change in a single variable on the value of y. The other variables are treated as constants here for derivative purposes.</p>
<p>Then y should be =a ; (bY+CZ) is treated as a constant here to find the partial differential of x.</p>
<p>Summary.</p>
<p>Then one forward feedback + one reverse adjustment of the weights makes the global error decrease, and do it several more times until the global error meets the desired minimum error. This is one training completion.</p>
<p>Multiple training requires providing different input values.</p>
<p>Taking the above example again.</p>
<p>That is, I take the lottery results of period 1 as input values and the lottery results of period 2 as expected values to calculate the error, which is a complete training.</p>
<p>The second training is to take the second period as input and the third period as expected value.</p>
<p>And so on until there are no new training samples. The trained neural network can then be used to predict the outcome of any future lottery draw.</p>
<p>(Of course, since welfare lotteries are completely random and irregular, the prediction results are actually very bad.)</p>
<p>The BP algorithm is theoretically able to approximate all linear functions.</p>
<p>In layman’s terms, BP algorithms can be trained to predict results when you have enough historical data and don’t know the pattern.</p>
<p>But there must be a pattern to what you are trying to do (you just don’t know or it’s too complicated), and your training set must contain all “valid factors”.</p>
<p>I made up the term “valid factors”. Let’s say that 50% of your historical data affects the results, and the other 50% has nothing to do with the results. This is allowed by the neural algorithm.</p>
<p>But if that 50% of the data does not contain all the factors that lead to the outcome, then the results will vary greatly.</p>
<p>Let’s say the welfare lottery (here we go again) has a factor of air quality (hypothetical!) , for example the probability of a certain number appearing without haze is high.</p>
<p>However you do not include this factor in your training set, then the results are very inaccurate.</p>
<h2 id="Current-Problems"><a href="#Current-Problems" class="headerlink" title="Current Problems"></a>Current Problems</h2><p>There are two obvious problems with the current BP algorithm:</p>
<ol>
<li>easy to fall into local minima</li>
</ol>
<p>For example, the following figure.</p>
<p><img src="http://56ssc.cc/2020/11/21/Backward feed-back neural network algorithm/3.jpg" alt="3.jpg"></p>
<p>The whole BP algorithm is similar to solving for the minimum of this function.</p>
<p>But we see that this function has 2 minima, and in reality there can be very many minima.</p>
<p>When we adjust the weights from right to left, we reach the first trough. Then after continuing to adjust the weights we find that the error value becomes larger and the algorithm mistakenly thinks it has found the minimum solution. However, it does not get the global optimal solution.</p>
<p>This is called getting into a local minimum.</p>
<ol start="2">
<li>Overfitting problem<br>Overfitting the training set. Like I just gave an example that 50% of the training data is irrelevant to the outcome. So the best result of the algorithm is actually to ignore this 50% useless training set. But the problem is that we don’t know what the law is, and we don’t know which training set is the valid factor (if we knew, we wouldn’t have come up with a BP algorithm).</li>
</ol>
<p>So over-conforming the training set is also wrong.</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Chrome architecture</title>
    <url>/2020/10/02/Chrome%20architecture/</url>
    <content><![CDATA[<h2 id="Processes-and-threads"><a href="#Processes-and-threads" class="headerlink" title="Processes and threads"></a>Processes and threads</h2><p>However, before I introduce processes and threads, I need to explain what parallel processing is, because if you understand the concept of parallel processing, then understanding the relationship between processes and threads becomes much easier.</p>
<h2 id="What-is-parallel-processing"><a href="#What-is-parallel-processing" class="headerlink" title="What is parallel processing?"></a>What is parallel processing?</h2><p>Parallel processing in computers is the processing of multiple tasks at the same time, for example, we want to calculate the values of the following three expressions and display the results.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">A = <span class="number">1</span>+<span class="number">2</span></span><br><span class="line">B = <span class="number">20</span>/<span class="number">5</span></span><br><span class="line">C = <span class="number">7</span>*<span class="number">8</span></span><br></pre></td></tr></table></figure>

<p>In writing the code, we can break the process down into four tasks.</p>
<ul>
<li>Task 1 is to calculate A=1+2.</li>
<li>Task 2 is to calculate B=20/5.</li>
<li>Task 3 is to calculate C=7*8.</li>
<li>Task 4 is to display the result of the last calculation.<br>Normally the program can be handled in a single thread, that is, in four steps to execute each of these four tasks in order.</li>
</ul>
<p>What happens if it is multi-threaded? We just need to divide into “two steps”: the first step, use three threads to execute the first three tasks at the same time; the second step, and then execute the fourth display task.</p>
<p>In a comparative analysis, you will see that it takes four steps to execute in a single thread, but only two steps to execute in multiple threads. Therefore, using parallel processing can greatly improve performance.</p>
<h2 id="Threads-vs-Processes"><a href="#Threads-vs-Processes" class="headerlink" title="Threads vs. Processes"></a>Threads vs. Processes</h2><p>Multiple threads can handle tasks in parallel, but threads cannot stand alone, they are started and managed by processes. So what is a process again?</p>
<p>A process is a running instance of a program. To elaborate, when you start a program, the operating system creates a memory for the program to store code, running data, and a main thread to execute tasks.</p>
<p>Threads are dependent on processes, and the use of multi-threaded parallel processing in processes can improve computational efficiency.</p>
<p>In summary, the relationship between processes and threads has the following four characteristics. 1.</p>
<ol>
<li>an error in the execution of any thread in the process will cause the entire process to crash.</li>
</ol>
<p>We can simulate the following scenario.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">A = <span class="number">1</span>+<span class="number">2</span></span><br><span class="line">B = <span class="number">20</span>/<span class="number">5</span></span><br><span class="line">C = <span class="number">7</span>*<span class="number">8</span></span><br></pre></td></tr></table></figure>

<p>I modified the above three expressions slightly, and when calculating the value of B, I changed the denominator of the expression to 0. When the thread executes to B = 20/0, because the denominator is 0, the thread will execute an error, which will cause the whole process to crash, and of course the other two threads will execute with no result.</p>
<ol start="2">
<li>sharing data between threads in the process.</li>
</ol>
<p>Threads can read and write to the process’s public data.</p>
<ol start="3">
<li>When a process closes, the operating system will reclaim the memory occupied by the process.</li>
</ol>
<p>When a process exits, the operating system will reclaim all the resources requested by the process; even if any thread leaks memory due to improper operation, the memory will be properly reclaimed when the process exits.</p>
<p>For example, the previous Internet Explorer browser supported many plug-ins, and these plug-ins could easily lead to memory leaks, which means that as long as the browser is on, the memory usage may increase, but when the browser process is closed, all this memory will be reclaimed by the system.</p>
<ol start="4">
<li>Processes are isolated from each other’s content.</li>
</ol>
<p>Process isolation is a technique used to protect the processes in an operating system from interfering with each other, so that each process can only access the data that it occupies, thus avoiding the situation where process A writes data to process B. Process isolation is a technique used to protect the processes in an operating system from interfering with each other. It is because the data between processes is strictly isolated, so if a process crashes or hangs, it will not affect other processes. If you need to communicate data between processes, you need to use a mechanism for inter-process communication (IPC).</p>
<p>The era of the single-process browser<br>Once we understand processes and threads, let’s look at the architecture of a single-process browser together. As the name implies, a single-process browser is one in which all of the browser’s functional modules, including the network, plug-ins, JavaScript runtime, rendering engine, and pages, are running in a single process. </p>
<p>So many functional modules running in a single process is one of the main factors that make single-process browsers unstable, unwieldy and insecure. I’m going to analyze each of the reasons why these problems occur.</p>
<p>Problem 1: Instability<br>Early browsers needed plug-ins for powerful features such as web video and web games, but plug-ins were the most error-prone module and ran within the browser process, so an accidental crash of one plug-in could cause the entire browser to crash.</p>
<p>In addition to plugins, the rendering engine module is also unstable, and often complex JavaScript code can cause the rendering engine module to crash. As with plugins, a crash of the rendering engine can cause the entire browser to crash.</p>
<p>Problem 2: Sluggishness<br>As you can see from the above “Single Process Browser Architecture Diagram”, all the page rendering modules, JavaScript execution environment and plugins are running in the same thread, which means that only one module can be executed at a time.</p>
<p>For example, the following infinite loop script.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">freeze</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">	<span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">		<span class="built_in">console</span>.log(<span class="string">"freeze"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">freeze();</span><br></pre></td></tr></table></figure>

<p>What do you think would happen if you let this script run in a single-process browser page?</p>
<p>Since this script is infinite loop, when it executes, it takes over the entire thread, which results in other modules running in the thread not having a chance to be executed. Since all the pages in the browser are running in the thread, they don’t get a chance to execute their tasks, which causes the entire browser to become unresponsive and cluttered. I’ll go into more detail about this in a later module on the page’s event loop system.</p>
<p>In addition to the aforementioned scripts or plug-ins that can make a single-process browser stutter, memory leaks on a page are also a major cause of single-process slowdowns. Usually the kernel of the browser is very complex, running a more complex page and then close the page, there will be a situation where the memory can not be fully recycled, which leads to the problem that the longer you use, the higher the memory usage, the slower the browser will become.</p>
<p>Problem 3: Insecurity<br>This can still be explained in terms of both plugins and page scripts.</p>
<p>Plugins can be written in code such as C/C++, and they allow you to access any resource on your operating system, so when you run a plugin on a page, it means that the plugin can fully operate your computer. If it’s a malicious plugin, it can release viruses, steal your account password and cause security issues.</p>
<p>As for page scripts, they can gain system access through browser vulnerabilities, and these scripts can also do malicious things to your computer after gaining system access, which can also cause security issues.</p>
<p>These were the characteristics of browsers at the time, unstable, unwieldy, and insecure. It’s an unpleasant past that you may not have experienced, but imagine a scenario where you are opening multiple pages in your browser and suddenly one page crashes or becomes unresponsive, followed by the entire browser crashing or becoming unresponsive, and then you realize that the email page you wrote to your boss has disappeared with it, and you feel as devastated as the page?</p>
<p>the era of multi-process browsers<br>The good news is that modern browsers have solved these problems, how? That brings us to our “multi-process browser era”.</p>
<p>Let’s first look at how to solve the instability problem. Since the processes are isolated from each other, when a page or plugin crashes, it affects only the current page process or plugin process and does not affect the browser and other pages, which is a perfect solution to the problem that a page or plugin crash can cause the entire browser to crash, which is also known as instability.</p>
<p>Next, let’s take a look at how the unwieldy issue is resolved. Again, JavaScript runs in the rendering process, so even if JavaScript blocks the rendering process, it only affects the currently rendered page, not the browser and other pages, because the scripts for the other pages are running in their own rendering process. So when we run the above dead-loop script again in Chrome, only the current page will not respond.</p>
<p>The solution for memory leaks is even easier, because when you close a page, the entire rendering process is closed, and any memory used by that process is reclaimed by the system, thus easily solving the memory leak problem for browser pages.</p>
<p>Finally, let’s look at how the two security issues above are resolved. Chrome locks the plug-in and rendering processes inside the sandbox, so that even when the rendering process is running, it can’t write any data to your hard drive or read any data in sensitive locations such as your documents or desktop. If a malicious program executes inside a process or plug-in process, the malicious program will not be able to break out of the sandbox and gain access to the system.</p>
<p>Well, after analyzing the early days of Chrome, I’m sure you’ve already understood the need for a multi-process architecture for your browser.</p>
<h2 id="Current-multi-process-architecture"><a href="#Current-multi-process-architecture" class="headerlink" title="Current multi-process architecture"></a>Current multi-process architecture</h2><p>The latest Chrome includes: 1 main browser process, 1 GPU process, 1 NetWork process, multiple rendering processes, and multiple plug-in processes.<br>Let’s analyze the functions of each of these processes.</p>
<p>Browser process. It is responsible for displaying the interface, user interaction, child process management, and providing storage and other functions.<br>Rendering process. By default, Chrome creates a rendering process for each tab. For security reasons, the rendering processes are run in sandbox mode.<br>GPU Processes. Actually, when Chrome was first released, there was no GPU process. The GPU was originally intended to be used for 3D CSS, but then the web, and Chrome’s UI interface, opted for GPU rendering, making GPU’s a common browser requirement. Finally, Chrome also introduced GPU processes to its multi-process architecture.<br>Web processes. It used to run as a module inside the browser process, but only recently has it become a separate process.<br>Plugin process. It is mainly responsible for the running of the plug-in, because the plug-in is prone to crash, so it needs to be isolated by the plug-in process to ensure that the crash of the plug-in process will not affect the browser and the page.<br>At this point, you should now be able to answer the question mentioned at the beginning of the article: why are there 4 processes when only 1 page is open? This is because opening a page requires at least 1 network process, 1 browser process, 1 GPU process, and 1 rendering process, 4 in total, plus 1 plug-in process if the page is running a plugin.</p>
<p>However, there are two sides to every coin, and while the multi-process model improves browser stability, fluidity, and security, it also inevitably brings up some issues.</p>
<p>Higher resource usage. Because each process will contain a copy of the public infrastructure (e.g., the JavaScript runtime environment), this means that the browser will consume more memory resources.<br>More complex architecture. High coupling between browser modules and poor scalability can lead to an architecture that is already difficult to adapt to new demands.<br>For both of these issues, the Chrome team was looking for a resilient solution that would address both the high resource consumption and the complex architecture.</p>
<h2 id="Future-service-oriented-architecture"><a href="#Future-service-oriented-architecture" class="headerlink" title="Future service-oriented architecture"></a>Future service-oriented architecture</h2><p>To address these issues, in 2016, the official Chrome team designed the new Chrome architecture using the idea of a Services Oriented Architecture (SOA). This means that Chrome’s overall architecture will move in the direction of the “Services Oriented Architecture” used by modern operating systems, and the various modules will be reorganized into separate services, each of which can run in a separate process and access the services ( (Service) must use defined interfaces to communicate via IPC to build a more cohesive, loosely coupled, easy-to-maintain and scalable system that better achieves Chrome’s goals of simplicity, stability, speed, and security.</p>
]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>Browser</tag>
      </tags>
  </entry>
  <entry>
    <title>DBMS</title>
    <url>/2020/09/20/DBMS/</url>
    <content><![CDATA[<h2 id="What-is-the-difference-between-DB-DBS-and-DBMS"><a href="#What-is-the-difference-between-DB-DBS-and-DBMS" class="headerlink" title="What is the difference between DB, DBS and DBMS?"></a>What is the difference between DB, DBS and DBMS?</h2><p>When it comes to DBMS, there are a few concepts you need to understand.</p>
<p>A DBMS, database management system, can actually manage multiple databases, so you can understand that DBMS = multiple databases + management program.</p>
<p>A database is a collection of stored data and you can think of it as multiple data tables.</p>
<p>Database system. It’s a much larger concept that includes databases, database management systems, and DBAs, database managers.</p>
<p>It’s important to note here that although we sometimes call Oracle, MySQL, etc. databases, they should be called database management systems, or DBMS, to be precise.</p>
<p>A relational database is a database based on a relational model, and SQL is the query language for relational databases.</p>
<p>Compared to SQL, NoSQL refers to non-relational databases in general, including key-value databases, document databases, search engines and column stores on the list, as well as graph databases.</p>
<p>Key-Value databases store data by means of Key-Value keys, where Key and Value can be simple or complex objects. key as a unique identifier, the advantage is that it is faster to find, which is significantly better than a relational database, but also has the obvious disadvantage that it can not use conditional filtering as freely as a relational database. If you don’t know where to look for data, you have to traverse all the keys, which consumes a lot of computation. A typical use case for key-value databases is as a content cache. redis is the most popular key-value database.</p>
<p>Document databases are used to manage documents, which are the basic unit of information processed in a database.</p>
<p>Search engines are also an important application in database retrieval, and common full-text search engines include Elasticsearch, Splunk and Solr. although relational databases use indexes to improve retrieval efficiency, full-text indexes are less efficient.</p>
<p>Columnar database is relative to the row-type storage database, Oracle, MySQL, SQL Server and other databases are using row-type storage, and columnar database is to store data to the database in accordance with the column, the advantage of doing so is that you can greatly reduce the system’s I/O, suitable for distributed file system, the shortage is relatively limited in function.</p>
<p>Graph databases, which make use of a data structure such as a graph to store relationships between entities. The best example is the relationship between people in a social network, the data model is implemented mainly in nodes and edges and is characterized by the ability to solve complex relationship problems efficiently.</p>
<p>There are many categories of NoSQL, such as key-value, document, search engine, columnar storage, and graphical databases, all of which fall into the NoSQL camp. It is only by using the term NoSQL that these technologies can be encompassed. Even so, the SQL camp is much heavier in the DBMS rankings, with 4 of the top 5 most influential DBMSs being relational databases and 12 of the top 20 DBMSs being relational databases. So, mastering SQL is very necessary.</p>
<p>This is because SQL has been dominating the DBMS, so many people wondered if there was a database technology that could move away from SQL, so NoSQL was born, but as it evolved, it became more and more indispensable, and the DBMSs in the NoSQL camp all have SQL-like features.</p>
<p>NoSQL is a great complement to SQL, and it allows us to make better use of database technologies, such as fast reads and writes, that can be scaled more easily and inexpensively in the age of cloud computing.</p>
<p>Twelve of the top 20 DBMSs are in the SQL camp, with the top three DBMSs being in the SQL camp: Oracle, MySQL, and SQL Server.</p>
<p>In 1979, Oracle 2 was born, the first commercially available RDBMS, which was later sold to military customers. As the fame of Oracle software grew, the company changed its name to Oracle Corporation. in the 1990s, Oracle founder Ellison became the second richest man after Bill Gates, and it can be said that IBM created two empires, one with Microsoft, the dominant software industry, and the other with Oracle, the dominant enterprise software market. today, Oracle’s annual revenue is $40 billion, which is a testament to the value of commercial database software. Oracle’s annual revenue has reached $40 billion, which is enough to prove the value of commercial database software. We can also see from this point, if you choose a big track, it is necessary to commercialize as soon as possible to occupy large enterprise customers can create tremendous business value, but also enough to prove that a software enterprise does not need to rely on selling hardware can also make a lot of money.</p>
<p>MySQL is an open source database management system that was born in 1995, and because of its free and open source features, it was loved by developers and its user base grew rapidly, making it the No.1 open source database. Oracle bought it, and Oracle also had the management rights of MySQL, so Oracle became the absolute leader in the database field. From here we can also see that although MySQL is a free product, but the number of users is enough to prove the great user value. A product with great user value, even if it has no direct business value, will be valued by business giants as infrastructure.</p>
<p>However, at the same time that Oracle acquired MySQL, the creators of MySQL were concerned about the risk of MySQL being closed source, so they created a MySQL offshoot project, MariaDB, which is for the most part compatible with MySQL and adds many new features, such as support for more storage engines type. Many businesses have also switched from MySQL to MariaDB.</p>
<p>SQL Server is a commercial database developed by Microsoft and was born in 1989. In fact, Microsoft also introduced the Access database, which is a desktop database, with both background storage and front-end interface development functions, more lightweight, suitable for small-scale applications. Because the background storage space is limited, generally only 2G, Access has the advantage of being able to easily develop the interface in the foreground. SQL Server, on the other hand, is a large database, used for background storage and querying, and does not have the functions of interface development. For example, Oracle is more suitable for large multinational enterprises because they are not cost-sensitive, but have higher requirements for performance and security, while MySQL is more popular with many Internet companies, especially early-stage entrepreneurs. Company favorites.</p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>In 1974, the SEQUEL paper was published, in 1979 the first commercial relational database Oracle 2 was created, in 1995 the MySQL open source database was created, and today, NoSQL has evolved and the DBMS race around the SQL standard has never stopped. In this history, there are both SQL camp, and NoSQL camp, both commercial database software, and open source products, in different application scenarios, the same company may also have different DBMS layout.</p>
<p>If different DBMSs represent the interests of different companies, then as users we should pay more attention to the scenarios of these DBMSs. For example, Oracle, as the commercial database software with the highest market share, is suitable for large multinational enterprises, while for lightweight desktop databases, we can use Access. For free and open source products, we can use MySQL or MariaDB, and in the NoSQL camp, we also need to understand the difference between key-value, document, search engine, columnar database and graph database.</p>
]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>DBMS</tag>
      </tags>
  </entry>
  <entry>
    <title>Formal logic</title>
    <url>/2020/10/31/Formal%20logic/</url>
    <content><![CDATA[<p>The Dartmouth Conference in 1956 heralded the birth of artificial intelligence. In its infancy, the founding fathers, including future Turing Award winners such as John McCarthy, Herbert Simon, and Marvin Minsky, had a vision of how “a program capable of abstract thought could explain how synthetic matter could possess the mind of a human being”.</p>
<p>In layman’s terms, the ideal AI would be capable of learning, reasoning, and induction in the abstract, and would be far more versatile than the algorithms used to solve concrete problems like chess or Go.</p>
<p>An indispensable foundation for such an AI is formal logic. Early researchers of artificial intelligence believed that the basic unit of human cognition and thinking is symbols, and the cognitive process is the logical operation of symbols, so that human abstract logical thinking can be simulated by the operation of logic gates in the computer, and then realize the mechanization of human cognition.</p>
<p>In turn, formal logic is how intelligent behavior is described, and any system that can translate some physical patterns or symbols into other patterns or symbols has the potential to produce intelligent behavior, which is artificial intelligence.</p>
<p>Artificial intelligence is able to simulate intelligent behavior on the basis of having knowledge, but knowledge itself is also an abstract concept that needs to be represented in a way that a computer can understand.</p>
<p>In AI, common ways of representing knowledge include data structures and processing algorithms. Data structures are used to statically store the problem to be solved, the intermediate solution to the problem, the final solution to the problem, and the knowledge involved in the solution; processing algorithms are used to dynamically interact between existing problems and knowledge, and together they form a complete knowledge representation system.</p>
<p>In the study of artificial intelligence, it is a common method to realize knowledge representation by formal logic. Formal logic can be said to be all-encompassing, the simplest example of which is the three-stage theory proposed by the ancient Greek philosopher Aristotle and passed down to the present day, which consists of two premises and a conclusion.</p>
<ul>
<li>Science is constantly evolving.</li>
<li>Artificial intelligence as science.</li>
<li>So, artificial intelligence is constantly evolving.</li>
</ul>
<p>Aristotle’s contribution was not only to demonstrate the continuous development of artificial intelligence, but also to identify the formal process of deducing a conclusion on the basis of major and minor premises, a process that was completely free of content limitations. The resulting birth of symbolic reasoning has had a profound impact on the study of mathematical logic.</p>
<p>The main application in artificial intelligence is first-order predicate logic. Predicate logic is the most fundamental logical system and a fundamental part of formal logic. A special case of predicate logic is propositional logic. In predicate logic, a proposition is the basic unit of logical processing and can only be judged as true or false.</p>
<p>However, the limitation of propositional logic is that it cannot reflect the structure and logical features of the objects it describes, nor can it reflect the common features of different things. If the proposition “Old Li is Xiao Li’s father” is given alone, the relationship between Old Li and Xiao Li cannot be determined without context, and the truth of the proposition is meaningless.</p>
<p>In order to extend the representational power of formal logic, predicate logic was born on the basis of propositional logic. Predicate logic splits propositions into individual words, predicates, and quantifiers with the following meanings.</p>
<ul>
<li>Individual words are concrete or abstract objects of description that can exist independently.</li>
<li>Predicates are used to describe the properties and interrelationships of individual words.</li>
<li>Quantifiers are used to describe the quantitative relationships of individual words, including the holomorphic quantifiers ∀ and the presence quantifiers.</li>
</ul>
<p>All three elements can be used together to form a proposition. Different propositions can be linked by logical connectives to form compound propositions from simple propositions. There are five types of logical conjunctions, in descending order of priority.</p>
<ul>
<li>Negative (¬): compound proposition ¬P denies the truth value of proposition P, i.e. “not P”.</li>
<li>Hopefully (∧): Composite P∧Q denotes the sum of proposition P and Q, i.e. “P and Q”.</li>
<li>(∨): Complex P∨Q denotes the disjunction of proposition P or Q, i.e. “P or Q”.</li>
<li>Implicit (→): The compound proposition P→Q means that proposition P is a condition of proposition Q, i.e. “if P, then Q”.</li>
<li>Equivalence (↔): The compound proposition P↔Q means that proposition P and proposition Q are mutually implicit, i.e. “if P, then Q and if Q, then P”.</li>
</ul>
<p>Not only constant symbols appear in predicate logic, but also variable symbols are legal, and function symbols can also appear. The introduction of variables and functions expands the scope of predicate logic representation and enhances its universality. Predicate logic can be used to represent both factual knowledge such as concepts, states, and properties of things, as well as rule-based knowledge with definite causal relations between things.</p>
<p>Factual knowledge is usually represented by predicate formulas connected by parsimony and concatenation symbols, while rule-based knowledge is usually represented by predicate formulas connected by implication symbols. In a general sense, knowledge is expressed using predicate logic in the following steps.</p>
<ul>
<li>Define the predicate and the individual and determine the exact meaning of each predicate and each individual.</li>
<li>Assign specific values to the variables in each predicate, depending on the thing or concept to be expressed.</li>
<li>Connect the individual predicates with appropriate logical conjunctions based on the semantics of the knowledge to be expressed.</li>
</ul>
<p>After the processing of the above steps, the abstract sense of knowledge can be transformed into a computer can recognize and process the data structure.</p>
<p>For example, if you want to use predicate logic to represent the knowledge that “all natural numbers are integers greater than zero”, you first have to define all relations as corresponding predicates. The predicate N(x) means x is a natural number, P(x) means x is greater than zero, and I(x) means x is an integer, and then these predicates are connected semantically to obtain the predicate formula.<br><img src="http://56ssc.cc/2020/10/31/Formal logic/1.jpg" alt></p>
<p>The use of formal logic for knowledge representation is only a means to enable AI to automate reasoning, induction and deduction on the basis of knowledge in order to obtain new conclusions and new knowledge. At the present stage, the main difference between human intelligence and artificial intelligence is reflected in the reasoning ability.</p>
<p>Instead of plunging headlong into the vast amount of data to learn, human judgment is based on induction and deduction based on the characteristics of a small amount of data, with the general rules derived as the basis for judgment. A little interference in the digital image can make the neural network mistake a sea turtle for a rifle, but this trick cannot deceive human beings with thinking ability, and this is the reason why.</p>
<p>The basis for automatic reasoning by artificial intelligence is the generative system. Generative systems use generative rules to describe symbolic strings instead of operations to represent the process of reasoning and behavior as generative rules.</p>
<p>The generative rule is usually used to represent the causal relationship between things, and its basic form is P → Q. It can be used to represent the conclusion Q under the premise P, or the action Q under the condition P. Here P is called the rule antecedent, which can be a simple condition, or a complex condition formed by multiple simple conditions through a conjunction, while Q is called the rule postecedent.</p>
<p>When a set of generative rules cooperate and act in concert with each other, the conclusions generated by one generative rule can be used as known premises or conditions for another generative rule to further solve more complex problems, and such a system is a generative system.</p>
<p>Generative systems, in general, consist of three basic parts: the rule base, the fact base, and the reasoning machine.</p>
<p>The rule base is the core and foundation of the expert system, storing a collection of rules in the generative form, in which the completeness, accuracy and reasonableness of the rules will have a direct impact on the system performance.</p>
<p>The fact base stores input facts, intermediate results and final results. When the premise of a generating formula in the rule base can be matched with some known facts in the fact base, the generating formula is activated and its conclusion can be stored as a known fact in the fact base.</p>
<p>A reasoning machine, on the other hand, is a program used to control and coordinate the operation of the rule base with the fact base, and includes both a reasoning approach and a control strategy.</p>
<p>Specifically, there are three types of reasoning: forward reasoning, backward reasoning, and bidirectional reasoning.</p>
<p>Forward reasoning adopts a bottom-up approach, which starts from known facts and then derives the target conclusion by continuously selecting matching rule front pieces in the rule base to obtain matching rule back pieces.</p>
<p>Reverse reasoning uses a top-down approach, i.e., starting from the target hypothesis, one selects the matching rule antecedents by constantly matching the known facts with the rule antecedents in the rule base, and then backtracks to the known facts.</p>
<p>Two-way reasoning, on the other hand, is a more efficient way of combining forward and backward reasoning so that the reasoning proceeds in both top-down and bottom-up directions until it converges at some intermediate point.</p>
<p>Although automatic reasoning shows great ability in proving mathematical theorems, it is far from being intelligent in solving problems in daily life because of the lack of common sense. For humans, common sense is established through the process of socialization and growth.</p>
<p>However, computers are unable to reach understanding as humans grow up, so common sense, the prerequisite for intelligence, can only be instilled in a formalized manner on hard drives and in memory. This requires that the knowledge and beliefs of the average adult be explicitly expressed, flexibly organized and applied.</p>
<p>Without a clear expression of common sense and beliefs, AI is bound to get stuck in the mire of confusion, and obtaining generalized and adaptable intelligent behavior can only be a pipe dream.</p>
<p>The ultimate unavoidable and essential problem in talking about formal logic in AI lies in Gödel’s incompleteness theorem.</p>
<p>In 1900, the German mathematician David Hilbert asked the 23 most important mathematical questions of the 20th century at the International Congress of Mathematicians in Paris, the second of which was the non-contradiction of arithmetic axiomatic systems.</p>
<p>In 1931, the Austrian mathematician Kurt Gödel gave a negative answer to this question, the First Incompleteness Theorem.</p>
<p>In any formal system containing elementary number theory.<br>All must have an undecidable proposition.<br>By this theorem, Gödel proves that the Achilles’ heel of the axiomatized system is its inability to refer to itself, and that the following statement is a typical self-referential statement.</p>
<p>This mathematical proposition is not provable.<br>In the first place, this mathematical proposition discusses no other object than itself. The “present mathematical proposition” is a reference to the whole proposition. Secondly, the proposition gives a logical judgment that it cannot be proved.</p>
<p>Gödel proves that this proposition can neither be proven nor falsified, ruthlessly puncturing the dream that mathematical axiomatized systems have both consistency and completeness.</p>
<p>The impact of the incompleteness theorem on artificial intelligence lies in the understanding of the theoretical foundation that “the essence of cognition is computation”. From the perspective of “cognition is computation”, if a computer-based AI wants to achieve a thinking ability similar to that of human beings, it must also establish the concept of “self”, which will undoubtedly lead to the emergence of self-referencing and become a living target of the incompleteness theorem.</p>
<p>If a computer can create a symbol to represent itself in its calculations, then Gödel’s method of creating paradoxes can create illusions in the computer that are neither falsifiable nor verifiable.</p>
<p>In the shadow of Gödel’s incompleteness theorem, the “cognitive computationalist” research agenda based on Turing’s notion of computability has shown its great limitations. Today, the connectionist school, which relies on artificial neural networks, is flourishing, while the symbolist school, which relies on formal logic, is in decline.</p>
<p>But putting aside the differences in academic paths, what is the essential difference between human intelligence and artificial intelligence, and perhaps this is the greatest mystery left to us by the incompleteness theorem.</p>
<p>Today I share with you the essential foundations of formal logic necessary for artificial intelligence and the fundamentals of automatic reasoning using formal logic, the main points of which are as follows.</p>
<ul>
<li>If cognitive processes are defined as the logical manipulation of symbols, the basis of AI is formal logic.<br>(a) Predicate logic is the primary method of knowledge representation.</li>
<li>Predicate logic-based systems allow for artificial intelligence with automatic reasoning capabilities.</li>
<li>The incompleteness theorem challenges the fundamental notion of artificial intelligence that “cognition is essentially computational”.</li>
</ul>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>Database tuning</title>
    <url>/2020/10/17/Database%20tuning/</url>
    <content><![CDATA[<h2 id="Objectives-of-database-tuning"><a href="#Objectives-of-database-tuning" class="headerlink" title="Objectives of database tuning"></a>Objectives of database tuning</h2><p>Simply put, the goal of database tuning is to make the database run faster, which means faster response time and greater throughput.</p>
<p>However, with the increasing number of users and the increasing complexity of applications, it is difficult to define the goal of database tuning in terms of “faster” because users encounter different bottlenecks when accessing the server at different times, such as the Double Eleven sale, which brings massive concurrent access; and users who are conducting different business operations. The database transaction processing and SQL queries will be different at the time of tuning. Therefore, we also need to be more fine-grained to determine the goals of tuning.</p>
<p>How do we do this? In general, there are two ways to get feedback.</p>
<h3 id="Feedback-from-users"><a href="#Feedback-from-users" class="headerlink" title="Feedback from users"></a>Feedback from users</h3><p>Users are the ones we serve, so their feedback is the most direct. While they don’t make direct technical suggestions, some problems are often the first thing users notice. We need to take user feedback seriously and find issues that are relevant to the data.</p>
<h3 id="Log-analysis"><a href="#Log-analysis" class="headerlink" title="Log analysis"></a>Log analysis</h3><p>We can identify anomalies by looking at database logs and operating system logs, etc., and use them to locate problems encountered.</p>
<p>In addition to these specific feedbacks, we can also get an overall view of the server and database performance by monitoring the operational status.</p>
<h3 id="Server-Resource-Usage-Monitoring"><a href="#Server-Resource-Usage-Monitoring" class="headerlink" title="Server Resource Usage Monitoring"></a>Server Resource Usage Monitoring</h3><p>By monitoring the server’s CPU, memory, I/O and other usage, you can understand the server’s performance in real time and compare it with the historical situation.</p>
<h3 id="Database-Internal-Status-Monitoring"><a href="#Database-Internal-Status-Monitoring" class="headerlink" title="Database Internal Status Monitoring"></a>Database Internal Status Monitoring</h3><p>In database monitoring, active session monitoring is an important indicator. With it, you can see if the database is currently very busy, if there are SQL stacks, etc. In addition to active session monitoring, we can also monitor transactions, lock waits, etc. This helps us to have a more comprehensive view of the database running.</p>
<p>In addition to active session monitoring, we can also monitor transactions, lock waits, etc., which can help us get a more complete picture of the database’s performance.</p>
<h2 id="What-are-the-dimensions-that-can-be-selected-for-tuning-the-database"><a href="#What-are-the-dimensions-that-can-be-selected-for-tuning-the-database" class="headerlink" title="What are the dimensions that can be selected for tuning the database?"></a>What are the dimensions that can be selected for tuning the database?</h2><p>What we need to tune is the entire database management system, including not only SQL queries, but also database deployment configuration, architecture, etc. From this perspective, we think about more than just SQL optimization. From this point of view, we are not limited to the dimension of SQL optimization.</p>
<p>It sounds complicated, but we can actually go through it step by step with the following steps.</p>
<h3 id="Step-1-Select-the-right-DBMS"><a href="#Step-1-Select-the-right-DBMS" class="headerlink" title="Step 1: Select the right DBMS"></a>Step 1: Select the right DBMS</h3><p>We’ve talked about the SQL camp and the NoSQL camp before. In RDBMS, the common ones are Oracle, SQL Server and MySQL. If you have high requirements for transactional processing and security, you can choose commercial database products. These databases are strong in transaction processing and query performance, such as using SQL Server, then a single table to store hundreds of millions of data is no problem. If the data tables are well-designed, the query efficiency is not bad even if you don’t use a library-by-table approach.</p>
<p>In addition, you can also use the open-source MySQL for storage, which, as we have mentioned before, has many storage engines to choose from, including InnoDB for transactional processing and MyISAM for non-transactional processing.</p>
<p>The NoSQL camp includes key-value databases, document-based databases, search engines, columnar stores, and graph databases. These databases have different advantages and disadvantages and different usage scenarios. For example, columnar storage databases can greatly reduce system I/O and are suitable for distributed file systems and OLAP, but if data needs to be frequently added and deleted, columnar storage is not suitable. I’ve already explained the reasons in the Q&amp;A section, so I won’t repeat them here.</p>
<p>The choice of DBMS affects the whole design process, so the first step is to choose the right DBMS, if you have already decided on a DBMS, then this step can be skipped, but sometimes we have to choose according to the business needs.</p>
<h3 id="Step-2-Optimize-table-design"><a href="#Step-2-Optimize-table-design" class="headerlink" title="Step 2: Optimize table design"></a>Step 2: Optimize table design</h3><p>After selecting the DBMS, we need to do the table design.In RDBMS, each object can be defined as a table, and the relationship between the tables represents the relationship between the objects. If we are using MySQL, we can also choose a different storage engine depending on the usage requirements of different tables. In addition to this, there are some principles of optimization that can be followed.</p>
<p>The table structure should follow the principles of the third paradigm as much as possible. This makes the data structure clearer and more standardized, reduces the number of redundant fields, and also reduces the occurrence of exceptions when updating, inserting, and deleting data.<br>If there are a lot of analytical queries, especially if you need to perform multi-table lookups, you can use the inverse paradigm for optimization. The inverse paradigm uses a space-for-time approach to improve the efficiency of the query by adding redundant fields.<br>The choice of the data type of the table field is related to the efficiency of the query and the size of the storage space. In general, if the field can be of numeric type do not use character type; character length should be designed to be as short as possible. For the character type, when determining the character length is fixed, you can use the CHAR type; when the length is not fixed, usually using the VARCHAR type.<br>The structure of the data table design is very basic, but also very critical. Good table structure can still work in the case of business development and user volume increase, bad table structure design will make the data table becomes very bloated, query efficiency will be reduced.</p>
<h3 id="Step-3-Optimize-logical-queries"><a href="#Step-3-Optimize-logical-queries" class="headerlink" title="Step 3: Optimize logical queries"></a>Step 3: Optimize logical queries</h3><p>Once we have created the data table, we can then perform the add/drop operations on the data table. The first thing we need to consider at this point is logical query optimization, what is logical query optimization?</p>
<p>SQL query optimization can be divided into logical query optimization and physical query optimization. Logical query optimization is the process of making SQL execution more efficient by changing the contents of SQL statements, using an equivalent transformation of SQL statements to rewrite the query. The mathematical basis for query rewriting is relational algebra.</p>
<p>Query rewriting for SQL includes subquery optimization, equivalence predicate rewriting, view rewriting, conditional simplification, join elimination, and nested join elimination.</p>
<p>For example, when we explain EXISTS subqueries and IN subqueries, we will select the appropriate subqueries based on the principle of small table driving large tables. In the WHERE subquery, we will try to avoid functional operations on fields, they can invalidate the index of the field.</p>
<h3 id="Step-4-Optimize-physical-queries"><a href="#Step-4-Optimize-physical-queries" class="headerlink" title="Step 4: Optimize physical queries"></a>Step 4: Optimize physical queries</h3><p>Physical query optimization is the process of turning the contents of logical queries into physical operators that can be executed in preparation for the subsequent execution of the executor. It is all about efficiently creating indexes and using these indexes to do various optimizations.</p>
<p>But you should know that indexes are not everything and we need to create them according to the actual situation. So what are the situations that need to be considered?</p>
<p>If the data has a high degree of repetition, you do not need to create an index. Usually, if the repetition level is more than 10%, you may not create an index for this field.<br>Be aware of how the position of the index column affects the use of the index. For example, if we calculate an expression for an index field in the WHERE clause, it will cause the index of this field to be invalid.<br>Be aware of the effect of joint indexes on index usage. When we create a union index, we create indexes for multiple fields, and the order of the indexes is important. For example, if we create indexes for fields x, y, z, there will be a difference in the order of (x,y,z) or (z,y,x) at execution time.<br>Be aware of the effect of multiple indexes on index usage. The more indexes you have, the better, because each index requires storage space, and more indexes means more storage space is required. In addition, too many indexes can cause the optimizer to increase the computation time to filter out the indexes, which can affect the efficiency of the evaluation.<br>After equivalence transforming the SQL statement, the query optimizer also needs to determine the access path based on the index and data condition of the data tables, which determines the resources consumed when executing SQL. different data tables are required to be queried during SQL queries, so the physical query optimization phase also needs to determine the paths used by these queries, in the following cases.</p>
<p>Single table scan: for a single table scan, we can scan all the data in the whole table or partial scan.<br>Joining of two tables: common joining methods include nested loop joins, HASH joins and merge joins.<br>Joining multiple tables: when joining multiple data tables, the order is important because the query efficiency and search space will be different for different connection paths. When we are making multi-table joins, the search space may reach very high data levels, and the huge search space will obviously consume more resources, so we need to adjust the search space to an acceptable range by adjusting the order of the joins.<br>Physical query optimization is a technique that uses physical optimization after determining the logical query optimization to estimate the various possible access paths by computing the cost model to find the least costly of the execution methods as the execution plan. In this section, we need to master the focus on the creation and use of indexes.</p>
<h3 id="Step-5-Use-Redis-or-Memcached-as-a-cache"><a href="#Step-5-Use-Redis-or-Memcached-as-a-cache" class="headerlink" title="Step 5: Use Redis or Memcached as a cache"></a>Step 5: Use Redis or Memcached as a cache</h3><p>In addition to optimizing the SQL itself, we can also hire outside help to improve the efficiency of the queries.</p>
<p>Since the data is stored in the database, we need to take out the data from the database layer and put it into memory for business logic operations, and when the number of users increases, if we query the data frequently, it will consume a lot of resources in the database. If we put the frequently used data directly into the memory, we can improve the efficiency of the query greatly.</p>
<p>A key-value store database can help us solve this problem.</p>
<p>Commonly used key-value storage databases are Redis and Memcached, both of which can store data in memory.</p>
<p>In terms of reliability, Redis supports persistence, which allows us to keep our data on the hard drive, but this can also be quite performance intensive. Memcached, on the other hand, is only in-memory storage and does not support persistence.</p>
<p>It supports not only key-value data, but also List, Set, Hash and other data structures. We can use Redis when we have persistence or more advanced data processing needs, or Memcached for simple key-value storage.</p>
<p>Usually, we can consider in-memory databases for scenarios with high query response requirements (short response time, high throughput). Traditional RDBMSs store data on the hard drive, while in-memory databases are stored in memory and are much faster to query. Using different tools, however, also increases the cost of use for developers.</p>
<h3 id="Step-6-library-level-optimization"><a href="#Step-6-library-level-optimization" class="headerlink" title="Step 6, library-level optimization"></a>Step 6, library-level optimization</h3><p>Library level optimization is an optimization strategy that stands on the dimension of the database, such as controlling the number of data tables in a library. Alternatively, we can use a master-slave architecture to optimize our read and write strategies.</p>
<p>If the read and write business are very large, and they are all operating in the same database server, then the database performance will appear bottleneck, in order to improve the system performance and optimize the user experience, we can use the read and write separation to reduce the load of the master database, such as using the master database (master) to complete the write operation, use the database (slave) to complete the read operation.</p>
<p>In addition to this, we can also sub-database sub-tables. When the amount of data reaches billions, sometimes we need to cut a database into multiple copies and put them on different database servers to reduce the access pressure to a single database server. If you are using MySQL, you can use the partitioned table feature that comes with MySQL, of course you can also consider doing vertical and horizontal slicing yourself.</p>
<p>When to do a vertical slice and when to do a horizontal slice?</p>
<p>If there are too many data tables in the database, you can use vertical splitting to deploy the associated data tables on a single database.</p>
<p>If there are too many columns in the data table, you can use vertical slicing to split the data table into multiple sheets and put the columns that are often used together into the same table.</p>
<p>If the data table reaches billions or more, you can consider horizontal slicing, splitting the large data table into different sub-tables, keeping the same table structure for each table. For example, you can split the data by year and put the data from different years into different data tables. 2017, 2018, and 2019 data can be put into three separate data tables.</p>
<p>With vertical splitting, you are splitting a single data table into multiple tables, and with horizontal splitting, you are splitting a single data-heavy table into different smaller tables according to a certain attribute dimension.</p>
<p>However, it is important to note that splitting increases the cost of maintenance and usage while improving database performance.</p>
<p>How do we think about and analyze this database tuning thing?<br>Before we do anything, we need to identify our goals. In database tuning, our goals are faster response times and greater throughput. Using macro monitoring tools and micro log analysis can help us quickly find ideas and ways of tuning.</p>
<p>While everyone’s situation is different, we also need to have a holistic view of the database tuning thing. When thinking about database tuning, there are three dimensions to consider.</p>
<p>First, choice is more important than effort.</p>
<p>Before you do SQL tuning, you can choose how the DBMS and data tables are designed. As you can see, the different DBMS directly determines how the subsequent operations will be done, and the way the data tables are designed directly affects the subsequent SQL query statements.</p>
<p>In addition, you can divide SQL query optimization into two parts, logical query optimization and physical query optimization.</p>
<p>Although there are many techniques for SQL query optimization, the general direction can be completely divided into two major parts, logical query optimization and physical query optimization. Logical Query Optimization is the process of improving the efficiency of a query through SQL Equivalent Transformation, which to be blunt, means that the query may be written in a different way and executed more efficiently. Physical query optimization, on the other hand, is done through techniques such as indexes and table joins, where the focus is on mastering the use of indexes.</p>
<p>Finally, we can enhance the performance of the database with external help.</p>
<p>A single database will always encounter various limitations, so it is better to take the strengths and make use of external assistance.</p>
<p>Also, by slicing the database vertically or horizontally, we can break through the access limitations of a single database or data table and improve the performance of the query.</p>
]]></content>
      <categories>
        <category>Database tuning</category>
      </categories>
      <tags>
        <tag>Database tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>From Input URL to Page Presentation</title>
    <url>/2020/10/31/From%20Input%20URL%20to%20Page%20Presentation/</url>
    <content><![CDATA[<ol>
<li>User input<br>When the user enters a query keyword in the address bar, the address bar determines whether the keyword is the search content or the requested URL.</li>
</ol>
<p>If it is a search, the address bar will use the browser’s default search engine to compose a new URL with the search keyword.<br>If the content matches the URL rules, such as time.geekbang.org, then the address bar will add a protocol to the content to make it a full URL, such as <a href="https://time.geekbang.org" target="_blank" rel="noopener">https://time.geekbang.org</a>, according to the rules.<br>When the user enters the keyword and types “Enter”, the browser enters the following state.</p>
<p><img src="http://56ssc.cc/2020/10/31/From Input URL to Page Presentation/1.jpg" alt></p>
<p>As you can see in the figure, the icon on the tab enters the loading state when the browser first starts to load an address. But at this time, the page in the figure still shows the content of the previously opened page, and is not immediately replaced with a geek time page. This is because you need to wait for the document submission phase before the page content is replaced.</p>
<ol start="2">
<li>the URL request process<br>Next, the page resource request process takes place. At this point, the browser process sends the URL request to the web process via inter-process communication, and the web process receives the URL request and initiates the actual URL request process here. What does that process look like?</li>
</ol>
<p>First, the web process looks for whether the resource is cached in the local cache. If there is a cached resource, it returns the resource directly to the browser process; if the resource is not found in the cache, it goes directly to the web request process. The first step before this request is to perform DNS parsing to get the IP address of the server that is requesting the domain name. If the requesting protocol is HTTPS, then a TLS connection is also established.</p>
<p>The next step is to establish a TCP connection to the server using the IP address. After the connection is established, the browser end constructs the request line, request header, and other information, appends data such as cookies related to the domain name to the request header, and then sends the constructed request information to the server.</p>
<p>After receiving the request information, the server generates response data based on the request information and sends it to the web process. Once the network process has received the response line and response header, it begins to parse the response header.</p>
<p>(1) Redirect.</p>
<p>After receiving the response header from the server, the web process starts to parse the response header, if the returned status code is 301 or 302, the server needs the browser to redirect to another URL, the web process reads the redirected address from the Location field of the response header, and then initiates a new HTTP or HTTPS message. request, and everything starts all over again.</p>
<p>For example, we enter the following command in the terminal.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">curl -I http:<span class="comment">//time.geekbang.org/</span></span><br></pre></td></tr></table></figure>

<p>The command curl -I + URL is to receive information about the response header returned by the server. After executing the command, we see the following response header information returned by the server.</p>
<p><img src="http://56ssc.cc/2020/10/31/From Input URL to Page Presentation/2.jpg" alt></p>
<p>As you can see in the diagram, the Geek Time Server converts all HTTP requests into HTTPS requests by redirecting them. This means that when you make a request to the Geek Time server using HTTP, the server will return a response header containing a 301 or 302 status code and fill the Location field of the response header with the HTTPS address, which tells the browser to redirect to the new address.</p>
<p>Now let’s make another request for Geek Time using the HTTPS protocol to see what the server’s response header information looks like.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">curl -I http:<span class="comment">//time.geekbang.org/</span></span><br></pre></td></tr></table></figure>

<p><img src="http://56ssc.cc/2020/10/31/From Input URL to Page Presentation/3.jpg" alt></p>
<p>As you can see from the image, the server returned a response header with a status code of 200, which is telling the browser that everything is fine and it’s ready to move on to the request.</p>
<p>Okay, that’s the description of the redirect content. Now you should understand that during navigation, if the server response line status code contains a 301, 302 type of jump information, the browser will jump to the new address to continue navigation; if the response line is 200, then it means that the browser can continue to process the request.</p>
<p>(2) Response data type processing</p>
<p>After processing the bounce information, we continue with the analysis of the navigation flow. the data type of the URL request, sometimes a download type, sometimes a normal HTML page, so how does the browser distinguish between them?</p>
<p>The answer is Content-Type.Content-Type is a very important field in the HTTP header, it tells the browser server what type of response body data is being returned, and then the browser will decide how to display the content of the response body based on the value of the Content-Type.</p>
<p>Let’s take the example of Geek Time and see what the Content-Type value returned by the Geek Time website is. Enter the following command in the terminal.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">curl -I http:<span class="comment">//time.geekbang.org/</span></span><br></pre></td></tr></table></figure>

<p><img src="http://56ssc.cc/2020/10/31/From Input URL to Page Presentation/4.jpg" alt></p>
<p>As you can see in the figure, the value of the Content-type field in the response header is text/html, which is what tells the browser that the data returned by the server is in HTML format.</p>
<p>Next, let’s use curl to request the address of the Geek Time installation package, as follows.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">curl -I https:<span class="comment">//res001.geekbang.org/apps/geektime/android/2.3.1/official/geektime_2.3.1_20190527-2136_offical.apk</span></span><br></pre></td></tr></table></figure>

<p><img src="http://56ssc.cc/2020/10/31/From Input URL to Page Presentation/5.jpg" alt></p>
<p>From the response header information returned, the value of its Content-Type is application/octet-stream, which shows that the data is of byte stream type, and usually the browser will process the request according to the download type.</p>
<p>It should be noted that if the server does not configure the Content-Type correctly, such as configuring the text/html type to application/octet-stream, the browser may misinterpret the content of the file, for example, it may turn a page that was intended for display into a download file.</p>
<p>Therefore, the subsequent processing flow is also very different for different Content-Types. If the value of the Content-Type field is determined by the browser to be a download type, the request is submitted to the browser’s download manager, and the navigation process for the URL request ends there. In the case of HTML, however, the browser will continue the navigation process. Since Chrome’s page rendering runs within the rendering process, the next step is to prepare the rendering process.</p>
<ol start="3">
<li>Preparing the rendering process<br>By default, Chrome assigns a rendering process to each page, which means that a new rendering process is created for each new page that is opened. However, there are some exceptions, and in some cases the browser will have multiple pages running directly in the same rendering process.</li>
</ol>
<p>And when do multiple pages run in a single rendering process at the same time?</p>
<p>To solve this problem, we need to understand what a same-site is. Specifically, we define “same-site” as the root domain name (e.g., geekbang.org) plus a protocol (e.g., https:// or http://) that includes all the sub-domains and different ports under that root domain name, such as the following three.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https://time.geekbang.org</span><br><span class="line">https://www.geekbang.org</span><br><span class="line">https://www.geekbang.org:8080</span><br></pre></td></tr></table></figure>

<p>They all belong to the same site because their protocol is HTTPS and the root domain is geekbang.org.</p>
<p>Chrome’s default policy is to have one rendering process per tag. However, if a new page is opened from a page that belongs to the same site as the current page, the new page will multiplex the rendering process of the parent page. Officially, this default policy is called process-per-site-instance.</p>
<p>To summarize, the rendering process strategy used to open a new page is.</p>
<p>A separate rendering process is normally used to open new pages.<br>If you open page B from page A, and both A and B belong to the same site, then page B multiplexes the rendering process from page A. In other cases, the browser process creates a new rendering process for B. The rendering process is not ready yet, because the document data is not yet submitted to the rendering process.<br>When the rendering process is ready, you can’t immediately enter the document parsing state, because the document data is still in the network process, and has not been submitted to the rendering process, so the next step is to submit the document.</p>
<ol start="4">
<li>Submit document<br>To be clear, the term “document” refers to the data in the response body of a URL request.</li>
</ol>
<p>The “submit document” message is sent by the browser process. The rendering process receives the “submit document” message and establishes a “pipeline” with the web process to transfer the data.<br>The rendering process will return a “Submit Confirmation” message to the browser process when the document data is transmitted.<br>When the browser process receives the “Submit Confirmation” message, it updates the browser interface state, including the security state, the URL of the address bar, the forward and backward history, and the Web page.</p>
<ol start="5">
<li>Rendering phase<br>Once the document has been submitted, the rendering process starts the page parsing and sub-resource loading. All you need to know is that once the page is generated, the rendering process sends a message to the browser process, which receives the message and stops the loading animation on the tab icon.</li>
</ol>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>The server can control the browser’s behavior based on response headers, such as jumping and network data type determination.<br>Chrome defaults to one rendering process per tab, but if two pages belong to the same site, then the two tabs will use the same rendering process.<br>The browser’s navigation process covers all the intermediate stages from the time the user initiates a request to the time the document is submitted to the rendering process.<br>If you understand the navigation process, you will be able to link the entire page display process together, which is the key to understanding how the browser works.</p>
]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>Browser</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP</title>
    <url>/2020/10/25/HTTP/</url>
    <content><![CDATA[<h2 id="HTTP-request-flow-on-the-browser-side"><a href="#HTTP-request-flow-on-the-browser-side" class="headerlink" title="HTTP request flow on the browser side"></a>HTTP request flow on the browser side</h2><ol>
<li>Constructing the request</li>
</ol>
<p>First, the browser builds the request line information, and when it is built, the browser is ready to initiate a network request.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET /index.html HTTP1.1</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Find cache<br>Before actually initiating a web request, the browser looks in the browser cache to see if the file to be requested is available. The browser cache is a technique for saving a copy of a resource locally for direct use in the next request.</li>
</ol>
<p>When the browser finds that a copy of the requested resource is already in the browser cache, it intercepts the request, returns a copy of that resource, and simply ends the request without going to the source server to re-download it. The benefits of this are.</p>
<p>Relieves server-side stress and improves performance (less time spent acquiring resources).<br>For websites, caching is an important part of achieving fast resource loading.<br>Of course, if the cache lookup fails, you’ll be in the web request process.</p>
<ol start="3">
<li>Prepare the IP address and port<br>But, not so fast, before we can understand web requests, we need to look at the relationship between HTTP and TCP. Since the browser uses HTTP as the application layer protocol to encapsulate the text of the request and TCP/IP as the transport layer to send it to the network, the browser needs to establish a connection to the server via TCP before the HTTP work can begin. This means that the content of HTTP is implemented through the data transfer phase of TCP, and you can better understand the relationship between the two by looking at the diagram below.</li>
</ol>
<p><img src="http://56ssc.cc/2020/10/25/HTTP/1.jpg" alt></p>
<p>The first step is to request DNS to return the IP address of the domain name, but the browser also provides DNS data caching service, if a domain name has already been resolved, the browser will cache the resolution results for the next query, this will also reduce a network request.</p>
<p>Once you have the IP address, the next step is to get the port number. Typically, the HTTP protocol defaults to port 80 if the URL doesn’t specify a port number.</p>
<ol start="4">
<li>Waiting for the TCP queue<br>Chrome has a mechanism that allows up to 6 TCP connections to be established on the same domain at the same time, so if 10 requests occur on the same domain at the same time, 4 of them will be queued until the ongoing request is completed.</li>
</ol>
<p>Of course, if the current number of requests is less than 6, it will go straight to the next step to establish a TCP connection.</p>
<ol start="5">
<li>Establishing a TCP connection<br>After waiting in line, you can finally shake hands with the server happily, and the browser establishes a connection to the server over TCP before the HTTP work begins.</li>
<li>Sending HTTP requests<br>Once the TCP connection is established, the browser can communicate with the server. It is during this communication that the data in the HTTP is transferred.</li>
</ol>
<p>You can see how the browser sends the requested information to the server in the figure below.</p>
<p><img src="http://56ssc.cc/2020/10/25/HTTP/2.jpg" alt><br>First the browser sends a request line to the server, which includes the request method, the request URI (Uniform Resource Identifier), and the HTTP version protocol.</p>
<p>Sending a request line tells the server what resource the browser needs, and the most common request method is Get, for example, by typing the time.geekbang.org domain name directly into the browser’s address bar, which tells the server to Get its home page resource.</p>
<p>Another common request method is POST, which is used to send some data to the server, such as logging into a website, which requires sending user information to the server via the POST method. If the POST method is used, then the browser also has to prepare data to send to the server, and the data prepared here is sent through the request body.</p>
<p>After the browser sends the request line command, it also sends some other information in the form of a request header, which tells the server some basic information about the browser. This includes, for example, information about the operating system used by the browser, the kernel of the browser, the domain name of the current request, the cookie information of the browser, and so on.</p>
<h2 id="Server-Side-Processes-HTTP-Request-Flow"><a href="#Server-Side-Processes-HTTP-Request-Flow" class="headerlink" title="Server Side Processes HTTP Request Flow"></a>Server Side Processes HTTP Request Flow</h2><ol>
<li>Returning requests<br>Once the server has finished processing the request, it is ready to return the data to the browser. You can view the returned request data through the utility curl by typing the following command on the command line.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -i  https://bing.com/</span><br></pre></td></tr></table></figure>

<p>Note that the -i is added to return the response line, response header, and response body data, which returns the results shown below.</p>
<p><img src="http://56ssc.cc/2020/10/25/HTTP/2.jpg" alt><br>First the server will return the response line, including the protocol version and status code.</p>
<p>But not all requests can be processed by the server, so what about some of the messages that cannot be processed or that are processed incorrectly? The server tells the browser the result of its processing by the status code of the request line, such as.</p>
<p>The most commonly used status code is 200, indicating successful processing.<br>If the page is not found, a 404 is returned.<br>There are many types of status codes, so I won’t go into too much detail here, there is a lot of information online that you can look up and learn on your own.</p>
<p>Subsequently, just as the browser sends a request header with the request, the server sends a response header to the browser with the response. The response header contains some information about the server itself, such as the time the server generates the returned data, the type of data returned (JSON, HTML, streaming, etc.), and the cookies the server wants to save on the client.</p>
<p>After sending the response header, the server can continue to send the response body of the data, usually, the response body contains the actual content of the HTML.</p>
<p>This is how the server responds to the browser.</p>
<ol start="2">
<li>Disconnection<br>Normally, once the server returns the requested data to the client, it has to close the TCP connection. However, if the browser or server includes the following in its header information.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Connection:Keep-Alive</span><br></pre></td></tr></table></figure>

<p>Then the TCP connection will remain open after sending, so the browser can continue to send requests over the same TCP connection. Keeping the TCP connection open eliminates the time it takes to establish a connection for the next request and speeds up resource loading. For example, if you initialize a persistent connection to a Web page with embedded images from the same Web site, you can reuse that connection to request other resources without having to make a new TCP connection.</p>
<ol start="3">
<li>Redirection<br>At this point it looks like the request process is almost over, but there is one more thing you should know, for example, when you open geekbang.org in your browser, you’ll see that the page you end up on is <a href="https://www.geekbang.org" target="_blank" rel="noopener">https://www.geekbang.org</a>.</li>
</ol>
<p>The reason the two URLs are different is that there is a redirect involved. As before, you can still use curl to see what a request to geekbang.org will return.</p>
<p>Enter the following command at the console.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -I geekbang.org</span><br></pre></td></tr></table></figure>

<p>Note that the parameter entered here is -I, which is not the same as -i. -I means that you only need to get the response header and response line data, but not the response body data, and the final returned data is shown below.<br><img src="http://56ssc.cc/2020/10/25/HTTP/3.jpg" alt><br>As you can see, the status code returned by the response line is 301, which tells the browser that I need to redirect to another URL, and the URL to be redirected is contained in the Location field of the response header. The redirect execution process. This explains why you type in geekbang.org and end up with <a href="https://www.geekbang.org" target="_blank" rel="noopener">https://www.geekbang.org</a>.</p>
<h2 id="Why-do-many-sites-open-quickly-the-second-time"><a href="#Why-do-many-sites-open-quickly-the-second-time" class="headerlink" title="Why do many sites open quickly the second time?"></a>Why do many sites open quickly the second time?</h2><p>If the page opens quickly the second time, the main reason is that some time-consuming data is cached during the first page load.</p>
<p>So, what data gets cached? From the core request path described above, we can see that two pieces of data, DNS cache and page resource cache, are cached by the browser. Among them, DNS cache is relatively simple, it is mainly associated with the corresponding IP and domain name in the local browser, here we do not do too much analysis.</p>
<p>Let’s focus on the browser resource cache, and here’s how the cache is processed.</p>
<p><img src="http://56ssc.cc/2020/10/25/HTTP/4.jpg" alt></p>
<p>When the server returns the HTTP response header to the browser, the browser uses the Cache-Control field in the response header to set whether to cache the resource or not. Usually, we also need to set a cache expiration time for this resource, which is set by the Max-age parameter in Cache-Control.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cache-Control:Max-age=2000</span><br></pre></td></tr></table></figure>

<p>This means that if the cached resource is requested again before the cache expires, it will be returned to the browser directly, but if the cache expires, the browser will continue to make web requests with the following message in the HTTP request header.</p>
<p>However, if the cache has expired, the browser will continue to make web requests with the following HTTP request header.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">If-None-Match:&quot;4f80f-13c-3a1xb12a&quot;</span><br></pre></td></tr></table></figure>

<p>When the server receives the request header, it determines if the requested resource has been updated based on the value of the If-None-Match.</p>
<p>If there are no updates, a 304 status code is returned, which is the equivalent of the server telling the browser, “This cache can continue to be used, so I won’t send you data repeatedly this time.”<br>If the resource has an update, the server returns the latest resource directly to the browser.</p>
<p>Briefly, many websites are able to open in seconds on a second visit because they cache many of their resources locally, and the browser cache saves time by responding to requests directly using a local copy without generating a real web request. At the same time, the DNS data is also cached by the browser, which eliminates the need for DNS lookups.</p>
<h2 id="How-is-login-status-maintained"><a href="#How-is-login-status-maintained" class="headerlink" title="How is login status maintained?"></a>How is login status maintained?</h2><p>With the above, you’ve learned how caching works. Let’s take a look at how the login status is maintained.</p>
<p>The user opens the login page, fills in the username and password in the login box, and clicks the OK button. Clicking the OK button triggers the page script to generate the user login information and then calls the POST method to submit the user login information to the server.<br>After receiving the information submitted by the browser, the server will query the backend to verify if the user login information is correct, if so, it will generate a string to show the user’s identity and write the string to the Set-Cookie field of the response header as shown below, then send the response to the browser.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Set-Cookie: UID=3431uad;</span><br></pre></td></tr></table></figure>

<p>After receiving the response header from the server, the browser starts to parse the response header, and if it encounters a response header that contains a Set-Cookie field, the browser will save this field information locally. If the response header contains a Set-Cookie field, the browser will save this information locally, for example, keeping UID=3431uad local.<br>When the user visits again, the browser makes an HTTP request, but before the request is made, the browser reads the previously saved cookie data and writes the data into the Cookie field in the request header, and then the browser sends the request to the server.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cookie: UID=3431uad;</span><br></pre></td></tr></table></figure>

<p>When it finds the information containing UID=3431uad, the server queries the background and determines that the user is logged in, then generates page data containing the user information and sends the generated data to the browser.<br>After receiving the page data of the current user, the browser can display the logged in status information of the user correctly.<br>Now, you can see from this flow that the browser page status is achieved by using cookies. cookie flow can be seen in the following figure.<br><img src="http://56ssc.cc/2020/10/25/HTTP/5.jpg" alt></p>
<p>Simply put, if a server-side response is sent with a field for Set-Cookie in the response header, the browser will keep the contents of that field local. The next time the client sends a request to the server, the client will automatically add a cookie value to the request header before sending it out. When the server finds the cookie, it checks which client sent the connection request and compares it with the server’s records to get the user’s status information.</p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>The HTTP request in a browser goes through eight stages from initiation to completion: building the request, finding the cache, preparing the IP and port, waiting for the TCP queue, establishing the TCP connection, initiating the HTTP request, processing the request by the server, returning the request by the server, and disconnecting the connection.</p>
]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title>How the browser works</title>
    <url>/2020/10/10/How%20the%20browser%20works/</url>
    <content><![CDATA[<h2 id="For-applications-the-browser-has-always-been-important"><a href="#For-applications-the-browser-has-always-been-important" class="headerlink" title="For applications, the browser has always been important"></a>For applications, the browser has always been important</h2><p>In 1995, Netscape Inc. rose to prominence with the release of the “Netscape Browser”, after which it attempted to develop a web operating system that relied on the browser. This attracted Microsoft’s attention and vigilance, so in the same year Microsoft released Windows 95 and bundled with IE, which was a great success, and by 2002, Microsoft had already taken 80% of the browser market share.</p>
<p>This monopoly was not broken until 2008, when Chrome was introduced, completely overturning the previous browser’s architecture and design, dominating in speed and security, and increasing its market share dramatically. At the end of 2010, Google also introduced a web operating system, ChromeOS.</p>
<p>As you can see, the browser has been important since its inception, and that importance continues to grow. I’ve identified three broad evolutionary paths from the browser’s history that will hopefully give you an idea of what current web applications can do and what new areas they can be applied to in the future.</p>
<p>The first is the Webification of applications. With the spread of cloud computing and the rapid development of HTML5 technology, more and more applications are shifting to browser/server (B/S) architecture, which makes the browser increasingly important.</p>
<p>The second one is the mobility of Web applications. Although there are still problems to be solved at the technical level, Google has introduced PWA scheme to integrate the advantages of Web and local applications. PWA, by the way, is also something I’m personally looking forward to.</p>
<p>The third one is Web OS. In my opinion, the Web OS has two meanings: one is to use Web technology to build a pure operating system, such as ChromeOS; the other is to move the underlying structure of the browser in the direction of the OS architecture, which will involve many changes in the context of the overall architectural evolution.</p>
<p>Chrome is evolving towards SOA, and in the future, many modules will be provided as services for upper-level applications.<br>Introducing support for multiple programming languages in the browser, such as the newly supported WebAssembly.<br>Simplify the rendering process, making it more straightforward and efficient.<br>Increased support for system equipment features.<br>Support for the development of complex web projects.<br>In other words, the browser has gradually evolved into an “operating system” on top of the operating system.</p>
<h2 id="Why-do-I-need-to-learn-how-browsers-work"><a href="#Why-do-I-need-to-learn-how-browsers-work" class="headerlink" title="Why do I need to learn how browsers work?"></a>Why do I need to learn how browsers work?</h2><ol>
<li>Accurately assess the feasibility of web development projects<br>With the tremendous richness of Web features and increased browser performance, more and more projects can be developed using the Web. Understanding how the browser works allows you to make more accurate decisions about whether or not you can develop projects using the Web.</li>
</ol>
<p>For example, last year I worked on a gym virtual trainer project that was very time-intensive, with a lot of high speed rendering of animations and fast interaction with the scene. If I used traditional C++ to develop the interface, it would be impossible to deliver on time, and the maintenance would be very troublesome. So I decided to use a Web solution to develop the interface because it would reduce the development cost and shorten the delivery cycle. In the end, I realized the early delivery of the project with this solution, and the result was very satisfactory.</p>
<p>For this example, I think the best thing I did was to choose the right solution, but on the other hand, if I didn’t know anything about browsers and HTML5, it would have been easy for me to abandon the optimal solution.</p>
<ol start="2">
<li>look at the page from a higher dimension<br>As a qualified developer, you’ll also have an important skill: the ability to think about page performance from a user experience perspective. Let’s look at a few common UX metrics below.</li>
</ol>
<p>When a user requests a website and doesn’t see critical content within 1 second, the user gets the feeling that the task is interrupted.<br>When a user clicks on some button, if it doesn’t respond within 100ms, the user experiences a delay.<br>If the animation in the Web does not reach 60fps, the user will experience a stutter in the animation.<br>Here the page load time, the user interaction feedback time, and the number of frames in the web animation all determine the smoothness of the user experience, and ultimately the effectiveness of the user experience. In a world where user experience is especially important, we must be able to effectively address these experience issues to avoid irreparable damage to our products.</p>
<p>Often, however, these metrics are the result of a complex set of factors. If you want to develop smooth pages or diagnose performance problems in web pages, you need to understand how URLs become pages, and only after you understand this can you locate problems or write efficient code from a global perspective.</p>
<p>You can of course think of the browser as a black box, where you type in a URL on the left, and after the black box process, the right side returns what you expect. If you don’t know anything about black boxes, you can still write front-end code and use a lot of best-practice strategies to optimize your code, just as you can write applications on an operating system without understanding how it works.</p>
<p>But if you understand how the black box works, it’s a different story. You can look at your project from a higher dimension and quickly locate areas of the project that don’t make sense with a full view. For example, the display of the first screen involves DNS, HTTP, DOM parsing, CSS blocking, JavaScript blocking, and other technical factors, and failure to address one of these can result in latency across the page.</p>
<p>If you understand how the browser works, you will be able to string these knowledge points together, and eventually form your own knowledge system, and practice your ability to think and solve problems like an expert.</p>
<ol start="3">
<li>grasp the essence in the fast-paced technology iteration<br>From 2011 to the present, front-end technology has exploded, with new technologies coming out all the time, and I consider Node.js to be one of the core drivers of front-end development. I think Node.js is one of the core driving forces behind the development of the front-end. The server program!</li>
</ol>
<p>Although Node.js has only been around for a short time, a huge ecosystem has already formed around it. At the same time, the front-end ecosystem is thriving with new standards and technologies.</p>
<p>Why has Node.js grown so fast? The fundamental reason is that the browser features and the entire front-end development environment are not enough to support the growing demand, so “change” is the main theme of this period. This change has directly expanded the knowledge radius of front-end engineers, which has resulted in many front-end development engineers becoming stack-busters.</p>
<p>Although front-end technology is changing fast, I think there is a bigger opportunity here, and those who can catch the change quickly will be able to reap the dividends of this wave of change.</p>
<p>I believe that as script execution efficiency improves, page rendering performance improves and the development tool chain improves, the next front end will enter a relatively smooth phase. In layman’s terms: when the core technology is sufficient to support the core requirements, then the front-end ecology will enter a relatively stable state.</p>
<p>If you understand the working mechanism of the browser, then you can sort out the development of front-end technology, more profound understanding of the current technology, you will also be clear about its shortcomings, as well as the direction of evolution. So let’s look at how front-end technology has evolved to address these core requirements.</p>
<p>The first is the issue of script execution speed. For example, the problem of JavaScript design flaws and execution efficiency can be addressed in two ways.</p>
<p>Keep revising and updating the language itself so that you should be aware of the need for ES6, ES7, ES8, or TypeScript to appear. Such revisions are minimal changes to the current ecosystem, so it’s easier to implement.<br>WebAssembly is compiled by a compiler, so it’s small, fast, and can dramatically improve language execution, but it takes a long time to improve the language itself, and to build the ecosystem.<br>Next is the front-end modular development. For example, with the Web applications in various areas of in-depth, Web engineering complexity is also increasing, which generates the need for modular development, so the corresponding WebComponents standard. The familiar React and Vue are adapting incrementally to the WebComponents standard, and the best practices of the various front-end frameworks will in turn influence the WebComponents standard.</p>
<p>If you understand how the browser works, you’ll have a better understanding of the technologies involved in WebComponents, such as Shawdow DOM, HTML Templates, and so on.</p>
<p>Finally, there is the issue of rendering efficiency. Again, if you understand the browser’s rendering process, you should know that there are still major flaws in the current rendering of pages, and then you’ll know how to avoid these problems and develop more efficient web applications. At the same time, the Chrome team is working on improving these flaws, such as LayoutNG, a next-generation layout solution in development, and Slim Paint, a render thinning solution that aims to make rendering easier and more efficient.</p>
<p>As you can see, the factors behind these changes are triggered by the realities of current technology constraints, so understanding how the browser works puts you on a higher plane of understanding the front end.</p>
]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>Browser</tag>
      </tags>
  </entry>
  <entry>
    <title>Information theory</title>
    <url>/2020/10/09/Information%20theory/</url>
    <content><![CDATA[<p>Recent scientific research has consistently confirmed that uncertainty is the essential nature of the objective world. In other words, God really does roll the dice. An uncertain world can only be described using probabilistic models, and it was the depiction of probability that led to the birth of information theory.</p>
<p>In 1948, Claude Shannon, a physicist working at Bell Labs, published his famous paper “A Mathematical Theory of Communication”, which provided a quantitative analysis of the qualitative concept of information and marked the birth of information theory as a discipline.</p>
<p>In A Mathematical Theory of Communication, Shannon begins, “The basic problem of communication is to reproduce at one point precisely or approximately the message selected at another point. Messages usually have meaning, i.e., according to some system, the message itself points to or relates to a physically or conceptually specific entity. But the semantic meaning of a message is irrelevant to the engineering problem; the important issue is that a message comes from a collection of all possible messages.”</p>
<p>In this way, all types of messages are abstracted into logical symbols, which extends the scope of the communication task and the applicability of information theory, as well as completely divorcing the communication and processing of messages.</p>
<p>Information theory uses the concept of “information entropy” to explain the amount of information in a single source and the quantity and efficiency of information transmitted in communication, and to bridge the gap between the uncertainty of the world and the measurability of information.</p>
<p>Shannon’s quantification of information was based on this idea, and he defined “entropy” as the most basic and important concept in information theory. The word “entropy” comes from another encyclopedic scientist, John von Neumann, who argued that no one knew what entropy was. Although the concept has been used extensively in thermodynamics, it was not until it was extended to information theory that the nature of entropy was explained, i.e., the degree of chaos inherent in a system.</p>
<p>In information theory, if an event A occurs with probability p(A), then the amount of self-information about this event is defined as</p>
<p><img src="http://56ssc.cc/2020/10/09/Information theory/1.jpg" alt></p>
<p>The information entropy of a source containing multiple symbols can be calculated based on the amount of self-information of a single event. The information entropy of a source is the statistical average of the amount of self-information for each symbol that may be emitted by the source over the probability space constituted by the source. If a discrete source X contains n symbols and each symbol ai takes the value p(ai), then the source entropy of X is</p>
<p><img src="http://56ssc.cc/2020/10/09/Information theory/2.jpg" alt></p>
<p>The source entropy describes the average amount of information provided by each symbol sent by the source and is the mean value of the overall information measure of the source. When each symbol in the source is taken with equal probability, the source entropy takes the maximum value log2n, meaning that the source is the most random.</p>
<p>There is the concept of conditional probability in probability theory, and extending conditional probability to information theory gives conditional entropy. If there is a correlation between two sources, the source entropy of the other source, Y, decreases if one of the sources, X, is known. The conditional entropy H(Y|X) represents the uncertainty of the other random variable Y given that the random variable X is known, i.e., the entropy calculated from the conditional probability of Y given X and then solved mathematically for X with the expectation that.</p>
<p><img src="http://56ssc.cc/2020/10/09/Information theory/3.jpg" alt></p>
<p>The meaning of conditional entropy is that the variable Y is first classified once according to the value of the variable X, and its individual information entropy is computed for each divided category, and then the information entropy of each class is computed according to the distribution of X and its mathematical expectation.</p>
<p>In the case of a class, for example, where students can choose any seat in the classroom, there will be many possible seat distributions, and their source entropy will be larger. If a restriction is added to the choice of seats, such as boys sitting on the left and girls sitting on the right, the distribution of seats on the left and the distribution of seats on the right will still be random, but the situation will be much simpler compared to when no restriction is added. This is the reduction in uncertainty brought about by classification.</p>
<p>Once the conditional information entropy is defined, the concept of mutual information can be further obtained</p>
<p><img src="http://56ssc.cc/2020/10/09/Information theory/4.jpg" alt></p>
<p>Mutual information is equal to the source entropy of Y minus the conditional entropy of Y when X is known, i.e., the removal of uncertainty about Y provided by X. It can also be seen as the information gain that X brings to Y. The information gain that X brings to Y can also be seen as the information gain. The name “mutual information” is often used in the field of communication, while “information gain” is often used in the field of machine learning, and the essence of both is the same.</p>
<p>In machine learning, information gain is often used in the selection of classification features. For a given training data set Y, H(Y) represents the uncertainty of classifying the training set when no features are given, while H(Y|X) represents the uncertainty of classifying the training set Y using feature X. H(Y|X) represents the uncertainty of classifying the training set Y using feature X when no features are given. The gain of information represents the degree to which the uncertainty of classification of the training set Y is reduced by feature X, i.e., how well feature X distinguishes the training set Y from other features.</p>
<p>Obviously, features with greater gain of information have better classification ability. However, the value of information gain depends heavily on the information entropy H(Y) of the dataset, and thus does not have absolute significance. To solve this problem, researchers have also developed the concept of information gain ratio and defined it as g(X,Y) = I(X;Y)/H(Y).</p>
<p>Another information theoretic concept often used in machine learning is called “Kullback-Leibler scatter”, or KL scatter, which is a method to describe the difference between two probability distributions P and Q. KL scatter is defined as follows</p>
<p><img src="http://56ssc.cc/2020/10/09/Information theory/5.jpg" alt></p>
<p>KL scatter is a measure of the amount of additional information. Given a source with a probability distribution of symbols P(X), one can devise an optimal encoding for P(X) such that the least number of average bits (equal to the source entropy of the source) is required to represent the source.</p>
<p>However, when the set of symbols of the source remains unchanged, and the probability distribution of conformity changes to Q(X), then the optimal encoding for P(X) is used to encode the symbols of the distribution of conformity Q(X), and the number of characters in the resultant encoding will be a few bits more than the optimal value.</p>
<p>The KL scatter is a measure of the average number of extra bits per character in this case, and can also represent the distance between the two distributions.</p>
<p>Two important properties of KL scatter are non-negativity and asymmetry.</p>
<p>Non-negativity means that the KL sparsity is greater than or equal to 0, and the equal sign is only taken when the two distributions are identical.</p>
<p>Asymmetry means that DKL(P||Q) ≠ DKL(Q||P), i.e., the deviation obtained by approximating Q(X) with P(X) is different from that obtained by approximating P(X) with Q(X), and thus the KL dispersion does not satisfy the mathematical definition of distance, which is important to note.</p>
<p>In fact, DKL(P||Q) and DKL(Q||P) represent two different ways of approximating the distance. For DKL(P||Q) to be minimal, it is necessary to have Q(X) equally not equal to 0 at locations where P(X) is not equal to 0. For DKL(Q||P) to be minimal, it is necessary to have Q(X) equally equal to 0 at locations where P(X) is equal to 0.</p>
<p>In addition to the indicators defined above, there is another important theorem in information theory, called the “maximum entropy principle”. The principle of maximum entropy is a criterion for determining the statistical properties of random variables in an attempt to best fit the objective situation. The worst-case scenario for an unknown probability distribution is that it takes every possible value with equal probability. This is the time when the probability distribution is most uniform, which means that the random variable is the most random, and predicting it is the most difficult.</p>
<p>From this point of view, the essence of the principle of maximum entropy is that it does not introduce any unnecessary constraints or assumptions when extrapolating an unknown distribution, and therefore the most uncertain results can be obtained with the least risk in prediction. The famous saying “Don’t put all your eggs in the same basket” in investment finance can be regarded as a practical application of the principle of maximum entropy.</p>
<p>The maximum entropy model can be obtained by applying the principle of maximum entropy to a classification problem. In the classification problem, first of all, it is necessary to determine some characteristic functions as the basis of classification. In order to ensure the validity of the feature functions, their mathematical expectations on the true distribution P(X) of the model and on the empirical distribution P~(X) derived from the training dataset should be equal, i.e., the estimate of the mathematical expectations of a given feature function should be an unbiased estimate.</p>
<p>In this way, each feature function corresponds to a constraint. The task of classification is to determine the best classification model given these constraints. Since there is no a priori knowledge of classification other than these constraints, it is necessary to solve the distribution of conditions with the greatest uncertainty using the principle of maximum entropy, i.e., let the following functions maximize their values</p>
<p><img src="http://56ssc.cc/2020/10/09/Information theory/6.jpg" alt></p>
<p>where p(y|x) is the distribution of objective conditions to be determined for the classification problem. Calculating the maximum value of the above equation is essentially a constrained optimization problem, and the constraints determined by the eigenfunction can be transformed into an unconstrained optimization problem by removing its influence through the introduction of the Lagrange multiplier. It can be proven mathematically that the solution to this model exists and is unique.</p>
<p>Today I share with you the essential information-theoretic foundations of AI, focusing on the interpretation of abstract concepts rather than the derivation of mathematical formulas, the main points of which are as follows.</p>
<ul>
<li>Information theory deals with uncertainty in the objective world.</li>
<li>Conditional entropy and information gain are important parameters in classification problems.</li>
<li>KL scatter is used to describe the difference between two different probability distributions.</li>
<li>The principle of maximum entropy is a common criterion in classification problems.</li>
</ul>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>Integrated Learning</title>
    <url>/2020/12/20/Integrated%20Learning/</url>
    <content><![CDATA[<h2 id="The-concepts-of-boosting-and-bagging"><a href="#The-concepts-of-boosting-and-bagging" class="headerlink" title="The concepts of boosting and bagging."></a>The concepts of boosting and bagging.</h2><p>(1) bagging: random sampling from the original data to get S datasets of the same size to train S base learners, which are not dependent on each other. It is a parallel method. </p>
<p>The weights of each classifier are equal. The classification results are classified using these S classifiers, and the category with the most classifier voting results is selected as the final classification result.(The sampling method is sampling with put-back: allowing that there can be duplicate values in each small data set.)</p>
<p>There is no restriction on bagging for weak learners, as with Adaboost. But the most common ones are generally decision trees and neural networks.</p>
<p>The aggregation strategy of bagging is also relatively simple. For classification problems, simple voting is usually used, and the category or one of the categories with the most votes is the final model output. For regression problems, simple averaging is usually used, where the final model output is obtained by arithmetic averaging the regression results obtained by T weak learners.</p>
<p>Advantages. </p>
<p>a. The algorithm samples each time to train the model, which has a strong generalization capability and is useful for reducing the variance of the model, but of course the fit to the training set will be worse, i.e., the model bias will be larger.</p>
<p>b. Training a Bagging integration is of the same order of complexity as training a learner directly using the base learning algorithm, which is efficient.</p>
<p>c. Standard AdaBoost is only applicable to binary classification, Bagging can be directly used for tasks such as multi-classification, regression, etc..</p>
<p>d. Because of self-sampling, each base learner uses only about 63.2% of the samples in the training set of the accident, and the remaining samples (36.8% called OOB) can be used as the validation set, etc.</p>
<p>(2) boosting: using all the data to train the base learner, there is a dependency between individual learners, each learner is based on the results of the previously trained learners, serial training, focus on the data that was wrongly scored to get a new learner, to achieve the effect of improvement. (In layman’s terms, this means learning one point at a time and then approaching the final value to be predicted step by step.) </p>
<p>The classification result is based on the weighted sum of all classifiers, and the weights of the classifiers are not equal, each weight represents the success of its corresponding classifier in the previous iteration.</p>
<p>Advantages: low generalization error, easy to implement, high classification accuracy, few adjustable parameters.</p>
<p>Disadvantages: more sensitive to outlier points.</p>
<p>Both are identical: the types of classifiers used are the same.</p>
<h2 id="Why-is-bagging-to-reduce-variance-variance-while-boosting-to-reduce-bias-bias"><a href="#Why-is-bagging-to-reduce-variance-variance-while-boosting-to-reduce-bias-bias" class="headerlink" title="Why is bagging to reduce variance variance, while boosting to reduce bias bias?"></a>Why is bagging to reduce variance variance, while boosting to reduce bias bias?</h2><p>　　(1) Bagging resamples the samples, trains a model for each resampled subset, and finally takes the average. Due to the similarity of the subsample sets and the use of the same model, each model has approximately equal bias and variance (in fact, the distribution of each model is also approximately the same, but not independent). bagging method to obtain each submodel is somewhat correlated, belongs to the intermediate state of the above two extreme conditions, so it can reduce the variance to some extent. variance, prediction is more focused)</p>
<p>　　(2) Therefore, boosting is minimizing the loss function sequentially (in series), and its bias naturally decreases gradually. (2) Thus, boosting is minimizing the loss function sequentially, and the bias decreases gradually. (lower bias, more accurate prediction)</p>
<p>　　(3) Intuitive explanation<br>　　boosting is the combination of many weak classifiers into one strong classifier. Weak classifiers have high bias, while strong classifiers have low bias, so boosting plays a role in reducing bias. variance is not the main consideration for boosting.<br>　　Bagging is averaging over many strong (or even overly strong) classifiers. Here, the bias of each individual classifier is low, and the bias remains low after averaging; and each individual classifier is strong enough to produce overfitting, i.e., the variance is high, and the averaging operation serves to reduce this variance.</p>
<h2 id="The-main-differences-between-the-two"><a href="#The-main-differences-between-the-two" class="headerlink" title="The main differences between the two"></a>The main differences between the two</h2><p>Sample selection: Bagging uses Bootstrap random with put-back sampling; while Boosting’s training set is constant for each round, and only the weight of each sample is changed.</p>
<p>Sample weights: Bagging uses uniform sampling, and each sample is equally weighted; Boosting adjusts the sample weights according to the error rate, and the larger the error rate, the larger the sample weight.</p>
<p>Prediction function: All the prediction functions of Bagging have equal weights; the smaller the error in Boosting, the greater the weight of the prediction function.</p>
<p>Parallel computation: Each prediction function of Bagging can be generated in parallel; each prediction function of Boosting must be generated sequentially and iteratively.</p>
<h2 id="The-following-are-the-new-algorithms-obtained-by-combining-decision-trees-with-these-algorithmic-frameworks"><a href="#The-following-are-the-new-algorithms-obtained-by-combining-decision-trees-with-these-algorithmic-frameworks" class="headerlink" title="The following are the new algorithms obtained by combining decision trees with these algorithmic frameworks."></a>The following are the new algorithms obtained by combining decision trees with these algorithmic frameworks.</h2><p>1) Bagging + Decision Tree (CART) = Random Forest</p>
<p>2) AdaBoost + Decision Tree = Boosted Tree</p>
<p>3) Gradient Boosting + Decision Tree = GBDT</p>
<h2 id="Possible-benefits-of-combining-learners"><a href="#Possible-benefits-of-combining-learners" class="headerlink" title="Possible benefits of combining learners 　　"></a>Possible benefits of combining learners 　　</h2><p>(1) Improved generalization </p>
<p>(2) Reduced risk of local optimum </p>
<p>(3) Expanded hypothesis space and better similarity.</p>
<h2 id="Methods-strategies-for-model-fusion"><a href="#Methods-strategies-for-model-fusion" class="headerlink" title="Methods/strategies for model fusion"></a>Methods/strategies for model fusion</h2><p>(1) Averaging: For numerical regression prediction problems, the commonly used combination strategy is averaging, that is, averaging the outputs of several weak learners to obtain the final prediction output.</p>
<p>(2) Voting method: The simplest voting method is the relative majority voting method, which is often referred to as minority rule.</p>
<p>(3) Learning method: stacking (the output of this layer is used as part of the input data of the next layer)</p>
<p>When using the combination strategy of stacking, instead of doing simple logic processing on the results of the weak learner, we add another layer of learners, that is, we take the learning results of the weak learner in the training set as input and the output of the training set as output, and retrain a learner to get the final results.</p>
<h2 id="Principles-of-common-fusion-frameworks-advantages-and-disadvantages-will-fusion-necessarily-improve-performance-Why-fusion-may-improve-the-prediction-effect"><a href="#Principles-of-common-fusion-frameworks-advantages-and-disadvantages-will-fusion-necessarily-improve-performance-Why-fusion-may-improve-the-prediction-effect" class="headerlink" title="Principles of common fusion frameworks; advantages and disadvantages; will fusion necessarily improve performance? Why fusion may improve the prediction effect?"></a>Principles of common fusion frameworks; advantages and disadvantages; will fusion necessarily improve performance? Why fusion may improve the prediction effect?</h2><p>Principle: more than one is better than one + ensure accuracy, prevent overfitting + weak learners obviously + good but different </p>
<p>Common: bagging (parallel + less variance), boosting (serial + less bias), stacking (output-&gt;input) </p>
<p>Not necessarily, good but different </p>
<p>Models differ, reflect different expressive power</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Machine Learning</title>
    <url>/2020/11/07/Introduction%20to%20Machine%20Learning/</url>
    <content><![CDATA[<p>I don’t know if you have ever noticed in your life that we can easily distinguish Japanese, Koreans and Thais by their looks, but we are blind to the faces of English, Russians and Germans. The reason for this is that, on the one hand, Japan, Korea, and Thailand are our neighbors and we have more opportunities to observe ordinary people in these countries; on the other hand, regardless of clothing and make-up, the same people make it easier to compare and distinguish facial features.</p>
<p>Therefore, a lot of observations can lead to the conclusion that Chinese people have moderate jaws, Japanese people have long faces and long noses, Koreans have small eyes and high cheekbones, and Thais have dark complexions. It is these characteristics that are used as the basis for determining whether passerby A is from Japan or passerby B is from Korea.</p>
<p>The above example is a simplified version of the human learning mechanism: extracting recurring patterns and motifs from a large number of phenomena. The implementation of this process in artificial intelligence is machine learning.</p>
<p>Defined formally, an algorithm is said to implement machine learning if it uses some experience to improve its own performance on a particular class of tasks. And from a methodological point of view, machine learning is the discipline in which computers build probabilistic statistical models based on data and use the models to predict and analyze the data.</p>
<p>Machine learning can be described as coming from the data and going to the data. Assuming that the existing data has certain statistical characteristics, the different data can be considered as samples that meet the same distribution independently. What machine learning does is to derive a model describing all the data based on the existing training data and to achieve optimal prediction of the unknown test data based on the derived model.</p>
<p>In machine learning, data are not quantitative values in the usual sense, but descriptions of certain properties of objects. The described properties are called attributes, the values of the attributes are called attribute values, and the vectors obtained from the orderly arrangement of the different attribute values are the data, also called instances.</p>
<p>In the example at the beginning of this article, the typical attributes of a yellow face include skin color, eye size, nose length, and cheekbone height. The standard Chinese example A is a combination of the attributes {light, large, short, low }, while the standard Korean example B is a combination of the attributes {light, small, long, high }.</p>
<p>According to the knowledge of linear algebra, the different attributes of the data can be considered independent of each other, and thus each attribute represents a different dimension that together tensor the feature space.</p>
<p>Each set of attribute values is a point in this space, and thus each instance can be considered a vector, or feature vector, in the feature space.</p>
<p>It is important to note that the feature vector here is not the one corresponding to the feature value, but the vector in the feature space. The output can be obtained by classifying the input data according to the feature vector.</p>
<p>In the previous example, the input data is a person’s facial features, and the output data is a Chinese / Japanese / Korean / Thai person out of four. In the actual machine learning task, the form of output may be more complex. Depending on the type of input and output, the prediction problem can be divided into the following three categories.</p>
<ol>
<li><p>Categorical problems: the output variable is a finite number of discrete variables, the simplest bicategorical problem when the number of variables is 2.</p>
</li>
<li><p>Regression problems: both input and output variables are continuous variables.</p>
</li>
<li><p>Labeling problem: Both input and output variables are sequences of variables.</p>
</li>
</ol>
<p>In reality, however, people from every country are not the same and naturally look very different, so a Korean with thick eyebrows and big eyes could be mistaken for a Chinese, or a Japanese with darker skin could be mistaken for a Thai.</p>
<p>The same problem can exist in machine learning. It is impossible for an algorithm to match all the training data exactly, nor can it predict all the test data accurately. Therefore, error performance becomes one of the important metrics in machine learning.</p>
<p>In machine learning, error is defined as the difference between the actual predicted output of the learner and the true output of the sample. In classification problems, the common error function is the error rate, which is the proportion of all samples that are classified incorrectly.</p>
<p>Errors can be further divided into two categories: training errors and test errors. Training error refers to the error of the learner on the training dataset, also known as empirical error; test error refers to the error of the learner on the new sample, also known as generalization error.</p>
<p>The training error describes the correlation between the input attributes and the output classification and can determine whether a given problem is an easy one to learn. Test error, on the other hand, reflects the ability of the learner to predict an unknown test data set and is an important concept in machine learning. Practical learners are those that have low test error, i.e., they perform better on new samples.</p>
<p>Learners rely on known data to fit the real situation, i.e., the model obtained by the learner should be as close to the real model as possible, and therefore extract universal laws that apply to all unknown data in the training dataset as much as possible.</p>
<p>However, once too much emphasis is placed on the training error and the degree of conformity between the prediction laws and the training data is pursued, some of the non-universal properties of the training sample itself will be mistaken for the universal nature of all data, leading to a decrease in the generalization ability of the learner.</p>
<p>In the preceding example, if there are few foreigners in contact with the training data and no Koreans with double eyelids, the erroneous stereotype of “single eyelids are all Koreans” will inevitably appear in one’s thinking, which is a typical overfitting phenomenon that mistakes the features of the training data for the features of the whole.</p>
<p>The reason for overfitting is usually that the model contains too many parameters during learning, resulting in low training errors but high testing errors.</p>
<p>The counterpart to overfitting is underfitting. If the cause of overfitting is too much learning, the cause of underfitting is too little learning, so that the basic properties of the training data are not learned. The consequence of underfitting is that the learner may even mistake an image of a chimpanzee for a human if the learner’s capabilities are inadequate.</p>
<p>In practical machine learning, underfitting can be overcome by improving the learner’s algorithm, but overfitting cannot be avoided, and its effects can only be minimized. Since the number of training samples is limited, a model with a finite number of parameters is sufficient to include all training samples.</p>
<p>But the more parameters the model has, the less data there is to accurately match the model, and when such a model is applied to an infinite amount of unknown data, overfitting is unavoidable. Moreover, the training sample itself may contain some noise, and this random noise will bring additional errors to the accuracy of the model.</p>
<p>On the whole, the relationship between test error and model complexity is parabolic. When the complexity of the model is low, the test error is high; as the complexity of the model increases, the test error will gradually decrease and reach a minimum value; then as the complexity of the model continues to increase, the test error will then increase, corresponding to the occurrence of overfitting.</p>
<p>In model selection, in order to make a more accurate estimate of the test error, a widely used method is cross-validation. The idea of cross-validation lies in the repeated use of a limited number of training samples, by cutting the data into subsets, so that different subsets make up the training set and test set respectively, and on this basis, repeated training, testing and model selection to achieve the optimal effect.</p>
<p>If the training data set is divided into 10 subsets D1-10 for cross-validation, each model needs to be trained in 10 rounds, where the training set used in round 1 is the 9 subsets D2<del>D10, and the trained learners are tested on subset D1; the training set used in round 2 is D1 and The 9 subsets D3</del>D10, the trained learners are tested on subset D2. And so on, when the model completes testing on all 10 subsets, its performance is the average of the 10 test results. The model with the lowest average test error among the different models is also the optimal model.</p>
<p>In addition to the algorithm itself, the value of the parameters is also an important factor that affects the performance of the model, the same learning algorithm in different parameter configurations, the performance of the model will be significantly different. Therefore, parameter tuning, i.e., setting the parameters of an algorithm, is an important engineering problem in machine learning, and this is particularly evident in today’s neural networks and deep learning.</p>
<p>Assuming that a neural network contains 1000 parameters, each with 10 possible values, for each training/test set there are 100010 models to be examined, and thus a major problem in the tuning process is the trade-off between performance and efficiency.</p>
<p>In human learning, some people may have a high level of knowledge and some may have no knowledge at all. A similar classification is used in machine learning. Depending on whether the training data has label information or not, machine learning tasks can be divided into the following three categories.</p>
<ol>
<li><p>Supervised learning: learning based on training data from known categories.</p>
</li>
<li><p>Unsupervised learning: learning based on unknown categories of training data.</p>
</li>
<li><p>Semi-supervised learning: learning using both known and unknown categories of training data.</p>
</li>
</ol>
<p>Depending on the learning style, the better learning algorithms perform supervised learning tasks. Even AlphaGo Zero, which is claimed to be self-taught and completely independent of its dependence on Go games, is subject to Go win-loss rules and thus cannot be separated from supervised learning.</p>
<p>Supervised learning assumes that the training data satisfies the condition of being independently and homogeneously distributed, and learns a mapping model from the training data to the input to the output. There may be an infinite number of models reflecting this mapping, all of which together form the hypothesis space. The task of supervised learning is to find the optimal model in the hypothesis space according to a specific error criterion.</p>
<p>Depending on the learning method, supervised learning can be divided into two categories: generative and discriminative methods.</p>
<p>The generative method determines the conditional probability distribution P(Y|X) based on the joint probability distribution between input data and output data, which represents the generative relationship between input X and output Y. The discriminant method directly learns the conditional probability distribution P(Y|X) or the decision function f(X), which represents the prediction method to obtain output Y based on input X. The generative method is faster than the discriminant method, which is faster than the conditional probability distribution P(Y|X) or the decision function f(X).</p>
<p>In contrast, the generative method has a faster convergence speed and a wider range of applications, while the discriminant method has a higher accuracy and is simpler to use.</p>
<p>Today I have shared with you the basic principles and foundational concepts of machine learning, the highlights of which are as follows.</p>
<ul>
<li>Machine learning is the discipline in which computers construct probabilistic statistical models based on data and use the models to predict and analyze the data.</li>
<li>Depending on the type of input and output, machine learning can be divided into three categories: classification problems, regression problems, and labeling problems.</li>
<li>Overfitting is an unavoidable problem in machine learning and its effects can be reduced by selecting the right model.</li>
<li>Supervised learning is currently the mainstream task of machine learning, including both generative and discriminative methods.</li>
</ul>
<p>In the field of image recognition, high recognition rates are backed by a large number of finely labelled image samples, and the labelling of millions of digital images is undoubtedly labor-intensive.</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Redis(2)</title>
    <url>/2020/11/13/Introduction%20to%20Redis(2)/</url>
    <content><![CDATA[<p>Redis is one of the hottest in-memory databases, and by reading and writing data in memory, it greatly increases the read and write speed, so it can be said that Redis is an indispensable part of achieving high concurrency on a website.</p>
<p>When we use Redis, we are exposed to Redis’ five object types (string, hash, list, collection, and ordered collection), and the richness of the types is one of Redis’ major advantages over Memcached and others. On top of understanding the usage and characteristics of the 5 object types of Redis, it is helpful to further understand the memory model of Redis, such as.</p>
<ol>
<li><p>Estimate the memory usage of Redis. To date, the cost of memory usage is still relatively high, so you can’t be reckless with the use of memory; according to the needs of a reasonable assessment of Redis memory usage, choose the appropriate machine configuration, you can save costs while meeting the demand.</p>
</li>
<li><p>Optimize memory usage. Understanding the Redis memory model can help you choose more appropriate data types and encodings to better utilize Redis memory.</p>
</li>
<li><p>Analyze and solve problems. When Redis has problems such as blocking and memory usage, you can find out the cause of the problem as soon as possible to analyze and solve the problem.</p>
</li>
</ol>
<p>This article introduces the Redis memory model (3.0 as an example), including Redis memory usage and how to query it, how different object types are encoded in memory, memory allocators (jemalloc), simple dynamic strings (SDS), RedisObject, etc.; and then introduces several applications of the Redis memory model on this basis.</p>
<h2 id="Redis-memory-statistics"><a href="#Redis-memory-statistics" class="headerlink" title="Redis memory statistics"></a>Redis memory statistics</h2><p>After the client connects to the server via redis-cli, you can view the memory usage through the info command.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">info memory</span><br></pre></td></tr></table></figure>

<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/1.jpg" alt></p>
<p>The info command can display a lot of information about the redis server, including basic server information, CPU, memory, persistence, client connection information, and so on; memory is a parameter, which means only memory-related information is displayed.</p>
<p>A few of the more important statements in the returned results are as follows.</p>
<p>(1) used_memory: The total amount of memory (in bytes) allocated by the Redis allocator, including the virtual memory used (i.e. swap); the Redis allocator will be introduced later. used_memory_human just shows more friendly.</p>
<p>(2) used_memory_rss: the amount of memory (in bytes) occupied by the Redis process is the same as that seen by the top and ps commands; in addition to the memory allocated by the allocator, used_memory_rss also includes the memory needed by the process itself, memory fragments, etc., but not virtual memory.</p>
<p>Thus, used_memory and used_memory_rss, the former is the amount obtained from the Redis perspective, and the latter is the amount obtained from the operating system perspective. The difference between the two is due to memory fragmentation and the memory required by Redis processes to run, which makes the former smaller than the latter, and the existence of virtual memory, which makes the former larger than the latter.</p>
<p>As the amount of data in Redis is relatively large in actual applications, the memory occupied by running processes is much smaller than the amount of Redis data and memory fragmentation; therefore, the ratio of used_memory_rss and used_memory becomes a parameter to measure the Redis memory fragmentation rate; this parameter is mem_fragmentation_ratio.</p>
<p>(3) mem_fragmentation_ratio: memory fragmentation ratio, the ratio of used_memory_rss / used_memory.</p>
<p>mem_fragmentation_ratio: the ratio of used_memory_rss / used_memory. mem_fragmentation_ratio is greater than 1, and the larger the value, the larger the memory fragmentation ratio. mem_fragmentation_ratio&lt;1 means that Redis uses virtual memory, because the medium of virtual memory is disk, which is much slower than the memory speed. It should be addressed promptly, such as adding Redis nodes, increasing Redis server memory, optimizing applications, etc.</p>
<p>In general, mem_fragmentation_ratio around 1.03 is a healthy state (for jemalloc); the mem_fragmentation_ratio value in the above screenshot is large because no data has been deposited into Redis yet, and the memory in which the Redis process itself is running makes the used_fragmentation_ratio value large. memory_rss is much larger than used_memory.</p>
<p>(4) mem_allocator: Redis uses a memory allocator, specified at compile time; it can be libc, jemalloc or tcmalloc, the default is jemalloc; the one used in the screenshot is the default jemalloc.</p>
<h2 id="Redis-memory-partitioning"><a href="#Redis-memory-partitioning" class="headerlink" title="Redis memory partitioning"></a>Redis memory partitioning</h2><p>Redis, as an in-memory database, stores mainly data (key-value pairs) in memory; as you can see from the previous description, in addition to data, other parts of Redis also occupy memory.</p>
<p>The memory consumption of Redis can be divided into the following main parts.</p>
<ol>
<li>Data<br>As a database, the data is the main part; the memory occupied by this part is counted in used_memory.</li>
</ol>
<p>Redis uses key-value pairs to store data, where the values (objects) consist of 5 types, namely strings, hashes, lists, collections, and ordered collections. These five types are provided externally by Redis, and in fact, there may be two or more internal coded implementations of each type within Redis; in addition, Redis does not throw data directly into memory when storing objects, but rather wraps objects in various ways: redisObject, SDS, etc. Later in this article, we will focus on data storage in Redis. The details.</p>
<ol start="2">
<li>The memory required to run the process itself<br>The main Redis process itself must use memory to run, such as code, constant pools, and so on; this memory is in the order of a few megabytes, which is negligible in most production environments compared to the memory used by Redis data. This memory is not allocated by jemalloc, so it is not counted in used_memory.</li>
</ol>
<p>Additional note: In addition to the main process, Redis also uses memory for running child processes, such as those created when Redis performs AOF and RDB rewrites. Of course, this memory does not belong to Redis processes and is not counted in used_memory or used_memory_rss.</p>
<ol start="3">
<li><p>Buffer memory<br>Buffered memory includes client buffers, copy backlog buffers, and AOF buffers; the client buffer stores the input and output buffers for client connections; the copy backlog buffer is used for partial replication functions; and the AOF buffer is used to save the most recent write commands when performing an AOF rewrite. It is not necessary to know the details of these buffers before knowing the corresponding functions; this part of memory is allocated by jemalloc, so it is counted in used_memory.</p>
</li>
<li><p>Memory fragmentation<br>Memory fragmentation is created by redis during the process of allocating and reclaiming physical memory. For example, if changes to data are frequent and the size of the data varies greatly from one another, it may result in memory fragmentation when the space freed by redis is not freed in physical memory, but redis is unable to use it efficiently. Memory fragmentation is not counted in used_memory.</p>
</li>
</ol>
<p>The generation of memory fragmentation is related to the operations performed on the data, the characteristics of the data, etc. In addition, it is also related to the memory allocator used: if the memory allocator is properly designed, the generation of memory fragmentation can be reduced as much as possible. The jemalloc we’ll talk about later is a good way to control memory fragmentation.</p>
<p>If the memory fragmentation in the Redis server is already large, it can be reduced by a safe reboot: after the reboot, Redis re-reads the data from the backup file, reorganizes it in memory, and selects the appropriate memory cell for each data to reduce memory fragmentation.</p>
<h2 id="Redis-data-store-details"><a href="#Redis-data-store-details" class="headerlink" title="Redis data store details"></a>Redis data store details</h2><ol>
<li>Overview<br>The details of the Redis data store involve memory allocators (such as jemalloc), simple dynamic strings (SDS), the five object types and their internal coding, and redisObject.</li>
</ol>
<p>The following diagram shows the data model involved in executing set hello world.</p>
<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/2.jpg" alt></p>
<p>(1) dictEntry: Redis is a Key-Value database, so for each key-value pair will have a dictEntry, which stores pointers to Key and Value; next points to the next dictEntry, which has nothing to do with the Key-Value.</p>
<p>(2) Key: Key (“hello”) is not directly stored as a string, but is stored in the SDS structure, visible in the upper right corner of the figure.</p>
<p>(3) redisObject: Value(“world”) is neither stored directly as a string nor in SDS like Key, but in a redisObject. In fact, Value is stored in redisObject regardless of which of the five types it is; the type field in redisObject specifies the type of the Value object, and the ptr field points to the address of the object. However, it can be seen that string objects, although wrapped in redisObject, still need to be stored via SDS.</p>
<p>In fact, in addition to the type and ptr fields, there are other fields in redisObject that are not given in the diagram, such as those used to specify the internal encoding of the object; these are described in detail later.</p>
<p>(4) jemalloc: Whether it is a DictEntry object, a redisObject, or an SDS object, a memory allocator (such as a jemalloc) is required to allocate memory for storage. The DictEntry object, for example, is composed of three pointers, accounting for 24 bytes in a 64-bit machine. jemalloc will allocate 32 bytes of memory for it.</p>
<p>The following is an introduction to jemalloc, redisObject, SDS, object types and internal coding, respectively.</p>
<ol start="2">
<li>jemalloc<br>Redis will specify a memory allocator at compile time; the memory allocator can be libc, jemalloc or tcmalloc, and the default is jemalloc.</li>
</ol>
<p>The default is jemalloc. jemalloc is the default memory allocator for Redis, and it does a relatively good job of reducing memory fragmentation. jemalloc divides the memory space in a 64-bit system into three ranges: small, large and huge.</p>
<p>The memory units divided by jemalloc are shown in the following figure.</p>
<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/3.jpg" alt></p>
<ol start="3">
<li>redisObject<br>As mentioned earlier, there are five types of Redis objects; regardless of the type, Redis does not store them directly, but through the redisObject object.</li>
</ol>
<p>The redisObject object is very important. Redis object types, internal coding, memory recycling, shared objects, etc., all require redisObject support.</p>
<p>The definition of a redisObject is as follows (may vary slightly from version to version of Redis).</p>
<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/4.jpg" alt></p>
<p>(1) type<br>The type field represents the type of the object and is four bits long; it currently includes REDIS_STRING (string), REDIS_LIST (list), REDIS_HASH (hash), REDIS_SET (set), and REDIS_ZSET (ordered set).</p>
<p>When we execute the type command, we are reading the type field of RedisObject to get the type of the object; the following image shows.<br><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/5.jpg" alt><br>(2) encoding<br>encoding represents the internal encoding of an object and takes up 4 bits.</p>
<p>For each type supported by Redis, there are at least two encodings. For example, for strings, there are int, embstr, and raw encodings. Through the encoding property, Redis can set different encodings for objects according to different usage scenarios, which greatly improves the flexibility and efficiency of Redis. Redis uses compressed lists to store the elements in a list, which takes up less memory and can be loaded faster than double-ended lists.</p>
<p>The object encoding command allows you to see how the object is encoded, as shown below.</p>
<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/6.jpg" alt></p>
<p>(3) lru<br>The lru records the last time an object was accessed by the command program, and the number of bits it occupies varies by version (e.g., 24 bits for version 4.0 and 22 bits for version 2.6).</p>
<p>The object idle time can be calculated by comparing the lru time with the current time; the object idletime command displays the idle time in seconds.</p>
<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/7.jpg" alt></p>
<p>The lru value is not only printed by the object idletime command, but also has to do with Redis memory recuperation: if Redis has the maxmemory option turned on and the memory recuperation algorithm chooses volatile-lru or allkeys-lru, then when Redis memory exceeds the value specified by maxmemory, Redis will prioritize the object with the longest idle time to be freed.</p>
<p>(4) refcount<br>refcount and shared objects</p>
<p>refcount records the number of times the object has been referenced, type is integer. refcount’s role, mainly in the object’s reference count and memory recovery. When a new object is created, refcount is initialized to 1; when a new program uses the object, refcount is added to 1; when the object is no longer used by a new program, refcount is subtracted from 1; when refcount becomes 0, the memory occupied by the object will be freed.</p>
<p>The object that has been used many times in Redis (refcount&gt;1) is called a shared object.Redis saves memory by not creating a new object when some objects are repeated, but still uses the original object. This reused object is called a shared object. Currently, shared objects only support string objects with integer values.</p>
<p>The concrete implementation of shared objects</p>
<p>Redis’ shared objects currently only support string objects with integer values. This is actually a balance between memory and CPU (time): while shared objects reduce memory consumption, it takes extra time to determine if two objects are equal. The complexity of the operation is O(1) for integer values; O(n) for ordinary strings; and O(n^2) for hashes, lists, collections, and ordered collections.</p>
<p>While shared objects can only be string objects with integer values, shared objects may be used for all five types (e.g., elements of hashes, lists, etc. can be used).</p>
<p>In the current implementation, the Redis server creates 10,000 string objects with integer values from 0 to 9999 at initialization; when Redis needs to use string objects with values from 0 to 9999, it can use these shared objects directly. 10,000 can be adjusted with the parameter REDIS_SHARED_INTEGERS (in 4.0 it was OBJ_SHARED_INTEGERS) value to change.</p>
<p>The number of references to a shared object can be viewed with the object refcount command, as shown in the following figure. The result page of the command confirms that only integers between 0 and 9999 are shared.</p>
<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/8.jpg" alt></p>
<p>(5) ptr<br>The ptr pointer points to the specific data, as in the previous example, set hello world, and ptr points to the SDS containing the string world.</p>
<p>(6) Summary<br>In summary, the structure of a redisObject is related to object type, encoding, memory recycling, and shared objects; a redisObject object has a size of 16 bytes.</p>
<p>4bit+4bit+24bit+4Byte+8Byte=16Byte.</p>
<ol start="4">
<li>SDS<br>Instead of using C strings (i.e., arrays of characters ending with the null character ‘\0’) as the default string representation, Redis uses SDS, which is an acronym for Simple Dynamic String.</li>
</ol>
<p>(1) SDS structure</p>
<p><img src="http://56ssc.cc/2020/11/13/Introduction to Redis(2)/9.jpg" alt></p>
<p>(2) Comparison of SDS and C strings<br>The addition of the free and len fields to the C string by SDS brings a number of benefits.</p>
<p>Get string length: SDS is O(1), C string is O(n)<br>Buffer overflow: When using the API for C string, if the string length increases (e.g., strcat operation) and memory is forgotten to be reallocated, it is easy to cause a buffer overflow; while SDS records the length, the corresponding API will automatically reallocate memory when it may cause a buffer overflow, thus eliminating buffer overflow.<br>Memory reallocation when modifying a string: For C string, if you want to modify the string, you must reallocate memory (release first and then apply), because if you don’t reallocate, the memory buffer overflow will be caused when the string length increases, and the memory leak will be caused when the string length decreases. As for SDS, since len and free can be recorded, the correlation between string length and space array length is relieved, and can be optimized on this basis: the space pre-allocation policy (i.e., allocating more memory than actually needed) reduces the probability of reallocating memory when string length increases; the inert space release policy reduces the probability of reallocating memory when string length decreases. Significantly reduced.<br>Accessing binary data: SDS is OK, C string is not. Because the C string identifies the end of a string as a null character, and for some binary files (such as images), the contents may include an empty string, so the C string cannot be accessed correctly; SDS identifies the end of a string as a string length len, so there is no such problem.<br>In addition, since the buf in SDS still uses the C string (i.e. ends with ‘\0’), SDS can use some of the functions in the C string library; however, it should be noted that it can only be used when SDS is used to store text data, but not when storing binary data (‘\0’ is not necessarily the end of the string).</p>
<p>(3) Application of SDS to C-strings<br>Redis always uses SDS instead of C string when storing objects. For example, with the set hello world command, hello and world are both stored as SDS. The sadd myset member1 member2 member3 command stores both the key (“myset”) and the elements in the set (“member1”, “member1”). “member2” and “member3”), both of which are stored in the form of SDS. In addition to storing objects, SDS is also used to store various buffers.</p>
<p>The C string is only used if the string does not change, such as when printing a log.</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Redis(3)</title>
    <url>/2020/11/21/Introduction%20to%20Redis(3)/</url>
    <content><![CDATA[<h2 id="Redis-object-types-and-internal-coding"><a href="#Redis-object-types-and-internal-coding" class="headerlink" title="Redis object types and internal coding"></a>Redis object types and internal coding</h2><p>Redis supports five object types, and each structure has at least two encodings; this has the advantage of separating the interface from the implementation, so that when the internal encoding needs to be added or changed, the user is not affected, and on the other hand, the internal encoding can be switched according to different scenarios to improve efficiency.</p>
<p>With respect to the conversion of Redis internal encodings, the following rule applies: the encoding conversion is done when Redis writes data, and the conversion process is not reversible, only small-memory encodings can be converted to large-memory encodings.</p>
<ol>
<li>String<br>(1) Overview<br>Strings are the most basic type, since all keys are strings and several other elements of complex types other than strings are also strings.</li>
</ol>
<p>The length of a string cannot exceed 512 MB.</p>
<p>(2) Internal encoding<br>There are three types of internal coding for string types and they are used in the following scenarios.</p>
<p>int: 8 bytes long integer. When the string value is integer, the value is represented by a long integer.<br>embstr: &lt;= 39 bytes string. embstr and raw both use redisObject and sds to store data, the difference is that embstr uses only one memory allocation (so redisObject and sds are continuous), while raw needs two memory allocations (redisObject and (sds allocates space). So the advantages of embstr over raw are less space allocated once when creating, less space released when deleting, and the ease of finding all of the object’s data linked together. The downside of embstr is also obvious: if the length of a string increases and memory needs to be reallocated, the entire redisObject and sds will need to be reallocated, so embstr is implemented as read-only in redis.<br>raw: strings larger than 39 bytes<br>An example is shown in the figure below.</p>
<p><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/1.jpg" alt><br>The length of embstr and raw to distinguish is 39; because the length of redisObject is 16 bytes, and the length of sds is 9 + string length; therefore, when the string length is 39, the length of embstr is exactly 16 + 9 + 39 = 64, and jemalloc can allocate exactly 64 bytes of memory units.</p>
<p>(3) encoding conversion<br>When the int data is no longer an integer, or the size exceeds the range of the long, it is automatically converted to raw.</p>
<p>As for embstr, because its implementation is read-only, when the embstr object is modified, it will be converted to raw first and then modified, so as long as the embstr object is modified, the modified object must be raw, regardless of whether it reaches 39 bytes. An example is shown in the following figure.</p>
<p><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/2.jpg" alt></p>
<ol start="2">
<li>List<br>(1) Overview<br>Lists are used to store multiple ordered strings, each string is called an element; a list can store 2^32-1 elements.Lists in Redis support insertion and popup at both ends, and can get elements at a specified location (or range), and can act as arrays, queues, stacks, and so on.</li>
</ol>
<p>(2) Internal coding<br>The internal encoding of the list can be either a ziplist or a linkedlist.</p>
<p>A double-ended linked list consists of a list structure and multiple listNode structures; a typical structure is shown in the following figure.</p>
<p><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/3.jpg" alt></p>
<p>As you can see from the diagram, the double-ended chain table holds both header and tail pointers, and each node has pointers to the front and back; the length of the list is saved in the chain table; and dup, free, and match set type-specific functions for the node values, so the chain table can be used to save a variety of different types of values. And each node in the chain table points to a redisObject of type string.</p>
<p>Compressed list: Compressed list is a sequential data structure consisting of a series of specially coded sequential blocks (instead of a double-ended table where each node is a pointer) to save memory. Compared with double-ended chain table, compressed list can save memory space, but the complexity is higher when modify or add or delete operations; therefore, when the number of nodes is small, you can use compressed list; but when the number of nodes is large, it is cost-effective to use double-ended chain table.</p>
<p>The compressed list is not only used to implement the list, but also used to implement the hash, ordered list; very widely used.</p>
<p>(3) encoding conversion<br>A compressed list is used only if both of the following two conditions are met: the number of elements in the list is less than 512; and all string objects in the list are less than 64 bytes. If one of the conditions is not met, a double-ended list is used; and encoding is only possible from a compressed list to a double-ended chained list, not the other way around.</p>
<p>The following diagram illustrates the characteristics of list encoding conversion.<br><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/4.jpg" alt></p>
<p>where a single string cannot exceed 64 bytes to facilitate a uniform distribution of the length of each node; here, 64 bytes is the length of the string, excluding the SDS structure, because compressed lists use continuous, fixed-length blocks of memory to store strings and do not require the SDS structure to specify the length. The compressed list mentioned later will also emphasize the length of no more than 64 bytes, similar to the principle here.</p>
<p>3、Hash<br>(1) Overview<br>A hash (as a data structure) is not only one of the five object types provided by redis (along with strings, lists, collections, and ordered combinations), it is also the data structure used by Redis as a Key-Value database. For ease of explanation, when the term “inner hash” is used later in this document, it refers to one of the five object types provided by redis; when the term “outer hash” is used, it refers to the data structure used by Redis as the Key-Value database. The data structure.</p>
<p>(2) Internal coding<br>The inner hash uses an internal code that can be either a ziplist or a hashtable; the outer hash of Redis uses only the hashtable.</p>
<p>The compressed list was described earlier. Compared to the hashtable, the compressed list is used for scenarios where the number of elements is small and the length of the elements is small; its advantage is that it is stored centrally, which saves space; at the same time, although the complexity of the operations on the elements is changed from O(1) to O(n), there is no significant disadvantage in the time required for the operations because the number of elements in the hash is small.</p>
<p>hashtable: a hashtable consists of a dict structure, two dictht structures, an array of dictEntry pointers (called a bucket), and multiple dictEntry structures.</p>
<p>The relationship between the parts under normal circumstances (i.e., when the hashtable is not rehashing) is shown in the following diagram.</p>
<p><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/5.jpg" alt></p>
<p>The following describes the components in order from the bottom up.</p>
<p>dictEntry</p>
<p>The dictEntry structure is used to hold key-value pairs and is defined as follows.</p>
<p><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/6.jpg" alt></p>
<p>The functions of each of these attributes are as follows.</p>
<p>key: the key of the key-value pair.<br>val: the value in a key-value pair, implemented using union (i.e., a common body), which may store either a pointer to a value, a 64-bit integer, or an unsigned 64-bit integer.<br>next: point to the next dictEntry for hash conflict resolution<br>On 64-bit systems, a dictEntry object takes up 24 bytes (8 bytes each for key/val/next).</p>
<p>dictEntry</p>
<p>The size of the bucket array in redis is calculated according to the following rules: greater than dictEntry, the smallest 2^n; for example, if there are 1000 dictEntry, then the bucket size is 1024; if there are 1500 dictEntry, then the bucket size is 2048.</p>
<p>dictht</p>
<p>The dictht structure is as follows.</p>
<p><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/7.jpg" alt></p>
<p>The function of each of these attributes is described below.</p>
<p>The table attribute is a pointer to the bucket.<br>The size attribute records the size of the hash table, i.e., the size of the bucket.<br>used records the number of dictEntry’s that have been used.<br>The value of the sizemask attribute is always size-1. This attribute, along with the hash value, determines where a key is stored in the table.<br>sizemask</p>
<p>In general, by using the dictht and dictEntry structures, it is possible to implement the functionality of a normal hash table; however, in the Redis implementation, there is a dict structure on top of the dictht structure. The following is the definition and function of the dict structure.</p>
<p>The dict structure is as follows.<br><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/8.jpg" alt></p>
<p>The type attribute and the privdata attribute are used to accommodate different types of key-value pairs and are used to create polymorphic dictionaries.</p>
<p>The ht attribute and the trehashidx attribute are used for rehash, i.e., when the hash table needs to be expanded or contracted. ht is an array containing two items, each of which points to a dictht structure, which is why Redis hashes will have one dict and two dictht structures. Normally, all the data is in ht[0] of the put dict, and ht[1] is only used in the rehash. dict does the rehash operation by rehashing all the data in ht[0] into ht[1]. It then assigns ht[1] to ht[0] and empties ht[1].</p>
<p>Thus, the reason why the hash in Redis has a dict structure in addition to the dictht and dictEntry structure is to accommodate different types of key-value pairs on the one hand and rehash on the other.</p>
<p>(3) Encoding conversion<br>As mentioned earlier, the inner hash in Redis may use either a hash table or a compressed list.</p>
<p>A compressed list is used only if both of the following two conditions are met: the number of elements in the hash is less than 512; and the length of the key and value strings for all key-value pairs in the hash is less than 64 bytes. If one of the conditions is not met, the hash table is used; and the encoding can only be converted from a compressed list to a hash table, not the other way around.</p>
<p>The following diagram illustrates the features of the hash encoding conversion within Redis.</p>
<p><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/9.jpg" alt></p>
<ol start="4">
<li>Assembly<br>(1) Overview<br>A set is similar to a list in that it is used to store multiple strings, but there are two differences between a set and a list: the elements in a set are unordered, so you can’t manipulate the elements by indexing them; and the elements in a set can’t be duplicated.</li>
</ol>
<p>A collection can store up to 2^32-1 elements; in addition to the usual add/drop support, Redis also supports multiple sets of intersection, concatenation, differential set.</p>
<p>(2) Internal coding<br>The internal encoding of a collection can be either an inset or a hashtable.</p>
<p>The hash table has already been discussed, so I’ll skip it here; it should be noted that when you use a hash table, the values are all set to null.</p>
<p>The structure of an integer set is defined as follows.<br><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/10.jpg" alt><br>The encoding represents the type of content stored in the content. Although the content is of type int8_t, it actually stores the value of int16_t, int32_t or int64_t, which is determined by encoding; length represents the number of elements.</p>
<p>The integer set is suitable for when all elements of the set are integers and the number of elements in the set is small, compared to a hash table, the advantage of integer sets is centralized storage, saving space; at the same time, although the complexity of the operation of the elements from O (1) to O (n), but due to the smaller number of sets, the operation of the time is not a significant disadvantage.</p>
<p>(3) Code conversion<br>Sets use integer sets only if both of the following conditions are met: the number of elements in the set is less than 512; all elements in the set are integer values. If one of the conditions is not satisfied, a hash table is used; and the encoding can only be converted from an integer set to a hash table, not the other way around.</p>
<p>The following diagram illustrates the characteristics of the set encoding conversion.<br><img src="http://56ssc.cc/2020/11/21/Introduction to Redis(3)/11.jpg" alt></p>
<ol start="5">
<li>Orderly assembly<br>(1) Overview<br>An ordered set is the same as a set, the elements cannot be duplicated; however, unlike a set, the elements in an ordered set are ordered. With the list of index subscript as the basis for sorting different, ordered collection for each element to set a score (score) as the basis for sorting.</li>
</ol>
<p>(2) Internal coding<br>The internal encoding of an ordered collection can be either a compressed list (ziplist) or a jump table (skiplist). ziplist is used in both lists and hashes and has already been discussed, so I’ll skip it here.</p>
<p>A jump table is an ordered data structure that enables fast access to nodes by maintaining multiple pointers to other nodes in each node. In addition to jump tables, another typical implementation of an ordered data structure is a balanced tree; in most cases, the efficiency of jump tables is comparable to that of balanced trees, and they are much simpler to implement than balanced trees, so they are used instead of balanced trees in redis. It supports the average O(logN) and worst O(N) complex points for node lookup, and also supports sequential operations. The specific structure is relatively complex, slightly.</p>
<p>(3) Code Conversion<br>A compressed list is used only if both of the following two conditions are met: the number of elements in the ordered set is less than 128; and all members of the ordered set are less than 64 bytes in length. If one of these conditions is not met, a jump table is used; and encoding is only possible from a compressed list to a jump table, not the other way around.</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Redis</title>
    <url>/2020/11/01/Introduction%20to%20Redis/</url>
    <content><![CDATA[<h2 id="What-is-Redis-and-why-so-soon"><a href="#What-is-Redis-and-why-so-soon" class="headerlink" title="What is Redis and why so soon?"></a>What is Redis and why so soon?</h2><p>Redis is called REmote DIctionary Server, and you can see from the name that it uses a dictionary structure to store data, that is, key-value type data.</p>
<p>According to the official data, Redis can process up to 100,000 requests per second.</p>
<p>Why is it so fast?</p>
<p>Redis is written in ANSI C, which is the same language as SQLite. The advantage of writing in C is that the underlying code execution is efficient and has few dependencies, since libraries developed in C do not have many runtime dependencies, and the system is compatible and stable.</p>
<p>In addition, Redis is a memory-based database, which, as we’ve talked about before, avoids disk I/O, hence Redis is also known as a caching tool.</p>
<p>Redis uses Key-Value to store the data, which means it uses a hash structure to operate, and the complexity of data operation is O(1).</p>
<p>It uses a single-process, single-threaded model, which has the advantage of avoiding context switching and unnecessary resource competition between threads.</p>
<p>Technically Redis also uses multiple I/O multiplexing. Here, multiplexing refers to multiple socket network connections, and multiplexing refers to multiplexing the same thread. The advantage of multiple I/O multiplexing is that multiple I/O requests can be handled in the same thread, minimizing network I/O consumption and increasing efficiency.</p>
<h2 id="Redis-Data-Types"><a href="#Redis-Data-Types" class="headerlink" title="Redis Data Types"></a>Redis Data Types</h2><p>The data types supported by Redis include strings, hashes, lists, collections, ordered collections, etc. The most basic data types provided by Redis are strings, hashes, lists, collections, and ordered collections.</p>
<p>The string type is the most basic data type provided by Redis, and the corresponding structure is key-value.</p>
<p>If we want to set the value of a key, we can use set key value, for example, if we want to set the value of name to test, we can write set name test, if we want to get the value of a key, we can use get key, for example, if we want to get the value of name, we can write get name.</p>
<p>A hash provides a mapping between fields and field values, and the corresponding structure is key-field-value.</p>
<p>If we want to set the hash value of a key, we can use hset key field value, and if we want to set username to t and age to 28 for user1, we can write something like this.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hset user1 username zhangfei</span><br><span class="line">hset user1 age 28</span><br></pre></td></tr></table></figure>

<p>If you want to get the value of a field for a key, you can use hget key field, for example, if you want to get the username of user1, then just write <code>hget user1 username</code>.</p>
<p>If we want to set multiple field-values to a key key at the same time, we can use hmset key field value [field value…] For example, the one above could be written as.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Hmset user1 username zhangfei age 28</span><br></pre></td></tr></table></figure>

<p>If you want to get the value of a field for a key, you can use hget key field, e.g. if you want to get the username of user1, then just write hget user1 username.</p>
<p>If you want to get multiple field values for a key at once, you can use hget key field[field…]. If you want to get the username and age of user1, for example, you can write hmget user1 username age.</p>
<p><img src="http://56ssc.cc/2020/11/01/Introduction to Redis/1.jpg" alt></p>
<p>Underlying the list is a two-way chain structure, so we can add elements to both ends of the list with O(1) time complexity, and we can also retrieve a fragment of the list.</p>
<p>If we want to add elements to the left side of the list we can use: LPUSH key value […] For example, if we add zhangfei, guanyu and liubei elements to the left side of the heroList, we can write.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">LPUSH heroList zhangfei guanyu liubei</span><br></pre></td></tr></table></figure>

<p>Similarly, we can also use the RPUSH key value […] To add elements to the right side of the list, for example, if we add two elements dianwei and lvbu to the right side of the heroList list, we can write something like this.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">RPUSH heroList dianwei lvbu</span><br></pre></td></tr></table></figure>

<p>If we want to get the content of a segment in the list, just use the LRANGE key start stop, e.g. if we want to get the heroList from 0 to 4 position, just write LRANGE heroList 0 4.</p>
<p><img src="http://56ssc.cc/2020/11/01/Introduction to Redis/2.jpg" alt></p>
<p>A set is an unordered collection of strings. It differs from a list in that the elements in the collection are unordered and the elements cannot be duplicated.</p>
<p>If you want to add elements to a set, you can use the SADD key member […] For example, if we add the elements zhangfei, guanyu, libubei, dianwei and lvbu to our heroSet collection, we can write.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SADD heroSet zhangfei guanyu liubei dianwei lvbu</span><br></pre></td></tr></table></figure>

<p>If you want to remove an element from a collection, you can use the SREM key member […] For example, if we want to remove the elements liubei and lvbu from the heroSet collection, we can write.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SREM heroSet liubei lvbu</span><br></pre></td></tr></table></figure>

<p>If we want to get all the elements in the set, we can use the SMEMBERS key, e.g. if we want to get all the elements in the heroSet set, we can write SMEMBERS heroSet.</p>
<p>If we want to determine if an element exists in the set, we can use the SISMEMBERS key member, e.g. if we want to determine if zhangfei and liubei exist in the heroSet set, we can write SMEMBERS as follows.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SISMEMBER heroSet zhangfei</span><br><span class="line">SISMEMBER heroSet liubei</span><br></pre></td></tr></table></figure>

<p><img src="http://56ssc.cc/2020/11/01/Introduction to Redis/3.jpg" alt></p>
<p>We can understand an ordered string collection as an upgrade to a collection. In fact, ZSET adds a score attribute to the set, which can be specified when you add a modified element. Each time it is specified, ZSET is automatically sorted by score, which means that we can specify the score when we add a member to the set key.</p>
<p>An ordered collection has some similarities to a list, in that both data types are ordered and both have access to a range of elements. But it is very different in the data structure of the two, the first list list is achieved by a two-way chain table, in the operation of the left and right side of the data will be very fast, and for the middle of the data operation is relatively slow. Ordered collection using the hash table structure to achieve, read sorted in the middle part of the data will also be fast. Also the ordered set can be adjusted by score, but if we want to adjust the position of the elements of the list, it is more difficult.</p>
<p>If we want to add elements and scores to an ordered set, use the ZADD key score member […] Let’s say we add the hp_max values of the following 5 heroes to the heroScore collection, as shown in the following table.</p>
<p><img src="http://56ssc.cc/2020/11/01/Introduction to Redis/4.jpg" alt></p>
<p>Then we could write something like this.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ZADD heroScore 8341 zhangfei 7107 guanyu 6900 liubei 7516 dianwei 7344 lvbu</span><br></pre></td></tr></table></figure>

<p>If we want to get the score of an element, we can use the ZSCORE key member, e.g. we want to get the score of guanyu, just write ZSCORE heroScore guanyu.</p>
<p>If we want to remove one or more elements, we can use ZREM key member [member …], for example if we want to remove the element guanyu, just use ZREM heroScore guanyu.</p>
<p>We can also get a list of elements in a range. Use the ZRANGE key start stop [WITHSCORES] if you want to sort the scores from smallest to largest, or the ZREVRANGE key start stop [WITHSCORES] if you want to sort the scores from largest to smallest. It should be noted that WITHSCORES is optional, if you use WITHSCORES it will show the scores together, e.g. if we want to find the top 3 heroes with scores in the ordered set heroScore and their values, write ZREVRANGE heroScore 0 2 WITHSCORES! .</p>
<p><img src="http://56ssc.cc/2020/11/01/Introduction to Redis/5.jpg" alt></p>
<p>In addition to these five data types, Redis also supports Bitmaps data structures, adding HyperLogLog after version 2.8, Geospatial and index radius queries after version 3.2, and Streams data type.</p>
<h2 id="How-to-use-Redis"><a href="#How-to-use-Redis" class="headerlink" title="How to use Redis"></a>How to use Redis</h2><p>We can manipulate Redis directly in Python, and we need to use the pip install redis install kit to reference it before using it, once installed, we need to use import redis before using it.</p>
<p>There are two ways to connect to Redis in Python, the first of which is directly, using the following line of commands.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = redis.Redis(host=<span class="string">'localhost'</span>, port= <span class="number">6379</span>)</span><br></pre></td></tr></table></figure>

<p>The second is the connected pool approach.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool = redis.ConnectionPool(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>)</span><br><span class="line">r = redis.Redis(connection_pool=pool)</span><br></pre></td></tr></table></figure>

<p>Normally, when we connect to Redis, we can create a Redis connection, perform Redis operations through it, and then release it when it’s done. However, in a high concurrency situation, this is very uneconomical because each connection and release consumes a lot of resources.</p>
<h2 id="Why-use-a-connection-pool-mechanism"><a href="#Why-use-a-connection-pool-mechanism" class="headerlink" title="Why use a connection pool mechanism"></a>Why use a connection pool mechanism</h2><p>Redis provides a connection pooling mechanism that allows us to create multiple connections in advance, place them in the connection pool, and fetch them directly from the connection pool when we need to perform Redis operations.</p>
<p>The connection pooling mechanism eliminates the need to create and release connections, improving overall performance.</p>
<h2 id="Principles-of-the-connection-pool-mechanism"><a href="#Principles-of-the-connection-pool-mechanism" class="headerlink" title="Principles of the connection pool mechanism"></a>Principles of the connection pool mechanism</h2><p>n the instance of the connection pool there will be two lists, held in _available_connections and _in_use_connections, which represent the set of connections that are available in the connection pool and the set of connections that are in use, respectively. When we want to create a connection, we can get a connection from _available_connections to use and put it in _in_use_connections. If there are no available connections, only then will we create a new connection and place it in _in_use_connections. If the connection is exhausted, it is removed from _in_use_connections and added to _available_connections for subsequent use.</p>
<p>The Redis library provides the Redis and StrictRedis classes, which both implement Redis commands, with the difference that Redis is a subclass of StrictRedis and is compatible with older versions. If we wanted to use the connection pooling mechanism and then instantiate it with StrictRedis, we could write something like this.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis </span><br><span class="line">pool = redis.ConnectionPool(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>)</span><br><span class="line">r = redis.StrictRedis(connection_pool=pool)</span><br></pre></td></tr></table></figure>

<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>In practice, we often use RDBMS and Redis together to complement each other.</p>
<p>As a common NoSQL database, Redis supports much richer data types than Memcached. In terms of I/O performance, Redis adopts a single-threaded I/O multiplexing model, while Memcached is multi-threaded and can take advantage of multi-core. In terms of persistence, Redis provides two persistence modes, which can keep data permanently, which is not available in Memcached.</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>JavaScript common technical summary</title>
    <url>/2020/08/01/JavaScript%20common%20technical%20summary/</url>
    <content><![CDATA[<p>Develop, read, and learn about some of the knowledge points encountered and organized.</p>
<h1 id="Several-implementations-of-JavaScript"><a href="#Several-implementations-of-JavaScript" class="headerlink" title="Several implementations of JavaScript"></a>Several implementations of JavaScript</h1><h2 id="Global-variable"><a href="#Global-variable" class="headerlink" title="Global variable"></a>Global variable</h2><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span>  count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">const</span>  countUp = <span class="function"><span class="params">()</span> =&gt;</span>  count++;</span><br></pre></td></tr></table></figure>

<h2 id="Closure"><a href="#Closure" class="headerlink" title="Closure"></a>Closure</h2><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// javascript</span></span><br><span class="line"><span class="keyword">const</span>  countUp = <span class="function">(<span class="params">(</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="keyword">let</span>  count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ++count;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;)();</span><br><span class="line"><span class="built_in">console</span>.log(countUp()); <span class="comment">// 1</span></span><br><span class="line"><span class="built_in">console</span>.log(countUp()); <span class="comment">// 2</span></span><br></pre></td></tr></table></figure>

<h2 id="Function-property"><a href="#Function-property" class="headerlink" title="Function property"></a>Function property</h2><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// javascript</span></span><br><span class="line"><span class="keyword">let</span>  countUp = <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> ++countUp.count;</span><br><span class="line">&#125;;</span><br><span class="line">countUp.count = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">console</span>.log(countUp()); <span class="comment">// 1</span></span><br><span class="line"><span class="built_in">console</span>.log(countUp()); <span class="comment">// 2</span></span><br></pre></td></tr></table></figure>

<h1 id="Function-properties-TS"><a href="#Function-properties-TS" class="headerlink" title="Function properties (TS)"></a>Function properties (TS)</h1><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">interface  Counter &#123;</span><br><span class="line">(): <span class="keyword">void</span>; <span class="comment">// Here the structure Counter must be defined to contain a function, which is required to have no parameters and a return value of void, i.e. no return value</span></span><br><span class="line">count: number; <span class="comment">// And the structure must also contain an attribute named count, with a value of type number.</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">const</span>  getCounter = (): <span class="function"><span class="params">Counter</span>  =&gt;</span> &#123; <span class="comment">// A function is defined here to return this counter.</span></span><br><span class="line">    <span class="keyword">const</span>  c = <span class="function"><span class="params">()</span> =&gt;</span> &#123; <span class="comment">// Define a function with the same logic as in the previous example.</span></span><br><span class="line">        c.count++;</span><br><span class="line">    &#125;;</span><br><span class="line">    c.count = <span class="number">0</span>; <span class="comment">// Add a count attribute to this function with an initial value of 0.</span></span><br><span class="line">    <span class="keyword">return</span>  c; <span class="comment">// Finally, this function object is returned</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span>  counter: Counter = getCounter(); <span class="comment">// Get this counter from the getCounter function</span></span><br><span class="line">counter();</span><br><span class="line"><span class="built_in">console</span>.log(counter.count); <span class="comment">// 1</span></span><br><span class="line">counter();</span><br><span class="line"><span class="built_in">console</span>.log(counter.count); <span class="comment">// 2</span></span><br></pre></td></tr></table></figure>

<h1 id="Front-end-voice"><a href="#Front-end-voice" class="headerlink" title="Front-end voice"></a>Front-end voice</h1><p>Voice broadcast: in the project you need to return the message of the ajax request for voice broadcast, str for the return of the data.</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span>  <span class="title">voiceAnnouncements</span>(<span class="params">str</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span>  url = <span class="string">"http://tts.baidu.com/text2audio?lan=zh&amp;ie=UTF-8&amp;text="</span> + <span class="built_in">encodeURI</span>(str);</span><br><span class="line">    <span class="keyword">var</span>  n = <span class="keyword">new</span>  Audio(url);</span><br><span class="line">    n.src = url;</span><br><span class="line">    n.play();</span><br><span class="line">&#125;</span><br><span class="line">voiceAnnouncements(<span class="string">'Hello World!'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Use-a-Proxy-to-implement-a-data-binding-and-listen-to-the"><a href="#Use-a-Proxy-to-implement-a-data-binding-and-listen-to-the" class="headerlink" title="Use a Proxy to implement a data binding and listen to the"></a>Use a Proxy to implement a data binding and listen to the</h2><p>Proxy Profile.</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span>  p = <span class="keyword">new</span>  <span class="built_in">Proxy</span>(target, handler);</span><br><span class="line"><span class="comment">// `target` Represents the object to which you need to add a proxy.</span></span><br><span class="line"><span class="comment">// `handler` Used to customize the operations in the object.</span></span><br></pre></td></tr></table></figure>

<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span>  onWatch = <span class="function">(<span class="params">obj, setBind, getLogger</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="keyword">let</span>  handler = &#123;</span><br><span class="line">        <span class="keyword">get</span>(target, property, receiver) &#123;</span><br><span class="line">            getLogger(target, property)</span><br><span class="line">            <span class="keyword">return</span>  <span class="built_in">Reflect</span>.get(target, property, receiver);</span><br><span class="line">        &#125;,</span><br><span class="line"></span><br><span class="line">        <span class="keyword">set</span>(target, property, value, receiver) &#123;</span><br><span class="line">            setBind(value);</span><br><span class="line">            <span class="keyword">return</span>  <span class="built_in">Reflect</span>.set(target, property, value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span>  <span class="keyword">new</span>  <span class="built_in">Proxy</span>(obj, handler);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span>  obj = &#123; <span class="attr">a</span>:  <span class="number">1</span> &#125;</span><br><span class="line"><span class="keyword">let</span>  value</span><br><span class="line"><span class="keyword">let</span>  p = onWatch(obj, (v) =&gt; &#123;</span><br><span class="line">    value = v</span><br><span class="line">&#125;, (target, property) =&gt; &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">`Get '<span class="subst">$&#123;property&#125;</span>' = <span class="subst">$&#123;target[property]&#125;</span>`</span>);</span><br><span class="line">&#125;)</span><br><span class="line">p.a = <span class="number">2</span>  <span class="comment">// bind `value` to `2`</span></span><br><span class="line">p.a  <span class="comment">// -&gt; Get 'a' = 2</span></span><br></pre></td></tr></table></figure>

<h1 id="Let’s-talk-about-closing-the-bag"><a href="#Let’s-talk-about-closing-the-bag" class="headerlink" title="Let’s talk about closing the bag."></a>Let’s talk about closing the bag.</h1><h2 id="Definition-of-first-class-citizenship"><a href="#Definition-of-first-class-citizenship" class="headerlink" title="Definition of first-class citizenship"></a>Definition of first-class citizenship</h2><p>  In programming languages, first-class citizens can be used as function arguments, as function return values, and as assignments to variables. For example, strings are first-class citizens in almost all programming languages, strings can be used as function arguments, strings can be used as function return values, and strings can be assigned to variables. For various programming languages, functions are not necessarily first-class citizens anymore, such as versions before Java 8. For JavaScript, functions can be assigned to variables, as function arguments, or as function return values, so functions are first-class citizens in JavaScript.</p>
<h2 id="Dynamic-vs-static-scopes"><a href="#Dynamic-vs-static-scopes" class="headerlink" title="Dynamic vs. static scopes"></a>Dynamic vs. static scopes</h2><p>Note that it says scopes, not types.<br>  If the scope of a language is a static scope, then the referential relationships between symbols can be clearly determined at compile time based on the program code, and will not change at run time. Where a function is declared, it has the scope of the scope in which it is located. It can access which variables, then bound to these variables, at runtime has always been able to access these variables. That is, static scopes can be determined by the program code, at compile time can be completely determined. Most languages are statically scoped.<br>  Dynamic Scope (DSC). That is, variable references are not tied to variable declarations at compile time. It is dynamically looking for a variable with the same name in the running environment at runtime. The bash scripting language, as used in macOS or Linux, is dynamically scoped.</p>
<h2 id="Closure-1"><a href="#Closure-1" class="headerlink" title="Closure"></a>Closure</h2><p>  The inherent contradiction of a closure is between the runtime environment and the scope at definition time. So we take the variables we need in our internal environment, package them up to the closure function, and it can access them at any time.</p>
<p>  The concept of a closure is a challenge for beginners. In fact, closure is the function in the static scope of the variables accessed in the life of the long, forming a function can be accessed by the data alone. Because the data can only be accessed by the function closed package, so it also has a package of information, hiding the internal details of the characteristics.</p>
<h2 id="Closures-and-Object-Orientation"><a href="#Closures-and-Object-Orientation" class="headerlink" title="Closures and Object-Orientation"></a>Closures and Object-Orientation</h2><p>  Does that sound familiar? Encapsulation, sealing the data and the operations on the data together, that’s object-oriented programming! A closure can be viewed as an object. On the other hand, can an object be viewed as a closure as well? An object’s attributes, which can also be viewed as environment variables that are exclusive to the method, must also have a lifespan to ensure that they can always be accessed properly by the method.</p>
<h2 id="Implementation-of-closed-packets"><a href="#Implementation-of-closed-packets" class="headerlink" title="Implementation of closed packets"></a>Implementation of closed packets</h2><p>Functions have to become first-class citizens of JavaScript. That is, they have to be able to assign functions to variables like normal values, which can be passed as arguments to other functions, and which can be used as the return value of the function.<br>To have the inner function always access variables in its environment, regardless of whether the outer function exits or not.</p>
<h2 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h2><ol>
<li><p>Because JavaScript is statically scoped, the variables needed in its internal environment are determined at compile time and do not change at runtime.</p>
</li>
<li><p>And because functions are first-class citizens in JavaScript, and can be called, passed as arguments, assigned to variables, or returned as functions, their runtime environment can easily change.</p>
</li>
<li><p>When a function returns as the return value of another function (the outer function), the variables in its outer function have been popped from the call stack, but we must make the variables it needs accessible to the inner function, thus creating a contradiction between the runtime environment and the scope at definition time.</p>
</li>
<li><p>So we take the variables we need in our internal environment and package them to the inner function (the closure function), which can access them at any time, creating a closed package.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title>Logistic regression LR</title>
    <url>/2020/12/09/Logistic%20regression%20LR/</url>
    <content><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Logistic regression is a machine learning algorithm that is very popular in interviews, because on the surface it is formally very simple and easy to master, but when asked, it is easy to be confused. So the first advice to everyone in the interview is not to say they are proficient in logistic regression, it is very easy to be asked down, thus reducing the score. Here is a summary of some of the problems I usually encounter when interviewing others and being interviewed as an interviewer.</p>
<h3 id="Formal-introduction"><a href="#Formal-introduction" class="headerlink" title="Formal introduction"></a>Formal introduction</h3><p>How to highlight that you are a person who already knows a lot about logistic regression. That is to summarize it in one sentence! Logistic regression assumes that the data obeys Bernoulli distribution, and uses gradient descent to solve the parameters by maximizing the likelihood function to achieve the purpose of dichotomizing the data.</p>
<p>This actually contains 5 points </p>
<p>1: the assumptions of logistic regression, </p>
<p>2: the loss function of logistic regression, </p>
<p>3: the solution method of logistic regression, </p>
<p>4: the purpose of logistic regression, </p>
<p>5: how to classify logistic regression. </p>
<p>These questions are to assess your basic understanding of logistic regression.</p>
<h4 id="Basic-assumptions-of-logistic-regression"><a href="#Basic-assumptions-of-logistic-regression" class="headerlink" title="Basic assumptions of logistic regression"></a>Basic assumptions of logistic regression</h4><p>Any model has its own assumptions under which the model is applicable. The first basic assumption of logistic regression is that the data obeys the Bernoulli distribution. A simple example of the Bernoulli distribution is a coin toss, where the probability of a positive toss is 𝑝 and the probability of a negative toss is 1-𝑝. In the model of logistic regression, it is assumed that ℎ𝜃(𝑥) is the probability of a positive sample and 1-𝑝 probability and 1-ℎ𝜃(𝑥) is the probability that the sample is negative. Then the whole model can be described as</p>
<p><code>ℎ𝜃(𝑥;𝜃)=𝑝</code></p>
<p>The second hypothesis of logistic regression is to assume that the probability of the sample being positive is </p>
<p><code>𝑝=11+𝑒−𝜃𝑇𝑥</code></p>
<p>So the final form of logistic regression </p>
<p><code>ℎ𝜃(𝑥;𝜃)=1/1+𝑒−𝜃𝑇𝑥</code></p>
<h4 id="Loss-function-of-logistic-regression"><a href="#Loss-function-of-logistic-regression" class="headerlink" title="Loss function of logistic regression"></a>Loss function of logistic regression</h4><p>The loss function of logistic regression is its great likelihood function</p>
<p><code>𝐿𝜃(𝑥)=∏𝑖=1𝑚ℎ𝜃(𝑥𝑖;𝜃)𝑦𝑖∗(1-ℎ𝜃(𝑥𝑖;𝜃))1-𝑦𝑖</code></p>
<h4 id="Solution-method-of-logistic-regression"><a href="#Solution-method-of-logistic-regression" class="headerlink" title="Solution method of logistic regression"></a>Solution method of logistic regression</h4><p>Since this great likelihood function cannot be solved directly, we generally keep forcing the optimal solution by performing gradient descent on the function. There is actually a bonus point in this area to check your knowledge of other optimization methods. The interviewer may ask about the advantages and disadvantages of these three methods and how to choose the most suitable gradient descent method.</p>
<p>In simple terms, batch gradient descent will get the global optimal solution, the disadvantage is that when updating each parameter, you need to traverse all the data, the computation will be very large, and there will be a lot of redundant calculations, the result is that when the data volume is large, the update of each parameter will be very slow.</p>
<p>Stochastic gradient descent is updated frequently with high variance, which has the advantage of making sgd jump to new and potentially better local optimal solutions, and the disadvantage of making the process of convergence to local optimal solutions more complicated.</p>
<p>Small-batch gradient descent combines the advantages of sgd and batch gd by using n samples for each update. It reduces the number of parameter updates and can achieve more stable convergence results, and we generally use this method in deep learning.</p>
<p>In fact, there is a hidden deeper plus point here, depending on whether you understand optimization methods such as Adam, momentum method. Because the above methods actually have two fatal problems.<br>The first one is how to choose the right learning rate for the model. It is not appropriate to keep the same learning rate from the beginning to the end. At the beginning, when the parameters are just learning, the parameters are far away from the optimal solution, so it is necessary to keep a large learning rate to approach the optimal solution as soon as possible. However, when the parameters are learned later, the parameters and the optimal solution are already close to each other, and you still keep the initial learning rate, it is easy to cross the optimal point and oscillate back and forth near the optimal point, which, in layman’s terms, makes it easy to overlearn and deviate.<br>The second is how to choose the appropriate learning rate for the parameters. In practice, it is not reasonable to keep the same learning rate for each parameter. Some parameters are updated frequently, so the learning rate can be smaller. Some parameters are updated slowly, so the learning rate should be larger. We will not expand here, and I will dedicate a special topic to it sometime.</p>
<h4 id="Purpose-of-logistic-regression"><a href="#Purpose-of-logistic-regression" class="headerlink" title="Purpose of logistic regression"></a>Purpose of logistic regression</h4><p>The purpose of this function is to dichotomize the data to improve the accuracy.</p>
<h4 id="How-logistic-regression-is-classified"><a href="#How-logistic-regression-is-classified" class="headerlink" title="How logistic regression is classified"></a>How logistic regression is classified</h4><p>Logistic regression is a regression (i.e., the y-value is continuous), so how does it apply to classification. y-value is indeed a continuous variable. The practice of logistic regression is to delineate a threshold, and those with y-values greater than this threshold are in one category, and those with y-values less than this threshold are in another category. The threshold value is adjusted according to the actual situation. Generally, 0.5 will be chosen as the threshold for classification.</p>
<h3 id="Further-questions-about-logistic-regression"><a href="#Further-questions-about-logistic-regression" class="headerlink" title="Further questions about logistic regression"></a>Further questions about logistic regression</h3><p>Although logistic regression is formally very simple, its connotation is very rich. There are many questions that can be considered</p>
<p>Why does the loss function of logistic regression use the maximum likelihood function as the loss function?<br>There are generally four types of loss functions, squared loss function, logarithmic loss function, HingeLoss0-1 loss function, and absolute value loss function. Taking the logarithm of the great likelihood function is equivalent to the logarithmic loss function. In this model of logistic regression, the training of the logarithmic loss function solves the parameters faster. As for the reason you can find out the gradient update of this equation</p>
<p><code>𝜃𝑗=𝜃𝑗-(𝑦𝑖-ℎ𝜃(𝑥𝑖;𝜃))∗𝑥𝑖𝑗</code></p>
<p>The update rate of this equation is only related to 𝑥𝑖𝑗, 𝑦𝑖. It is independent of the gradient of the sigmod function itself. This way the update speed is more stable from the beginning to the end.<br>Why not choose the squared loss function? One reason is that if you use the squared loss function, you will find that the gradient update speed is very correlated with the gradient of the sigmod function itself. sigmod function has a gradient no greater than 0.25 in its domain of definition. this will make the training very slow.</p>
<h4 id="What-is-the-impact-of-logistic-regression-if-there-are-many-features-that-are-highly-correlated-or-if-a-feature-is-repeated-100-times-during-the-training-process"><a href="#What-is-the-impact-of-logistic-regression-if-there-are-many-features-that-are-highly-correlated-or-if-a-feature-is-repeated-100-times-during-the-training-process" class="headerlink" title="What is the impact of logistic regression if there are many features that are highly correlated or if a feature is repeated 100 times during the training process?"></a>What is the impact of logistic regression if there are many features that are highly correlated or if a feature is repeated 100 times during the training process?</h4><p>First of all, if the loss function eventually converges, even if there are a lot of highly correlated features, it will not affect the classifier’s performance.</p>
<p>But for the features themselves, suppose there is only one feature, and you repeat it 100 times without considering sampling. After training, the data is still the same, but the feature itself is repeated 100 times, which essentially divides the original feature into 100 parts, each of which is one percent of the original feature weight value.</p>
<p>If in the case of random sampling, in fact, after training and convergence, we can still think that these 100 features and the original one feature play the same effect, but may be the value of many features in the middle of the positive and negative elimination.</p>
<h4 id="Why-do-we-still-remove-highly-correlated-features-in-the-training-process"><a href="#Why-do-we-still-remove-highly-correlated-features-in-the-training-process" class="headerlink" title="Why do we still remove highly correlated features in the training process?"></a>Why do we still remove highly correlated features in the training process?</h4><p>Removing highly correlated features will make the model more interpretable<br>It can greatly improve the speed of training. If there are many highly correlated features in the model, even if the loss function itself converges, the parameters actually do not converge, which will slow down the training speed. Secondly, more features will increase the training time.</p>
<h3 id="Summary-of-the-advantages-and-disadvantages-of-logistic-regression"><a href="#Summary-of-the-advantages-and-disadvantages-of-logistic-regression" class="headerlink" title="Summary of the advantages and disadvantages of logistic regression"></a>Summary of the advantages and disadvantages of logistic regression</h3><p>When interviewing, people often ask what you feel when using logistic regression. What do you think are its advantages and disadvantages.</p>
<h4 id="Here-we-summarize-some-advantages-of-logistic-regression-applied-to-industry"><a href="#Here-we-summarize-some-advantages-of-logistic-regression-applied-to-industry" class="headerlink" title="Here we summarize some advantages of logistic regression applied to industry."></a>Here we summarize some advantages of logistic regression applied to industry.</h4><p>The simplicity of the form and the interpretability of the model are very good. You can see the impact of different features on the final results from the weights of the features, and if the weight of a feature is high, then this feature will have a greater impact on the final results.</p>
<p>The model effect is good. It is acceptable in engineering (as baseline), if the feature engineering is done well, the effect will not be too bad, and the feature engineering can be developed by everyone in parallel, which greatly accelerates the speed of development.</p>
<p>The training speed is fast. When classifying, the amount of computation is only related to the number of features. And the distributed optimization of logistic regression sgd development is more mature, the training speed can be further improved by heaps of machines, so we can iterate several versions of the model in a short time.</p>
<p>The resource consumption is small, especially memory. Because only the feature values of each dimension need to be stored.</p>
<p>It is easy to adjust the output results. Logistic regression makes it easy to get the final classification results because the output is the probability score of each sample, and we can easily cutoff these probability scores, that is, divide the thresholds (those greater than a certain threshold are one class, those less than a certain threshold are one class).</p>
<h4 id="But-logistic-regression-itself-has-many-drawbacks"><a href="#But-logistic-regression-itself-has-many-drawbacks" class="headerlink" title="But logistic regression itself has many drawbacks:"></a>But logistic regression itself has many drawbacks:</h4><p>The accuracy rate is not very high. Because the form is very simple (very similar to a linear model), it is difficult to fit the true distribution of the data.</p>
<p>It is difficult to deal with data imbalance. For example, if we have a problem with very unbalanced positive and negative samples, such as a ratio of 10000:1, we can predict all the samples as positive and make the loss function smaller. But as a classifier, it will not be able to distinguish between positive and negative samples very well.</p>
<p>It is more troublesome to deal with nonlinear data. Logistic regression can only deal with linearly separable data without introducing other methods, or furthermore, with dichotomous problems .<br>Logistic regression itself cannot filter features. Sometimes, we use gbdt to filter features and then logistic regression on it.</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Mathematical statistics</title>
    <url>/2020/10/16/Mathematical%20statistics/</url>
    <content><![CDATA[<p>Mathematical statistics are also indispensable in the study of artificial intelligence. Basic statistical theory helps to interpret the results of machine learning algorithms and data mining, and the value of the data can only be reflected if reasonable interpretations are made. Mathematical statistics (mathematical statistics) is the study of random phenomena based on data obtained from observations or experiments, and makes reasonable estimates and judgments about the objective laws of the object of study.</p>
<p>Although mathematical statistics is based on the theory of probability, there is a fundamental difference in approach between the two. The probability theory is based on the premise that the distribution of random variables is known, and the characteristics and patterns of random variables are analyzed according to the known distribution; the object of mathematical statistics is the unknown distribution of random variables, and the research method is to make repeated independent observations of the random variables and infer the original distribution according to the observation results.</p>
<p>To put it in a less precise but more intuitive way: mathematical statistics can be seen as reverse probability theory. To use the analogy of buying a lottery ticket, probability theory deals with determining the likelihood of a number winning based on the known pattern of the lottery draw, while mathematical statistics deals with guessing the pattern of the lottery draw with a certain degree of accuracy based on a record of previous winning/unwinning numbers, although the attempt is often fruitless.</p>
<p>In mathematical statistics, the available resource is a finite set of data called a sample. Accordingly, all possible values of an observation are called a population. The task of mathematical statistics is to infer the numerical characteristics of the population from the sample. Samples are usually obtained from many independent repeated observations of the population, which ensures that the different sample values are independent of each other and have the same distribution as the population.</p>
<p>In statistical inference, it is often not the sample itself that is applied, but a function of the sample known as the statistic. The statistic itself is a random variable that is used as a tool to make statistical inferences. The sample mean and the sample variance are the two most important statistics.</p>
<p><img src="http://56ssc.cc/2020/10/16/Mathematical statistics/1.jpg" alt></p>
<p>The basic problems of statistical inference can be divided into two broad categories: estimation theory and hypothesis test.</p>
<h2 id="Estimation-theory"><a href="#Estimation-theory" class="headerlink" title="Estimation theory"></a>Estimation theory</h2><p>Estimation theory is a method of estimating the overall distribution from randomly selected samples, which can be further divided into point estimation and interval estimation. When the function form of the overall distribution is known, but one or more parameters are not known, estimating the value of the unknown parameter with the help of a sample of the overall distribution is called point estimation of the parameter. The core of point estimation is to construct a suitable statistic θ^ and use the observed value of this statistic as an approximation of the unknown parameter θ. Specific methods of point estimation include the method of moments and maximum likelihood estimation.</p>
<p>Moments represent the characteristics of the distribution of a random variable, and the kth order moment is defined as the mean of the kth power of the random variable, i.e., E(Xk). The idea of moment estimation is to use the k-order moments of the sample to estimate the k-order moments of the total, based on the theory that the function of the sample moment converges almost everywhere to the corresponding function of the total moment, which means that when the sample size is large enough, the sample parameter can be used to get an approximation of the corresponding total parameter almost every time.</p>
<p>As opposed to moment estimation based on the law of large numbers, maximum likelihood estimation stems from the way the frequency school views probability. The intuitive understanding of maximum likelihood estimation is that since the sample is a set of existing sample values, it is assumed that the probability of getting this set of sample values is higher, and therefore the parameter θ needs to be estimated in such a way that the existing sample values have the highest probability of occurring.</p>
<p>In maximum likelihood estimation, the likelihood function is defined as the probability of occurrence of the sample observations, and the criterion for determining the unknown parameter is to maximize the value of the likelihood function, which is the problem of solving the maximum value of the function in calculus. Since the different sample values are independent of each other, the likelihood function can be written as a number of probability mass function / probability density function multiplication, and further transformed into a logarithmic equation for solution.</p>
<p>Moment estimation and maximum likelihood estimation represent two ideas for inferring the overall parameter, but for the same parameter, the estimates obtained by different estimation methods are likely to differ, which raises the question of how to evaluate the estimates. In practice, there are three basic criteria that are usually considered in the evaluation of an estimate.</p>
<ul>
<li>Unbiased: the mathematical expectation of the estimated quantity is equal to the true value of the unknown parameter.</li>
<li>Validity: the variance of the unbiased estimates is as small as possible.</li>
<li>Consistency: as the sample size approaches infinity, the estimated quantity converges probabilistically to the true value of the unknown parameter.</li>
</ul>
<p>These three requirements form the overall criteria for determining the point estimate. Unbiased means that given the sample values, the estimates obtained from the estimated quantities may be larger or smaller than the true values. However, if the construction of the estimated quantities is kept unchanged, and instead multiple resamplings are performed, each time with a new sample to calculate the estimated values, then the deviation of these estimated values from the true values of the unknown parameters is equal to zero in an average sense, which means that there is no systematic error.</p>
<p>Although deviations between the estimated and true values are unavoidable, the smaller the deviation in an individual sense means that the performance of the estimate is more accurate, and validity measures precisely the degree of deviation between the estimated and true values. The degree of deviation depends not only on how the estimate is constructed, but also on the size of the sample size, and consistency considers the effect of sample size. Consistency indicates that as the sample size increases, the value of the estimated quantity will stabilize at the true value of the unknown parameter. Estimates that do not have consistency will never estimate the unknown parameter with sufficient accuracy, and are therefore undesirable.</p>
<p>The criterion for discriminating the estimated quantity involves the effect of the estimation error, which is an equally important parameter as the estimated value. In the process of estimating the unknown parameter θ, in addition to deriving an estimate, an interval needs to be estimated and the degree of confidence that this interval contains the true value of θ needs to be determined. In mathematical statistics, this interval is called the confidence interval, and this type of estimation is called interval estimation.</p>
<p>Confidence intervals can be interpreted visually in the following way: If the total population is sampled repeatedly, each time with the same sample size, a confidence interval (θ-,θ¯) can be determined from each set of sample values, with the upper and lower bounds being the two statistics of the sample, representing the upper and lower confidence limits, respectively.</p>
<p>Two possibilities exist for each confidence interval: a true value that contains θ or a true value that does not. If you count the ratio of all confidence intervals that contain the true value of θ, the resulting ratio is the confidence level. Therefore, the interval estimation provides a further range and error limit on top of the point estimate, corresponding to the confidence interval and the confidence level, respectively.</p>
<h2 id="Hypothesis-test"><a href="#Hypothesis-test" class="headerlink" title="Hypothesis test"></a>Hypothesis test</h2><p>The object of estimation theory is some parameter of the aggregate, while the object of hypothesis test is some assertion about the aggregate, i.e., a hypothesis about the aggregate. The hypothesis in a hypothesis test contains the original hypothesis H0 and the alternative hypothesis H1; the test is the process of choosing an acceptance between H0 and H1 based on the sample.</p>
<p>Ideally, the hypothesis H0(H1) should be assumed to be true and the hypothesis should be accepted. However, since the test is based on samples, false decisions will eventually occur, and they can be of two types: Type I errors correspond to cases where H0 is true but rejected, i.e., “discarded” errors; Type II errors correspond to cases where H0 is untrue but accepted, i.e., “rejected” errors. A “falsification” type of error.</p>
<p>The hypothesis-testing mindset is based on the idea that full propositions can only be falsified but not proven. The easier way to prove that the original hypothesis H0 is true is to prove that the alternative hypothesis H1 is false, since it is enough to be able to cite a counterexample. In hypothesis test, however, counterexamples are not violations of the hypothesis in an absolute sense, but rather in the form of small probability events.</p>
<p>In mathematical statistics, an event with a probability of less than 1% is called a small probability event and is considered unlikely to occur in a single experiment. If a small probability event occurs in a sample obtained from a single observation, then it is reasonable to assume that it is not really a small probability event, and the original assumption is thus overturned. If the alternative hypothesis is overturned, it implies acceptance of the original hypothesis; conversely, if the original hypothesis is overturned, it implies rejection of the original hypothesis.</p>
<p>From a mathematical and statistical point of view, the task of a supervised learning algorithm is to search the hypothesis space for hypotheses that make good predictions for a given problem. The ability of the learner to learn from the test dataset to obtain a model with generalizability that is applicable to new samples that are not part of the test set is called generalizability. Obviously, the better the generalization ability, the better the learner is.</p>
<p>The role of hypothesis test is to infer the strength of the generalization ability of the learner based on its performance on the test set and to determine the accuracy of the conclusions obtained, which can be further extended to compare the performance of different learners. Since a common measure of learner performance is the error rate, the hypothesis in hypothesis test is to infer the generalized error rate of the learner, and the inference is based on the test error rate on the test data set. There are many different ways to test this, and I will not go into them here.</p>
<p>In addition to inference, the interpretation of generalization performance is also an important part of the analysis of machine learning algorithms. The composition of generalization error can be divided into three parts: bias, variance, and noise.</p>
<p>Bias represents the deviation between the predicted value of the algorithm and the true result, depicting the underfitting characteristics of the model; variance represents the effect of the perturbation of the data on the prediction performance, depicting the overfitting characteristics of the model; and noise represents the minimum generalization error that can be achieved on the current learning task, depicting the difficulty of the task itself. For any realistic model, bias and variance are difficult to optimize simultaneously, reflecting the irreconcilable conflict between underfitting and overfitting.</p>
<p>Today I share with you the essential mathematical and statistical foundations of AI, focusing on the interpretation of abstract concepts rather than concrete mathematical formulas, the key points of which are as follows.</p>
<ul>
<li>The task of mathematical statistics is to infer the nature of the aggregate from an observable sample in turn.</li>
<li>The instrument of inference is the statistic, which is a function of the sample and is a random variable.</li>
<li>Estimation theory estimates unknown parameters of the overall distribution through randomly selected samples, including point estimates and interval estimates.</li>
<li>Hypothesis test accepts or rejects a certain judgment about the aggregate by a randomly selected sample and is commonly used to estimate the generalized error rate of machine learning models.</li>
</ul>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization</title>
    <url>/2020/10/01/Optimization/</url>
    <content><![CDATA[<p>Essentially, the goal of AI is optimization: making optimal decisions in complex environments and multi-vessel interactions. Almost all AI problems boil down to solving an optimization problem, so optimization theory is also a fundamental knowledge for AI.</p>
<p>Optimization theory deals with the problem of determining whether the maximum (minimum) value of a given objective function exists, and finding the value that causes the objective function to take the maximum (minimum) value. If a given objective function is viewed as a continuous mountain range, the optimization process is the process of determining the location of a peak and finding a path to reach it.</p>
<p>The function to be minimized or maximized is called the objective function or evaluation function, and most optimization problems can be solved by minimizing the objective function f(x), while maximization problems can be solved by minimizing -f(x).</p>
<p>The difference between the global minimum and the local minimum is that the global minimum is smaller than the function at all other points in the defined domain, whereas the local minimum is only smaller than the function at all neighboring points.</p>
<p>Ideally, the goal of the optimization algorithm is to find the global minimum. But finding the global optimum solution means performing the search on a global scale. Let’s take the example of a mountain range. The global minimum corresponds to the highest peak in the mountain range, and the best way to find this peak is to stand at a higher level, take in all the peaks, and then find the highest one among them.</p>
<p>Unfortunately, none of the current practical optimization algorithms have such a view of God. They all stand at the foot of the mountain and look for nearby peaks one step at a time. However, due to vision limitations, the peaks found are likely to be only the tops within a ten-mile radius, i.e., local very small values.</p>
<p>When the objective function has a large number of input parameters and a large solution space, most practical algorithms cannot meet the computational complexity requirements of global search, and thus can only find local very small values. However, in artificial intelligence and deep learning scenarios, as long as the value of the objective function is small enough, the value can be used as a global minimum as a compromise between performance and complexity.</p>
<p>Depending on the constraints, the optimization problem can be divided into two categories: unconstrained optimization and constrained optimization. The unconstrained optimization has no restriction on the value of the independent variable x, while the constrained optimization restricts the value of x to a specific set, that is, to meet certain constraints.</p>
<p>Linear programming is a typical example of constrained optimization, which solves the problem of maximizing the benefits of a finite cost constraint. Constrained optimization problems are usually more complex than unconstrained optimization problems, but problems with n variables and k constraints can be transformed into unconstrained optimization problems with (n+k) variables by the introduction of Lagrange multiplier. In its simplest form, the Lagrange function is as follows.</p>
<p><img src="http://56ssc.cc/2020/10/01/Optimization/1.jpg" alt><br>where f(x,y) is the objective function, φ(x,y) is the equation constraint, and λ is the Lagrange multiplier. In a mathematical sense, the Lagrange function, which is jointly composed of the original objective function and the constraints, has a common set of best merits and a common optimal objective function value, thus ensuring the invariance of the optimal solution.</p>
<p>The most common method for solving unconstrained optimization problems is gradient descent (gradient descent). Intuitively, the gradient descent method is to find the minimum value of the objective function in the direction where it decreases the fastest, like climbing a mountain on the steepest path to the top of the mountain. Mathematically, the direction of the gradient is the opposite direction of the derivative (derivative) of the objective function.</p>
<p>When the input to the function is a vector, the image of the objective function becomes a surface in high-dimensional space, and the gradient is a vector perpendicular to the contour of the surface and pointing in the direction of increasing height, which also carries information about the direction in high-dimensional space. And for the objective function to fall as fast as possible, it is necessary to have the independent variable move in the direction of the negative gradient. This conclusion is translated into mathematical language as “the multivariate function falls fastest in the direction of its negative gradient”, which is the theoretical basis of the gradient descent method.</p>
<p>Another important factor in the gradient descent algorithm is the step size, i.e. the value of x that changes with each update of f(x). A small step will result in a slower convergence process, and when f(x) is close to the minimum, a large step will result in a step past the minimum, which is called “too much”.</p>
<p>Thus, in gradient descent, the overall rule of step size selection is progressively smaller. This approach is also in accordance with the laws of our understanding. When calibrating an instrument, isn’t it always coarse adjustment and then fine adjustment?</p>
<p>The above is the gradient descent method for a single sample, when there are more than one available training samples, the use of samples is divided into two modes.</p>
<p>One is the batch processing mode (batch processing), that is, to calculate the gradient of the objective function in each sample, and then the gradient of the different samples to sum up the results of the sum as the update of the gradient of the objective function. In the batch processing mode, each update traverses all the samples in the training set, thus the computational effort is large.</p>
<p>Another model is called stochastic gradient descent, which uses only one sample in each update and another sample in the next update, traversing all the samples in an iterative update process. Interestingly, random gradient descent has been shown to perform better when the size of the training set is large.</p>
<p>The gradient descent method only uses the first-order derivative of the objective function and does not use the second-order derivative. The first-order derivative describes how the objective function varies with the input, while the second-order derivative describes how the first-order derivative varies with the input, providing information about the curvature of the objective function. The curvature affects the rate of descent of the objective function. When the curvature is positive, the objective function will fall more slowly than expected with gradient descent; conversely, when the curvature is negative, the objective function will fall more rapidly than expected with gradient descent.</p>
<p>The gradient descent method cannot take advantage of the curvature information contained in the second-order derivatives, but only the local properties of the objective function, and thus is inevitably blind in the search. It is known that the objective function may have increasing derivatives in several directions, implying that the descending gradient has multiple choices. However, there are clearly good and bad effects of different choices.</p>
<p>Unfortunately, the gradient descent method does not have access to information about the change in the derivative, and it is not known that the direction in which the derivative is permanently negative should be explored. Without a global view of the objective function, the gradient descent method takes some detours in use, resulting in slower convergence. The global information contained in the second derivative can provide guidance on the direction of gradient descent, which in turn leads to better convergence.</p>
<p>If the second-order derivative is introduced into the optimization process, the typical method obtained is Newton’s method. In Newton’s method, the objective function is first expanded by Taylor and written in the form of a second-order approximation (in contrast, the gradient descent method preserves only the first-order approximation of the objective function). The second-order approximation is then derived, and its derivative is equal to zero, and the resulting vector represents the direction of the fastest descent. Compared to the gradient descent method, Newton’s method converges more quickly.</p>
<p>Whether it is the use of first-order derivatives of the gradient descent method, or the use of second-order derivatives of the Newtonian method, the basic idea of finding the minimum point is to determine the direction, and then determine the step size, thus collectively known as the “linear search” (line search).</p>
<p>There is also a class of algorithms to find the minimum point of the basic idea is to determine the first step length, step length as a parameter to delineate a region, and then in this region to find the fastest direction of decline. This type of algorithm is known as the “trust region method” (trust region).</p>
<p>Specifically, the confidence region algorithm operates as follows: set a confidence radius s, and in the current point as the center, to s as the radius of a closed spherical region as a confidence region, in the confidence region to find the best of the quadratic approximation model of the objective function, the best and the distance between the current point is the calculated alternative displacement.</p>
<p>On the alternative displacement, if the quadratic approximation of the objective function produces a sufficient decline, then the current point is moved to the calculated most meritorious, and the calculation continues iteratively according to this rule, and s can be increased appropriately; if the asymptotic decline of the objective function is not good enough, then the step is too large, and s needs to be reduced and a new alternative displacement calculated until the termination condition is satisfied.</p>
<p>In addition to the above algorithms, there is a class of optimization methods called “heuristics”. Inspired by bionics, which was born in the 1950s, heuristics apply the mechanisms of natural phenomena such as biological evolution to the optimization of real-world complex problems, and have achieved remarkable results.</p>
<p>In contrast to traditional mathematical-based optimization methods, heuristics go back to the basics. The core idea behind heuristics is the survival-of-the-fittest rule of nature, with empirical factors such as selection and mutation added to the algorithm’s implementation.</p>
<p>In fact, more searches do not necessarily mean more intelligence, but more intelligence is the ability to use heuristics to solve problems without having to do a lot of searching. Examples of heuristic algorithms include genetic algorithms that simulate biological evolution, simulated annealing that simulates the crystallization of solids in statistical physics, and ant colony optimization that produces cluster intelligence in inferior animals.</p>
<p>Neural networks, which are all the rage today, are in fact a class of heuristics that simulate the mechanisms of neuronal competition and collaboration in the brain. If you are interested in these heuristics, you can check out the principles and implementation of different algorithms.</p>
<p>Today I am sharing with you the essential foundations of an optimal approach to AI, focusing on the interpretation of abstract concepts rather than concrete mathematical formulas, the main points of which are as follows.</p>
<p>Typically, the optimization problem is solving for the minimum value of a given objective function in the unconstrained case.</p>
<ul>
<li>In linear search, determining the direction of search in finding a minimum requires the use of the first- and second-order derivatives of the objective function.</li>
<li>The idea behind the confidence domain algorithm is to determine the search step before determining the search direction.</li>
<li>Heuristic algorithms, represented by artificial neural networks, are another important class of optimization methods.</li>
</ul>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>Plain Bayesian</title>
    <url>/2020/11/29/Plain%20Bayesian/</url>
    <content><![CDATA[<h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea."></a>Idea.</h3><p>For a given item x to be classified, the posterior probability distribution is calculated by the learned model, i.e., the probability of occurrence of each target class under the condition that this item occurs, and the class with the highest posterior probability is taken as the class to which x belongs. The posterior probability is calculated according to Bayes’ theorem.</p>
<p>Key: To avoid the combinatorial explosion and sample sparsity problems faced in solving Bayes’ theorem, the assumption of conditional independence is introduced. The features used for classification are conditionally independent under the condition that the classes are determined.</p>
<h3 id="What-is-plain-Bayesian-plain"><a href="#What-is-plain-Bayesian-plain" class="headerlink" title="What is plain Bayesian plain?"></a>What is plain Bayesian plain?</h3><p>Simply put: When solving the joint probability P(XY) using Bayes’ theorem, the conditional probability P(X|Y) needs to be computed. In calculating P(X|Y), the plain Bayesian makes a strong assumption of conditional independence (when Y is determined, the components of X take values independent of each other), i.e., P(X1=x1,X2=x2,… …Xj=xj|Y=yk) = P(X1=x1|Y=yk)<em>P(X2=x2|Y=yk)</em>… *P(Xj=xj|Y=yk).</p>
<p>Main applications: email spam filtering, news classification</p>
<p>Before training the plain Bayesian classifier, the training set is processed and the text is cleaned or there is much to learn.</p>
<p>The text is vectorized according to the extracted classification features, and then the plain Bayesian classifier is trained.</p>
<p>The difference in the number of de-high frequency words has an impact on the results.</p>
<p>Laplace smoothing has a positive effect on improving the classification of the plain Bayes classifier.</p>
<h3 id="What-is-the-difference-between-plain-Bayesian-and-LR"><a href="#What-is-the-difference-between-plain-Bayesian-and-LR" class="headerlink" title="What is the difference between plain Bayesian and LR?"></a>What is the difference between plain Bayesian and LR?</h3><p>In brief.</p>
<p>(1) Parsimonious Bayes is a generative model, which learns the prior probability P(Y) and conditional probability P(X|Y) based on Bayesian estimation of existing samples, and then finds the joint distribution probability P(XY), and finally solves P(Y|X) using Bayes’ theorem, while LR is a discriminant model, which directly finds the conditional probability P(Y|X) based on the maximized log-likelihood function.</p>
<p>(2) Parsimonious Bayes is based on a strong assumption of conditional independence (each feature variable takes values independently of each other under the condition that the classification Y is known), whereas LR does not require this.</p>
<p>(3) Parsimonious Bayes is applicable to scenarios with small data sets, while LR is applicable to large-scale data sets.</p>
<p>The former is a generative model and the latter is a discriminative model.</p>
<p>1) First, Navie Bayes finds the prior probability P(Y), and the conditional probability P(X|Y) from known samples, and for a given instance, calculates the joint probability, and then finds the posterior probability. That is, it tries to find exactly how this data was generated (produced) and then classify it. Whichever category is most likely to generate this signal belongs to that category.</p>
<p>Advantages: faster convergence when the sample size increases; applicable when hidden variables are present.</p>
<p>Disadvantages: long time; requires more samples; wastes computational resources</p>
<p>2) In contrast, logistic regression does not care about the proportion of categories in the sample and the probability of features appearing under the categories; it gives the equation of the prediction model directly. Let each feature have a weight, and the training sample data update the weight w to derive the final expression. Gradient method.</p>
<p>Advantages: direct prediction tends to be more accurate; simplifies the problem; can reflect the distribution of the data, the differential characteristics of the categories; applicable to the identification of more categories.</p>
<p>Disadvantages: slow convergence; not applicable to cases with hidden variables.</p>
<h3 id="What-if-the-probability-is-0-when-estimating-the-conditional-probability-P-X-Y"><a href="#What-if-the-probability-is-0-when-estimating-the-conditional-probability-P-X-Y" class="headerlink" title="What if the probability is 0 when estimating the conditional probability P(X|Y)?"></a>What if the probability is 0 when estimating the conditional probability P(X|Y)?</h3><p>In short: introduce λ. When λ=1, it is called Laplace smoothing. </p>
<h3 id="Advantages-and-disadvantages-of-plain-Bayesian"><a href="#Advantages-and-disadvantages-of-plain-Bayesian" class="headerlink" title="Advantages and disadvantages of plain Bayesian"></a>Advantages and disadvantages of plain Bayesian</h3><p>Advantages: good performance for small-scale data, suitable for multi-classification tasks, and suitable for incremental training.</p>
<p>Disadvantages: sensitive to the expression form of the input data (discrete, continuous, extremely small values, etc.).</p>
<h3 id="Why-is-the-assumption-of-attribute-independence-difficult-to-hold-in-practical-situations-but-plain-Bayes-still-achieves-better-results"><a href="#Why-is-the-assumption-of-attribute-independence-difficult-to-hold-in-practical-situations-but-plain-Bayes-still-achieves-better-results" class="headerlink" title="Why is the assumption of attribute independence difficult to hold in practical situations, but plain Bayes still achieves better results?"></a>Why is the assumption of attribute independence difficult to hold in practical situations, but plain Bayes still achieves better results?</h3><p>1) For classification tasks, as long as the conditional probabilities of each category are correctly ordered and no precise probability values are needed to lead to correct classification.</p>
<p>2) If inter-attribute dependencies affect all categories equally, or if the effects of dependencies can cancel each other out, the assumption of conditional independence of attributes does not negatively affect performance while reducing computational overhead.</p>
<h3 id="Why-a-posteriori-probability-maximization"><a href="#Why-a-posteriori-probability-maximization" class="headerlink" title="Why a posteriori probability maximization."></a>Why a posteriori probability maximization.</h3><p>Equivalently, expect risk minimization. Suppose a 0-1 loss function is chosen, i.e., 1 for correct classification and 0 for error, when the expected risk is minimized as</p>
<p><img src="http://56ssc.cc/2020/11/29/Plain Bayesian/1393464-20180504114329112-1062579832.png" alt="1393464-20180504114329112-1062579832.png"></p>
<h3 id="Algorithmic-problems"><a href="#Algorithmic-problems" class="headerlink" title="Algorithmic problems."></a>Algorithmic problems.</h3><p>In actual projects, the probability values are often very small decimals, and multiplying successive tiny decimals can easily cause the underflow to make the product 0.</p>
<p>Solution: Take the natural logarithm of the product, and change the multiplication into a continuous addition.</p>
<p>Also need to note: the length of the given feature vector may be different, which needs to be normalized to the through-length vector (here to text classification for example), for example, if it is a sentence word, the length is the length of the entire vocabulary, the corresponding position is the number of times the word appears.</p>
<h3 id="Calculation-of-the-prior-conditional-probability"><a href="#Calculation-of-the-prior-conditional-probability" class="headerlink" title="Calculation of the prior conditional probability."></a>Calculation of the prior conditional probability.</h3><ul>
<li>When discrete distribution: statistics of the frequency of occurrence of each category in the training sample. If the probability of a certain eigenvalue is 0 will make the whole probability product become 0 (called data sparsity), which destroys the assumption that each eigenvalue has the same status.</li>
</ul>
<p>Solution 1: Using Bayesian estimation (called Laplace smoothing when λ = 1).</p>
<p><img src="http://56ssc.cc/2020/11/29/Plain Bayesian/1393464-20180504114346515-581861539.png" alt="1393464-20180504114346515-581861539.png"></p>
<p>Solution 2: By clustering the non-occurring words to find out the system keywords, and find the average value according to the probability of related words.</p>
<ul>
<li>When continuous distribution: assume that its value obeys Gaussian distribution (normal distribution). That is, the sample mean and variance are calculated.</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Probability theory</title>
    <url>/2020/09/23/Probability%20theory/</url>
    <content><![CDATA[<p>In addition to linear algebra, probability theory (PROBABILITY) is an essential mathematical foundation for AI research. With the rise of the connectionist school of thought, probability statistics has replaced mathematical logic as the mainstream tool for AI research. In today’s world of exploding data and exponentially increasing computing power, probability theory has taken on a central role in machine learning.</p>
<p>Like linear algebra, probability theory represents a way of looking at the world that focuses on ubiquitous possibilities. The specification of a mathematical description of the probability of a random event occurring is the axiomatization process of probability theory. The axiomatized structure of probability embodies an understanding of the nature of probability.</p>
<p>When the same coin is tossed 10 times, it may be face-up either none or all of the times, which translates into frequencies corresponding to 0% and 100%, respectively. The frequency itself will obviously fluctuate randomly, but as the number of repetitions increases, the value of the frequency of a particular event will show stability and converge to a constant.</p>
<p>The method of understanding probability from the frequency of an event is called “frequencyentist probability”, and the “probability” in frequencyist parlance is actually a single result in a random experiment that can be repeated independently. Limits of frequency of occurrence. Because a stable frequency is an expression of statistical regularity, it is a reasonable idea to calculate the frequency through a large number of independent repeated trials and use it to characterize the probability of an event occurring.</p>
<p>In the quantitative calculation of probability, the frequency school relies on the basis of the classical probability model. In the classical probabilistic model, the outcome of a trial contains only a finite number of fundamental events, each of which has the same probability of occurring. In this way, assuming that the number of all elementary events is n and the number of elementary events contained in the random event A to be observed is k, the formula for the probability of an event under the classical probability model is as follows</p>
<p><img src="http://56ssc.cc/2020/09/23/Probability theory/1.jpg" alt></p>
<p>The probability of complex random events can be derived from this basic formula.</p>
<p>The previous definition of probability is for a single random event, but it’s not enough to describe the relationship between two random events. In a soccer match, the probability of a team winning 1:0 is obviously not the same as the probability of being down 0:2 and overturning the game 3:2. This brings us to the concept of conditional probability.</p>
<p>Conditional probability is a new probability distribution obtained by adjusting the sample space based on existing information. Assuming two random events A and B, conditional probability is the probability that event A will occur under the conditions that event B has already occurred, expressed by the following equation</p>
<p><img src="http://56ssc.cc/2020/09/23/Probability theory/2.jpg" alt></p>
<p>P(AB) in the above equation is called joint probability, which represents the probability of two events A and B occurring together. If the joint probability is equal to the product of the probabilities of the two events, i.e. P(AB) = P(A)⋅P(B), it means that the occurrence of these two events does not affect each other, i.e. they are independent of each other. For mutually independent events, the conditional probability is the probability of itself, i.e., P(A|B) = P(A).</p>
<p>Based on the conditional probability, the law of total probability formula can be derived. The function of the total probability formula is to transform the probability solution of a complex event into a summation of the probabilities of simple events occurring under different circumstances, i.e.</p>
<p><img src="http://56ssc.cc/2020/09/23/Probability theory/3.jpg" alt><br>The full probability formula represents the frequency school’s approach to solving probability problems by making some assumptions (P(Bi)) and then discussing the probability of a random event (P(A|Bi)) under these assumptions.</p>
<p>The inverse probability problem is an important one, which is solved by slightly tidying up the full probability formula. Inverse probability solves the problem of inferring the probability of various hypotheses (P(Bi|A)), given that the outcome of the event is certain (P(A)). Since this theory was first developed by the English clergyman Thomas Bayes, its common form is known as the Bayesian formula.</p>
<p><img src="http://56ssc.cc/2020/09/23/Probability theory/4.jpg" alt></p>
<p>Bayes’ formula can be further abstracted into Bayes’ theorem (Bayes’ theorem).</p>
<p><img src="http://56ssc.cc/2020/09/23/Probability theory/5.jpg" alt></p>
<p>where P(H) is referred to as prior probability, the probability that a predetermined hypothesis holds; P(D|H) is referred to as likelihood function, the probability that an outcome is observed if the hypothesis holds; and P(H|D) is referred to as posterior probability, the probability that an outcome is observed if the hypothesis holds.</p>
<p>In terms of scientific research methodology, Bayes’ theorem provides an entirely new logic. It looks for reasonable hypotheses based on observations, or the best theoretical explanation based on observations, and its focus is on posterior probability. The Bayesian school of probability was born out of this idea.</p>
<p>In the Bayesian view, probability describes how plausible a random event is. If the weather app on your phone gives an 85% probability that it will rain tomorrow, this cannot be explained in terms of frequency, but means that the probability of it raining tomorrow is 85%.</p>
<p>The frequency school considers the assumption to be objective and unchanging, i.e. there is a fixed a priori distribution, but we as observers have no way of knowing it. Therefore, when calculating the probability of a specific event, we need to determine the type and parameters of the probability distribution, and use this as the basis for probabilistic deduction.</p>
<p>The Bayesian school, by contrast, holds that a fixed prior distribution does not exist and that the parameters themselves are random numbers. In other words, the hypotheses themselves depend on observations, are uncertain and subject to revision. The role of data is to make constant revisions to the assumptions, bringing the observer’s subjective perception of probability closer to objective reality.</p>
<p>Probability theory is another theoretical basis for artificial intelligence in addition to linear algebra, and most machine learning models use a probabilistic-based approach. However, due to the limited training data available for practical tasks, the parameters of the probability distribution need to be estimated, which is the core task of machine learning.</p>
<p>There are two approaches to probability estimation: maximum likelihood estimation and maximum a posteriori estimation, both of which reflect the frequency and Bayesian ways of understanding probability, respectively.</p>
<p>The idea of maximum likelihood estimation is to maximize the probability of the training data, determine the unknown parameters in the probability distribution accordingly, and the estimated probability distribution will be the most consistent with the distribution of the training data. The idea of the maximum a posteriori likelihood method, on the other hand, is to maximize the probability of the occurrence of the unknown parameter based on the training data and other known conditions, and to select the most likely value of the unknown parameter taken as an estimate. In estimating the parameters, the maximum likelihood estimation method only requires the use of training data, the maximum a posteriori probability method requires additional information in addition to the data, which is the prior probability in the Bayesian formula.</p>
<p>From a theoretical point of view, the frequency school and the Bayesian school each play an irreplaceable role. However, specifically in the application field of artificial intelligence, the various methods based on Bayes’ theorem are more closely aligned with human cognitive mechanisms and play a more important role in fields such as machine learning.</p>
<p>An important application of probability theory is the description of random variables. According to the different value spaces, random variables can be divided into two categories: discrete random variable and continuous random variable. In practice, it is necessary to describe the probability of each possible value of a random variable.</p>
<p>Each possible value of a discrete variable has a probability greater than 0. The one-to-one correspondence between values and probabilities is the law of distribution of discrete random variables, also known as the probability mass function. Probability mass function in the continuous random variables on the correspondence is the probability density function (probability density function).</p>
<p>It should be noted that the probability density function reflects not the true probability of continuous random variables, but the relative relationship between the different possibilities. For continuous random variables, the number of possible values for the infinite can not be listed, when the probability of normalization is assigned to the infinite points, the probability of each point is an infinitesimal amount, take the limit is equal to zero. The role of the probability density function is to distinguish between these infinitesimal quantities. Although 1/x and 2/x are both infinitesimal at x → ∞, the latter is always twice as large as the former. This kind of difference in relative rather than absolute terms can be depicted by the probability density function. Integrating the probability density function yields the probability that the value of a continuous random variable will fall within a certain interval.</p>
<p>After defining the probability mass function and the probability density function, it is possible to give some characteristics of the important distributions. Important discrete distributions include two-point, binomial, and Poisson distributions, while important continuous distributions include uniform, exponential, and normal distributions.</p>
<ul>
<li><p>Bernoulli distribution: Applies to the case where the outcome of a random trial is binary and the probability of the event occurring/not occurring is p/(1-p). Any random trial with only two outcomes can be described by a two-point distribution, and the result of a coin toss can be considered a two-point distribution with equal probability.</p>
</li>
<li><p>Binomial distribution: The two-point distribution of random trials satisfying the parameter p is repeated n times independently, and the number of events is the binomial distribution satisfying the parameter (n,p). The expression for the binomial distribution can be written as P(X=k)=Cnk⋅pk⋅(1-p)(n-k),0≤k≤n.</p>
</li>
<li><p>Poisson distribution: The distribution satisfied by the number of particles released from radioactive material in the specified time. When n is large in the binomial distribution and p is small, the probability value can be approximated by the probability value of the Poisson distribution with λ=np.</p>
</li>
<li><p>Uniform distribution: A continuous random variable that satisfies a uniform distribution over an interval (a, b), with probability density function 1 / (b - a), which has equal probability of falling within any subinterval of equal length within the interval (a, b).</p>
</li>
<li><p>Exponential distribution: a random variable with parameter θ can only take positive value and its probability density function is e-x/θ/θ,x&gt;0. An important feature of exponential distribution is that it has no memory: P(X &gt; s + t | X &gt; s) = P (X &gt; t).</p>
</li>
<li><p>Normal distribution: the probability density function of a normal distribution with parameters as</p>
</li>
</ul>
<p><img src="http://56ssc.cc/2020/09/23/Probability theory/6.jpg" alt></p>
<p>When μ=0,σ=1, the above equation is called the standard normal distribution. The normal distribution is the most common and important type of distribution, and many phenomena in nature approximately obey the normal distribution.</p>
<p>In addition to the probability mass function / probability density function, another class of parameters that describe random variables are their numerical characteristics. Numerical characteristics are constants used to portray certain characteristics of random variables, including expected value, variance, and covariance.</p>
<p>Mathematical expectations, or mean values, represent the weighted average of the possible values of a random variable, i.e., the pattern that describes the random variable as a whole according to the probability of each value occurring. The variance is the degree to which the values of the random variables deviate from their mathematical expectation. Smaller variance means that the values of the random variables are concentrated near the mathematical expectation, while larger variance means that the values of the random variables are more scattered.</p>
<p>Both mathematical expectations and variance describe the numerical characteristics of a single random variable, and covariance and correlation coefficients are used to describe the interrelationship between two random variables. The covariance measures the linear correlation between two random variables, i.e. whether the variable Y can be expressed as aX+b with the other variable X as the dependent variable.</p>
<p>The correlation coefficient is a constant with an absolute value not greater than 1, equal to 1 means that the two random variables are perfectly positively correlated, equal to -1 means that they are perfectly negatively correlated, and equal to 0 means that they are not correlated. It is important to note that both covariance and correlation coefficients portray a linear correlation. If the relationship between the random variables satisfies Y=X2, such a non-linear correlation is beyond the expressive power of the covariance.</p>
<p>Today I share with you the essential probabilistic foundations of AI, focusing on the interpretation of abstract concepts rather than concrete mathematical formulas, the key points of which are as follows.</p>
<ul>
<li>Probability theory is concerned with the uncertainties or possibilities of life.<br>(a) The frequency school considers the a priori distribution to be fixed and the model parameters to be calculated by maximum likelihood estimation.</li>
<li>Bayesian school of thought that a priori distributions are stochastic and that model parameters are calculated by maximizing a posteriori probabilities.</li>
<li>Normal distribution is the most important kind of distribution of random variables.</li>
</ul>
<p>In today’s machine learning, a great deal of the task is to predict what is likely to happen based on existing data, hence Bayes’ theorem is widely used.</p>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>Rendering process</title>
    <url>/2020/11/07/Rendering%20process/</url>
    <content><![CDATA[<p>In chronological order of rendering, the pipeline can be divided into the following sub-stages: building the DOM tree, style calculation, layout stage, layering, drawing, blocking, rasterization, and compositing. It’s quite extensive, and I’ll devote two articles to explaining each of these sub-stages in detail. Next, as you introduce each phase, you should focus on the following three things.</p>
<ul>
<li>Begin each sub-stage with its own inputs.</li>
<li>Each sub-stage then has its own processing.</li>
<li>Ultimately each sub-stage will generate output content.</li>
<li>Understanding these three parts will give you a clearer understanding of each sub-stage.</li>
</ul>
<h2 id="Building-the-DOM-tree"><a href="#Building-the-DOM-tree" class="headerlink" title="Building the DOM tree"></a>Building the DOM tree</h2><p>Why build a DOM tree? This is because the browser can’t understand and use HTML directly, so it needs to be converted into a structure that the browser can understand - the DOM tree.</p>
<p>We also need to briefly explain what a tree is, but to make it more intuitive, you can look at some of the tree structures I’ve drawn below.</p>
<p><img src="http://56ssc.cc/2020/11/07/Rendering process/1.jpg" alt></p>
<p>To understand the DOM tree more visually, you can open Chrome’s Developer Tools, select the “Console” tab to open the console, and then type “document” into the console and enter “Enter” to see a complete DOM tree structure as shown below.</p>
<p><img src="http://56ssc.cc/2020/11/07/Rendering process/2.jpg" alt></p>
<p>As you can see, the DOM is almost identical to HTML, but unlike HTML, the DOM is a tree-like structure stored in memory that can be queried or modified in JavaScript.</p>
<p>Let’s see how you can modify the DOM in JavaScript by typing in the console.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">document.getElementsByTagName(&quot;p&quot;)[0].innerText = &quot;black&quot;</span><br></pre></td></tr></table></figure>

<p>The purpose of this line of code is to change the content of the first </p><p> tag to a black</p>
<p>After executing a piece of JavaScript code that modifies the first </p><p> tag, the content of the DOM’s first p node is successfully modified, as well as the content of the page.</p>
<p>Well, now we’ve generated the DOM tree, but we still don’t know the style of the DOM node, and to get the DOM node to have the correct style, we need style calculations.</p>
<h2 id="Recalculate-Style"><a href="#Recalculate-Style" class="headerlink" title="Recalculate Style"></a>Recalculate Style</h2><p>The purpose of the style calculation is to figure out the specific style of each element in the DOM node, which can be done in three roughly steps.</p>
<ol>
<li>converting the CSS into a structure that the browser can understand.</li>
</ol>
<p><img src="http://56ssc.cc/2020/11/07/Rendering process/3.jpg" alt></p>
<p>As you can see from the figure, there are three main sources of CSS styles.</p>
<p>External CSS files referenced by links<br>CSS within the <style> tag<br>CSS embedded in the element’s style attribute<br>Like HTML files, these plain-text CSS styles are not directly understood by the browser, so when the rendering engine receives the CSS text, it performs a conversion operation, converting the CSS text into a structure that the browser can understand - styleSheets.</p>
<p>To better understand the structure, you can view it in the Chrome console by typing document.styleSheets into the console, and you’ll see the structure as shown below.</p>
<p><img src="4.jpg" alt=""></p>
<p>As you can see, this style sheet contains many styles, including styles from all three sources. Of course, the structure of the style sheet is not the focus of today’s discussion, you just need to know that the rendering engine will convert all the retrieved CSS text to data in the styleSheets structure, and that the structure has both query and modification capabilities, which will provide the basis for later style operations.</p>
<p>Converting property values in style sheets to standardize them<br>Now that we’ve transformed our existing CSS text into a browser-understandable structure, the next step is to standardize its attribute values.</p>
<p>To understand what attribute value standardization is, you can look at a CSS text like this.</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">body</span> &#123; <span class="attribute">font-size</span>: <span class="number">2em</span> &#125;</span><br><span class="line"><span class="selector-tag">p</span> &#123;<span class="attribute">color</span>:blue;&#125;</span><br><span class="line"><span class="selector-tag">span</span>  &#123;<span class="attribute">display</span>: none&#125;</span><br><span class="line"><span class="selector-tag">div</span> &#123;<span class="attribute">font-weight</span>: bold&#125;</span><br><span class="line"><span class="selector-tag">div</span>  <span class="selector-tag">p</span> &#123;<span class="attribute">color</span>:green;&#125;</span><br><span class="line"><span class="selector-tag">div</span> &#123;<span class="attribute">color</span>:red; &#125;</span><br></pre></td></tr></table></figure>

<p>As you can see in the above CSS text there are a lot of attribute values, such as 2em, blue, bold, these types of values are not easily understood by the rendering engine, so all the values need to be converted to standardized calculated values that the rendering engine can easily understand, this process is attribute value standardization.</p>
<p>What do the standardized attribute values look like?<br><img src="5.jpg" alt=""></p>
<ol start="3">
<li>calculate the specific style of each node in the DOM tree.<br>Now that the attributes of the style have been standardized, the next step is to calculate the style attributes for each node in the DOM tree, how do you do this?</li>
</ol>
<p>This brings us to CSS inheritance rules and cascading rules.</p>
<p>The first is CSS inheritance, where each DOM node contains the style of its parent node. This may sound abstract, but let’s look at a concrete example of how a style sheet like this is applied to a DOM node.</p>
<p>The final effect of this style sheet applied to the DOM node is shown below.</p>
<p><img src="6.jpg" alt=""></p>
<p>It can be seen from the diagram that all child nodes inherit the style of their parent nodes. For example, if the font-size of the body node is 20, then the font-size of all nodes below the body node is equal to 20.</p>
<p>This screen presents a wealth of information and can be roughly described as follows.</p>
<p>First of all, you can select the style of the element you want to view (located in area 2 of the figure), click on the corresponding element in the first area of the figure to view the style of that element. For example, the element we selected here is the <p> tag, which is located under the path html.body.div.<br>Secondly, you can check the source of the style from the style source (in area 3 in the figure) to see if it is from the style file or from the UserAgent style sheet. If you don’t provide any styles, the UserAgent style will be the default.<br>Lastly, you can see how the style inheritance process works by looking at regions 2 and 3.<br>These are some of the features of CSS inheritance, the style calculation process will be based on the inheritance of the DOM nodes to calculate the node style.</p>
<p>The second rule in the style calculation process is style cascading. Cascading is a fundamental feature of CSS, it is an algorithm that defines how to merge attribute values from multiple sources. It is central to CSS, and is emphasized by the fact that CSS is called Cascading Style Sheets. We won’t go into too much detail about the rules of cascading here, there is a lot of information on the web, you can search and learn on your own.</p>
<p>In short, the purpose of the style calculation phase is to calculate the style of each element in the DOM node, which needs to comply with the CSS rules of inheritance and cascading. The final output of this phase is the style of each DOM node, which is stored in the ComputedStyle structure.</p>
<p>If you want to know the final computed style of each DOM element, you can open Chrome’s Developer Tools, select the first “element” tab and then select the “Computed” sub-tab, as shown below.</p>
<p><img src="7.jpg" alt=""></p>
<p>The red box above shows the ComputedStyle value of the html.body.div.p tag. If you want to see which element, just click on the corresponding tag on the left.</p>
<p>Layout stage<br>Now, we have the DOM tree and the styles of the elements in the DOM tree, but that’s not enough to display the page, because we don’t yet have information about the geometric position of the DOM elements. The next step is to calculate the geometry of the visible elements in the DOM tree, and we call this calculation process layout.</p>
<p>Chrome has two tasks to do in the layout phase: creating the layout tree and calculating the layout.</p>
<ol>
<li>creating the layout tree<br>You may have noticed that the DOM tree also contains a lot of invisible elements, such as head tags, and elements that use the display:none attribute. So before we display them, we will additionally build a layout tree containing only the visible elements.</li>
</ol>
<p>To build the layout tree, the browser does roughly the following.</p>
<p>Iterate through all visible nodes in the DOM tree and add these nodes to the layout.<br>Nodes that are not visible are ignored by the tree, such as all the content below the head tag, or the element body.p.span, which is not wrapped into the tree because its attribute contains <code>dispaly:none</code>.</p>
<ol start="2">
<li>layout calculation<br>Now we have a complete layout tree. So the next step is to calculate the coordinate positions of the layout tree nodes. The process of calculating the layout is very complex, so we’ll skip it here until I go into more detail in later chapters.</li>
</ol>
<p>When the layout operation is performed, the result of the layout calculation is rewritten back into the tree, so the tree is both input and output content, which is an irrational place to be in the layout phase, since the input content is not clearly separated from the output content. To address this, the Chrome team is refactoring the layout code with a next-generation layout system called LayoutNG, which attempts to more clearly separate input and output, thus making the newly designed layout algorithm simpler.</p>
</style></p>]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>Browser</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL is probably the most useful skill you&#39;ll ever acquire</title>
    <url>/2020/09/05/SQL%20is%20probably%20the%20most%20useful%20skill%20you&#39;ll%20ever%20acquire/</url>
    <content><![CDATA[<p>In 1946, the world’s first computer was born, and today, the Internet, which was developed by this computer, has become a river of its own. In these decades, countless technologies and industries have been floating in this river, some are still emerging, and some have gone through several ups and downs.</p>
<p>But in the midst of all this volatility, there is one technology that never goes away, and that is SQL.</p>
<p>SQL, as a language that deals directly with data, is a language that interacts with a variety of front-end and back-end languages.</p>
<p>Both front-end engineers and back-end algorithm engineers are bound to work with data and need to understand how to extract the data they want quickly and accurately. Not to mention data analysts, whose job it is to work with data and collate different reports in order to guide business decisions.</p>
<p>Although technical people use SQL to a greater or lesser extent, different people write SQL with different levels of efficiency; for example, a good SQL execution plan minimizes I/O operations because I/O is the most bottlenecked area of the DBMS, and arguably a lot of time is spent on I/O in database operations.</p>
<p>In addition, you also need to consider how to reduce CPU computation; using GROUP BY, ORDER BY, etc. in SQL statements consumes a lot of CPU computation resources, so we need to take a global view and consider not only the I/O performance of the database, but also CPU computation, memory usage, and so on.</p>
<p>For example, an EXIST query and an IN query can get the same result in some cases, but which is more efficient when it comes to execution?</p>
<p>Suppose I abstract the pattern as follows.</p>
<p><code>SELECT * FROM A WHERE cc IN (SELECT cc FROM B)</code></p>
<p><code>SELECT * FROM A WHERE EXIST (SELECT cc FROM B WHERE B.cc=A.cc)</code></p>
<p>During the query process, we need to determine the size of Table A and Table B. If Table A is larger than Table B, then the IN subquery is more efficient than the EXIST subquery. If table A is larger than table B, then the IN subquery is more efficient than the EXIST subquery.</p>
<p>Of course, the power of SQL extends far beyond IT technology to the product and operations side.</p>
<p>For example, let’s say you’re the product manager of a game and you want to find out what heroes are available under various conditions, such as who are the mage heroes with a max life value greater than 7000. Did you ask R&amp;D for help? Or are you looking slowly from the mass of data?</p>
<p>Of course it works both ways, but it would be embarrassing to call R&amp;D every time, and it would be inefficient to do it yourself.</p>
<p>In fact with a single SQL statement, you can get the answer directly from the data table.</p>
<p><code>SELECT * FROM heros WHERE hp_max &gt;= 7000 AND role = &#39;heroes&#39;</code></p>
<p>SQL statements are so intuitive that you can figure out what they mean from a basic level of English, even if you don’t have any SQL. That’s the best thing about SQL.</p>
<p>What if you’re an operations person and you want to see how many new users you’ve added in 7 days? First we need to get the current time, just use the NOW() function, then convert it to days, compare it with the user’s registration time, less than 7 days is our filtering criteria, and finally we can get the desired data.</p>
<p><code>SELECT COUNT(*) as num FROM new_user WHERE TO_DAYS(NOW())-TO_DAYS(regist_time)&lt;=7</code></p>
<p>The two examples above are relatively simple SQL queries, but SQL can also help you keep track of daily new, daily active, and next-day retained data.</p>
<p>In fact, besides business, SQL is also used in various data-based technologies, such as OLTP (online transaction processing), OLAP (online analytical processing), RDBMS (object-based relational database management system). Even on the NoSQL front, SQL-like operations are being used today, bearing in mind that the concept of NoSQL was originally conceived as a move away from SQL, but today people prefer to define NoSQL as Not Only SQL. In addition, in our familiar data formats such as XML, JSON, etc., there are various kinds of SQL, such as SQL for XML, SQL for JSON, etc. In addition, there are also SQL for XML, JSON, etc., and there are also SQL for JSON. In addition, there are also SQL for geolocation information, SQL for search, SQL for time series data, SQL for streams, and so on.</p>
<p>It can be said that SQL is needed for both business and data-related technologies.</p>
<p>If you’re programming or in the Internet industry, there’s nothing more valuable than learning the language SQL, and SQL is probably the most useful skill you’ll ever acquire. The need to understand data tends to be high-frequency, so mastering SQL yourself is essential in the real world.</p>
<p>Whether you’re a product manager, an operations person, a developer, or a data analyst, you can use the SQL language. It’s like a sword that not only increases your productivity, but also expands your work horizons.</p>
<ol>
<li><p>Basic chapter<br>The syntax of SQL is very simple, just like English, but it is very powerful and helps us to index, sort, group, etc. on data. However, these commands are used differently in different database management systems, so in this column, I will not only focus on the syntax of SQL itself, but also explain how this syntax is used in different database management systems like MySQL, Oracle, SQL Server, and so on.</p>
</li>
<li><p>Advanced<br>Many people who write SQL are faced with the question, “I’m also querying data with SQL, why do I write statements slower than others?”</p>
</li>
</ol>
<p>In fact, it’s the simplicity of SQL syntax that causes many people to write in an unconventional way, such as confusing the order of keywords, which inadvertently reduces the efficiency of SQL execution.</p>
<p>In this section, I’ll explain the problems you often encounter when using SQL in practice, and how to use tools to analyze and quickly locate performance problems and solutions. 3.</p>
<ol start="3">
<li>Advanced section<br>In the era of big data, there are many database management systems for different scenarios, including SQL-based databases, such as Oracle, MySQL, SQL Server, Access, WebSQL, SQLite, etc., and NoSQL-based databases, such as MongoDB, Redis, etc. In this section, I will talk about the use of various mainstream database management systems.</li>
</ol>
<p>In this part, I will talk about the use of various mainstream database management systems.</p>
<ol start="4">
<li>practical chapter<br>The above sections are to help you organize your knowledge of SQL, but only when you learn how to use SQL systematically for real projects, can you really put what you have learned into practice and make SQL work for you.</li>
</ol>
]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Support vector machine SVM</title>
    <url>/2020/12/04/Support%20vector%20machine%20SVM/</url>
    <content><![CDATA[<h3 id="What-is-the-principle-of-SVM"><a href="#What-is-the-principle-of-SVM" class="headerlink" title="What is the principle of SVM?"></a>What is the principle of SVM?</h3><p>　　SVM is a two-class classification model. Its basic model is a linear classifier that finds the separated hyperplane with maximized interval in the feature space. (Interval maximization is what distinguishes it from a perceptron)<br>It tries to find a hyperplane to segment the sample, separating the positive and negative examples in the sample with a hyperplane and maximizing the interval between the positive and negative examples as much as possible.</p>
<p>　　The basic idea of support vector machine can be summarized as, firstly, transforming the input space to a higher dimensional space by a nonlinear transformation, and then finding the optimal classification plane in this new space, i.e., the maximum interval classification plane, and this nonlinear transformation is achieved by defining an appropriate inner product kernel function.SVM is actually proposed according to the statistical learning theory in accordance with the principle of minimizing structural risk, which requires to achieve two purposes the separation of the two types of problems </p>
<p>(1) separation of the two types of problems (empirical risk minimization)</p>
<p>(2) maximizing margin (minimizing the upper bound on risk) both by choosing the function with the lowest empirical risk in the subset of guaranteed risk minimization.</p>
<p>Divided into 3 classes of support vector machines.<br>(1) When the training samples are linearly divisible, a linear classifier is learned by hard interval maximization, i.e., a linearly divisible support vector machine.</p>
<p>(2) When the training data are approximately linearly divisible, a linear classifier, i.e., linear support vector machine, is learned by soft interval maximization by introducing relaxation variables.</p>
<p>(3) When the training data is linearly indistinguishable, a nonlinear support vector machine is learned by using the kernel trick and soft interval maximization.</p>
<p>Note: The mathematical derivation of each of the above SVMs should be familiar with: hard interval maximization (geometric interval) - learned pairwise problem - soft interval maximization (introduction of relaxation variables) - nonlinear support vector machine (kernel trick).</p>
<h3 id="Main-features-of-SVM"><a href="#Main-features-of-SVM" class="headerlink" title="Main features of SVM"></a>Main features of SVM</h3><p>(1) Nonlinear mapping - theoretical basis </p>
<p>(2) Maximization of classification boundary - method core </p>
<p>(3) Support vector - computational results </p>
<p>(4) Small sample learning method</p>
<p>(5) The final decision function is determined by only a small number of support vectors, avoiding the “dimensional disaster”</p>
<p>(6) A small number of support vectors determines the final result –&gt; a large number of redundant samples can be “eliminated” + the algorithm is simple + has robustness (reflected in 3 aspects)</p>
<p>(7) Learning problem can be expressed as a convex optimization problem –&gt; global minimum</p>
<p>(8) can automatically control the model by maximizing the boundary, but requires the user to specify the type of kernel function and the introduction of relaxation variables</p>
<p>(9) Suitable for small samples, excellent generalization capability (because of minimal structural risk) (10) Low generalization error rate, fast classification, and easy to interpret results.</p>
<p>Disadvantages:<br>(1) Large-scale training samples (m-order matrix computation) </p>
<p>(2) Traditionally not suitable for multi-classification </p>
<p>(3) Sensitive to missing data, parameters, kernel functions</p>
<p>Why is SVM sensitive to missing data?</p>
<p>Missing data here means that some feature data is missing and the vector data is incomplete. sVM does not have a strategy to deal with missing values (decision tree does). And SVM expects the samples to be linearly separable in the feature space, so the goodness of the feature space is important to the performance of SVM. Missing feature data will affect the goodness of the training results.</p>
<h3 id="Why-does-SVM-use-interval-maximization"><a href="#Why-does-SVM-use-interval-maximization" class="headerlink" title="Why does SVM use interval maximization?"></a>Why does SVM use interval maximization?</h3><p>When the training data is linearly separable, there exist infinite separating hyperplanes to correctly separate the two types of data. The perceptual machine uses the misclassification minimization strategy to find the separation hyperplane, but there are infinite number of solutions at this time.</p>
<p>Linearly separable support vector machines use interval maximization to find the optimal separation hyperplane, at which point the solution is unique. On the other hand, the separation hyperplane at this point produces the most robust classification result, with the best generalization to unknown instances.</p>
<p>This should then be used to elaborate on the geometric interval, the functional interval, and the w and b when minimizing 1/2 ||w||^2 from the functional interval-&gt;solution. i.e., the origin of the linearly separable support vector machine learning algorithm-maximum interval method.</p>
<p>What is maximized and what is minimized in svm?</p>
<p>In various explanations of SVM, there is one knowledge point that is not well covered: the objective function of SVM is to maximize the geometric interval of the support vector, but how does it end up minimizing the normal vector (slope)?        </p>
<p>Imagine a hyperplane where the slope and intercept increase by the same multiple, and the hyperplane is constant. In other words, the parameters of a fixed hyperplane are not fixed. When we find the optimal hyperplane, the solution space becomes infinite. We can of course reduce the solution space by setting some constraints on these parameters in advance. Then, this constraint is: make the function interval of the support vector = 1.       </p>
<p>The advantage of this constraint is twofold: In the case that the hyperplane is not determined, of course no one knows which vectors the support vectors are, and there is only a formal expression for the geometric interval of the support vectors, not to mention how to express “maximize the geometric interval of the support vectors”. But with the above constraint, it does not matter who is the support vector in the expression of “geometric interval of support vector”, the only part related to the sample, that is, the function interval, has become 1. The function interval of other samples should be larger than the function interval of support vector, which is the only constraint to be satisfied. At this point, the solution space of the problem is no longer infinite, and there is a meaningful solution space.</p>
<p>Support vector regression essentially has nothing to do with SVM, and the name is more confusing. However, it is included in libSVM, so I have to talk about it. In fact, it is solving a linear regression problem, but due to the increase of the minimum parametric requirement for the slope, the optimization problem formally resembles SVM, and the final expression of the linear function is also very similar to SVM, appearing in the form of a wonderful inner product with the support vector.</p>
<h3 id="Why-should-we-convert-the-original-problem-of-solving-SVM-to-its-dual-problem"><a href="#Why-should-we-convert-the-original-problem-of-solving-SVM-to-its-dual-problem" class="headerlink" title="Why should we convert the original problem of solving SVM to its dual problem?"></a>Why should we convert the original problem of solving SVM to its dual problem?</h3><p>One, is that the dual problem is often easier to solve (when we look for the best advantage when the constraint exists, the existence of the constraint reduces the scope of the search required, but makes the problem more complex. (To make the problem manageable, our approach is to incorporate the objective function and constraints into a new function, the Lagrangian function, and then find the optimal point through this function.) </p>
<p>Note: Lagrangian pairing does not change the optimal solution, but changes the complexity of the algorithm: original problem - sample dimension; pairing problem - number of samples. So linear classification -&gt; sample dimension &lt; sample number: original problem solved (liblinear default); nonlinear - up-dimensional -&gt; general leads to sample dimension &gt; sample number: pairwise problem solved.</p>
<p>Second, naturally introduce kernel functions, which in turn extend to nonlinear classification problems.</p>
<h3 id="Explain-the-support-vector"><a href="#Explain-the-support-vector" class="headerlink" title="Explain the support vector"></a>Explain the support vector</h3><p>Definition in the linearly divisible case + definition in the linearly indivisible case . (Statistical learning methods) </p>
<p>(1) Several equivalent definitions of SV for linear separable SVM </p>
<p>(2) Several equivalent definitions of SV for linear SVM </p>
<p>(3) Compare the difference and connection between the definition of SV for linear separable SVM and the definition of SV for linear SVM</p>
<p>Why do SVMs introduce kernel functions?</p>
<p>When the sample is linearly indistinguishable in the original space, the sample can be mapped from the original space to a higher dimensional feature space, making the sample linearly distinguishable in this feature space.</p>
<p>Pairwise problems after the introduction of mapping. </p>
<p><img src="http://56ssc.cc/2020/12/04/Support vector machine SVM/1.png" alt="1.png"></p>
<p>In learning prediction, only the kernel function K(x,y) is defined instead of explicitly defining the mapping function ϕ. Because the feature space dimension may be high, possibly even infinite, it is more difficult to compute ϕ(x)-ϕ(y) directly. In contrast, it is easier to compute K(x,y) directly (i.e., directly in the original low-dimensional space without explicitly writing out the result of the mapping).</p>
<p>The kernel function is defined by K(x,y)=&lt;ϕ(x),ϕ(y)&gt;, i.e., the inner product in the feature space is equal to the result they compute in the original sample space by the kernel function K.</p>
<p>Except for SVM, any method that represents the computation as an inner product of data points can be extended nonlinearly using the kernel method.</p>
<h3 id="What-is-the-specific-formula-for-the-svm-RBF-kernel-function-Gaussian-kernel-function-also-called-Radial-Basis-Function-RBF-for-short-It-can-map-the-original-features-to-infinite-dimensions"><a href="#What-is-the-specific-formula-for-the-svm-RBF-kernel-function-Gaussian-kernel-function-also-called-Radial-Basis-Function-RBF-for-short-It-can-map-the-original-features-to-infinite-dimensions" class="headerlink" title="What is the specific formula for the svm RBF kernel function? (Gaussian kernel function, also called Radial Basis Function (RBF for short). It can map the original features to infinite dimensions)"></a>What is the specific formula for the svm RBF kernel function? (Gaussian kernel function, also called Radial Basis Function (RBF for short). It can map the original features to infinite dimensions)</h3><p><img src="http://56ssc.cc/2020/12/04/Support vector machine SVM/2.png" alt="2.png"></p>
<p>Advantages of RBF kernels.<br>It is applicable to both large and small. Specifically (1) infinite-dimensional, linear kernel is its special case (2) compared to polynomial ~, RBF requires fewer parameters to be determined (3) under certain parameters, it has a similar function to sigmoid ~.</p>
<p>The Gauss radial basis function, on the other hand, is a strongly localized kernel function whose extrapolation ability decreases with increasing parameter σ.</p>
<p>This kernel will map the original space to an infinite dimensional space. However, if σ is chosen to be very large, the weights on the higher order features actually decay very fast, so that it is actually (to approximate numerically) equivalent to a low-dimensional subspace; conversely, if σ is chosen to be very small, it can map arbitrary data to be linearly divisible - which, of course, is not necessarily a good thing, because very serious overfitting problems may follow. However, in general, by tuning the parameter σ , the Gaussian kernel is actually quite flexible and one of the most widely used kernel functions.</p>
<h3 id="How-does-SVM-handle-multi-classification-problems"><a href="#How-does-SVM-handle-multi-classification-problems" class="headerlink" title="How does SVM handle multi-classification problems?"></a>How does SVM handle multi-classification problems?</h3><p>There are two general approaches: one is the direct method, which directly modifies the objective function and combines the parameter solutions of multiple classification surfaces into one optimization problem. This seems to be simple, but the computational effort is very large.</p>
<p>The other approach is the indirect approach: combining the trainers. The typical ones are one-to-one, and one-to-many.</p>
<p>One-to-many is to train a classifier for each class. Since svm is binary, the two classes of this binary classifier are set as one class for the target class and another class for the remaining classes. In this way, k classifiers can be trained for k classes, and when a new sample comes in, the k classifiers are used to test which class the sample belongs to if the classifier has a high probability. This method is not very effective and the bias is relatively high.</p>
<p>If there are k classes, a total of C(2,k) classifiers are trained, so that when a new sample comes, the C(2,k) classifiers are used to test, and whenever a class is determined to belong to a class, the class is added one, and the class with the most votes is determined to be the class of the sample. </p>
<h3 id="Differences-and-connections-between-SVM-and-LR"><a href="#Differences-and-connections-between-SVM-and-LR" class="headerlink" title="Differences and connections between SVM and LR"></a>Differences and connections between SVM and LR</h3><p>Connection:</p>
<p>(1) classification (binary classification) </p>
<p>(2) can be added to the regularization term </p>
<p>Differences:<br>(1) LR-parametric model; SVM-non-parametric model? </p>
<p>(2) Objective function: LR-logistical loss; SVM-hinge loss </p>
<p>(3) SVM-support vectors; LR-reduce the weights of the farther point </p>
<p>(4) LR -model is simple, well understood, low accuracy, may be locally optimal; SVM-understanding, optimization complex, high accuracy, global optimal, transformed into a pairwise problem -&gt; simplify the model and computation </p>
<p>(5) LR can do what SVM can do (linearly divisible), SVM can do what LR may not be able to do (linearly indistinguishable)</p>
<p>The relationship between kernel function selection and FEATURE, sample.</p>
<p>(1) fea large ≈ the number of samples: LR or linear kernel </p>
<p>(2) fea small, the number of samples is not small nor small: Gaussian kernel </p>
<p>(3) fea large, the number of samples: manually add features after turning</p>
<p>Kernel function is actually a similarity measure of the input data, the input vectors form a similarity matrix K (Gram Matrix/Similarity/Kernel Matrix), K is symmetric semi-positive definite.</p>
<p>The sufficient condition for K(x,z) to be a positive definite kernel is that the Gram matrix corresponding to K(x,z) is a real semi-positive definite matrix. </p>
<p>Gram matrix: inner product of the points corresponding to the matrix. ktk, kkt </p>
<p>Semi-positive definite matrix: let A be a real symmetric matrix. A is said to be semi-positive definite if for any real non-zero column matrix X there is XTAX ≥ 0. </p>
<p>When testing whether a K is a positive definite kernel function, verify whether the real Gram matrix corresponding to K is a semi-positive definite matrix for any finite set of inputs {xi…}.</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP protocol</title>
    <url>/2020/10/18/TCP%20protocol/</url>
    <content><![CDATA[<p>The Internet is, in fact, an architecture of ideas and protocols. Of these, protocols are a set of well-known rules and standards that, if all parties agree to use them, there will be no barriers to communication between them.</p>
<p>Data in the Internet is transmitted in packets. If the data being sent is large, that data is split into many small packets to be transmitted. For example, the audio data you are listening to right now is split into smaller packets to be transmitted, not one large file at a time.</p>
<ol>
<li>IP: delivery of packets to destination hosts<br>In order for a packet to be transmitted over the Internet, it has to comply with the Internet Protocol (IP) standard. Different online devices on the Internet have unique addresses, the address is just a number, which is similar to most home receiving addresses, you only need to know the specific address of a home to send a package to that address so that the logistics system can deliver the item to its destination.</li>
</ol>
<p>The address of a computer is called an IP address, and accessing any website is really just your computer requesting information from another computer.</p>
<p>If you want to send a packet from host A to host B, the packet will be appended with host B’s IP address information before it is transmitted so that it can be addressed correctly during transmission. In addition, the packet is appended with host A’s own IP address so that host B can reply to host A. This additional information is packed into a data structure called the IP header, which is the information at the beginning of an IP packet and contains the IP version, source IP address, destination IP address, life time, etc. The IP header is the information at the beginning of the packet.</p>
<p><img src="http://56ssc.cc/2020/10/18/TCP protocol/1.jpg" alt></p>
<ol start="2">
<li>UDP: delivering packets to the application<br>IP is a very low-level protocol that is only responsible for sending packets to the other computer, but the other computer doesn’t know which program to give the packets to, the browser or the king? Therefore, it is necessary to develop protocols that deal with applications based on IP, most commonly known as “User Datagram Protocol” (UDP).</li>
</ol>
<p>One of the most important information in UDP is the port number, which is actually a number that every program that wants to access the network needs to bind a port number. The port number allows UDP to send the specified packet to the specified program, so IP sends the packet to the specified computer via the IP address information, and UDP distributes the packet to the correct program via the port number. Like the IP header, the port number is loaded into the UDP header, and the UDP header merges with the original packet to form a new UDP packet, which contains not only the destination port but also the source port number.</p>
<p><img src="http://56ssc.cc/2020/10/18/TCP protocol/2.jpg" alt></p>
<p>When using UDP to send data, there are various factors that can cause packet errors, and while UDP can verify that the data is correct, UDP does not provide a mechanism to re-send packets that are incorrect, it just discards the current packet, and UDP has no way of knowing if it will reach its destination after sending it.</p>
<p>Although UDP does not guarantee data reliability, but the transmission speed is very fast, so UDP will be used in some areas of concern for speed, but less strict requirements for data integrity, such as online video, interactive games and so on.</p>
<ol start="3">
<li><p>TCP: delivering data to the application in its entirety<br>For applications such as browser requests, or emails that require reliability of data transmission, there are two problems with using UDP for transmission.</p>
<p>The vulnerability of packets to loss during transmission.<br>Large files are split into many small packets for transmission, and these small packets go through different routes and arrive at the receiver at different times, while the UDP protocol does not know how to assemble these packets to reduce them to complete files.<br>Based on these two problems, we introduced TCP, a connection-oriented, reliable, byte-stream-based transport layer communication protocol. Compared to UDP, TCP has the following two characteristics:</p>
</li>
</ol>
<p>TCP provides a retransmission mechanism for the case of packet loss.<br>TCP introduces a packet-sorting mechanism to ensure that disordered packets are combined into a complete file.<br>Like the UDP header, the TCP header contains the destination port and the local port number in addition to the sequence number used to sort the packets so that the receiving end can rearrange the packets by the sequence number.</p>
<p>Here is a look at the flow of a single packet transmission under TCP.</p>
<p><img src="http://56ssc.cc/2020/10/18/TCP protocol/3.jpg" alt></p>
<p>The lifecycle of a complete TCP connection consists of three phases: “establish connection,” “transfer data,” and “disconnect.</p>
<p>First, the connection establishment phase. This phase establishes the connection between client and server through the “three handshakes”. Connection-oriented refers to the preparation between the two ends before data communication begins. The three handshakes mean that when a TCP connection is established, the client and server send a total of three packets of data to confirm the establishment of the connection.<br>Next, the data transfer phase. In this phase, the receiving end needs to perform an acknowledgement operation on each packet, which means that after receiving the packet, the receiving end needs to send an acknowledgement packet to the sender end. So when the sending end sends a packet and does not receive a confirmation message back from the receiving end within a specified time, the packet is judged to be lost and a retransmission mechanism is triggered on the sending end. Similarly, a large file is split into many smaller packets during transmission, and when these packets reach the receiving end, the receiving end sorts them according to the sequence number in the TCP header to ensure that they are composed of complete data.<br>Finally, the disconnect phase. Once the data has been transmitted, the connection is terminated, involving a final phase of “four waves” to ensure that both parties are disconnected.<br>At this point you should understand that TCP sacrifices packet speed to ensure the reliability of the data transmission, because the “three handshakes” and “packet-checking mechanism” double the number of packets in the transmission process.</p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>Data in the Internet is transmitted by packets of data, which can be easily lost or erroneous during transmission.<br>IP is responsible for sending the packets to the destination host.<br>UDP is responsible for sending the packets to the specific application.<br>TCP, on the other hand, ensures that data is transmitted intact, and its connection can be divided into three stages: establishing a connection, transferring data, and disconnecting.</p>
]]></content>
      <categories>
        <category>TCP</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title>The data table paradigm</title>
    <url>/2020/10/25/The%20data%20table%20paradigm/</url>
    <content><![CDATA[<h2 id="What-are-the-design-paradigms-for-the-database"><a href="#What-are-the-design-paradigms-for-the-database" class="headerlink" title="What are the design paradigms for the database?"></a>What are the design paradigms for the database?</h2><p>When designing a relational database model, the need to define the degree of rationalization of the connections between the various attributes within the relationship gives rise to different levels of specification requirements, which are called paradigms. You can think of a paradigm as a level of design criteria that the design structure of a data table needs to meet.</p>
<p>There are currently six paradigms for relational databases, which are, in order of paradigm level, from lowest to highest: 1NF, 2NF, 3NF, BCNF (Bass-Cord paradigm), 4NF (fourth paradigm), and 5NF (fifth paradigm, also known as the perfect paradigm).</p>
<p>The higher the paradigm design of the database, the lower the redundancy, and at the same time the higher-order paradigm must meet the requirements of the lower-order paradigm, for example, to meet 2NF must meet 1NF, to meet 3NF must meet 2NF, and so on.</p>
<p>In general data tables should be designed to meet 3NF as much as possible. but not absolutely. sometimes it is necessary to break the paradigm rules to improve the performance of certain queries. i.e., anti-normalization.</p>
<h2 id="Those-keys-in-the-data-table"><a href="#Those-keys-in-the-data-table" class="headerlink" title="Those keys in the data table"></a>Those keys in the data table</h2><p>The definition of a paradigm will use primary and candidate keys, and the keys in the database will consist of one or more attributes. I summarize the following definitions of keys and attributes that are commonly used in data tables.</p>
<p>The set of attributes that uniquely identifies a tuple is called a hyperkey.<br>Candidate key: if the super key does not include any extra attributes, then this super key is a candidate key.<br>Primary key: the user can select one of the candidate keys as the primary key.<br>Foreign key: If an attribute set in the data table R1 is not the primary key of R1, but the primary key of another data table R2, then this attribute set is a foreign key of the data table R1.<br>Primary attribute: the attribute included in any candidate key is called primary attribute.<br>Non-primary attributes: as opposed to primary attributes, attributes that are not included in any of the candidate keys.</p>
<p>From 1NF to 3NF<br>Now that you know the four types of keys in a database table, let’s look at 1NF, 2NF, and 3NF, which we’ll talk about later.</p>
<p>1NF refers to the fact that any attribute in a database table is atomic in nature and cannot be subdivided. In fact, any DBMS will satisfy the first paradigm by not splitting fields.</p>
<p>2NF refers to a data table in which non-primary attributes have a full dependency on the candidate key of the table. A full dependency is not the same as a partial dependency, i.e., you cannot depend on only part of a candidate key’s attributes, but you must depend on all of them!</p>
<p>This means that a field in the candidate key determines the non-primary attribute. You can also understand that for non-primary attributes, it is not entirely dependent on the candidate key. What kind of problem does this cause?</p>
<p>Data redundancy: If a player can play in m matches, then the player’s name and age are duplicated m-1 times. A match may also have n players in it, the time and place of the match is duplicated n-1 times.<br>Insert Exception: If we want to add a new match, but we haven’t decided who the players are yet, then it can’t be inserted.<br>Delete exception: if we want to delete a player number, the match information will be deleted at the same time if we don’t have a separate match sheet.<br>Update Exception: If we adjust the time of a match, then all the times in the table for that match need to be adjusted, otherwise the time of one match will be different from the other.</p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>Relational databases are designed based on a relational model, in which there are four types of keys, and the central role of these keys is identification.</p>
<p>Building on these concepts, I talk about 1NF, 2NF, and 3NF, three paradigms that we often work with to build databases that are less redundant and more structured.</p>
<p>One thing to keep in mind is that these paradigms only present design criteria, and in practice, the design of data tables may not have to conform to these principles. On the one hand, there are problems with these paradigms, which can lead to anomalies in insertions, updates, deletions, etc. (these will be illustrated in the next section), and on the other hand, they can reduce the efficiency of queries. Why is this? Because the higher the paradigm level, the more data tables are designed, multiple tables may need to be associated when querying data, which affects the efficiency of the query.</p>
]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>The data table paradigm</tag>
      </tags>
  </entry>
  <entry>
    <title>The difference between arrow functions and ordinary functions</title>
    <url>/2020/08/01/The%20difference%20between%20arrow%20functions%20and%20ordinary%20functions/</url>
    <content><![CDATA[<p>The ES6 standard adds a new function: Arrow Function, why is it called Arrow Function?</p>
<h1 id="Characteristics-of-the-arrow-function"><a href="#Characteristics-of-the-arrow-function" class="headerlink" title="Characteristics of the arrow function"></a>Characteristics of the arrow function</h1><p>Arrow functions are equivalent to anonymous functions and simplify the function definition. Arrow functions come in two formats, one containing only one expression, omitting even the { … } and return are omitted. The other type can contain multiple statements, in which case you can’t omit { … } and return can be included, in which case you can’t omit { … } and return.</p>
<ul>
<li>Arrow functions have a simpler syntax than normal functions.</li>
<li>The arrow function doesn’t bind to this, it captures the this value of the context it is in as its own this value<ul>
<li>The this object within a function is the object where it is at the time of definition, not the object where it is at the time of use.</li>
<li>Converting dynamic this to static this: This object in JavaScript has long been a headache, and you have to be very careful when using this in object methods. The arrow function, which “binds” this, solves this problem to a large extent.</li>
<li>The arrow function can make this pointing fixed, which is a great feature for wrapping callbacks.</li>
<li>Principle: the this pointing point is fixed not because the arrow function has an internal “this” binding mechanism, but because the function has no “this” of its own, which means that the internal “this” is the “this” of the outer block.</li>
</ul>
</li>
<li>The arrow function is an anonymous function, and cannot be used as a constructor, and cannot use the new command, or an error will be thrown. So the arrow function does not have new.target either.</li>
</ul>
<p>Reason: What does the constructor do with new? Simply put, there are four steps</p>
<ul>
<li>JS will first become an object internally.</li>
<li>And then pointing this in the function to that object.</li>
<li>Then executes the statements in the constructor.</li>
<li>Finally, the object instance is returned.</li>
<li>The arrow function doesn’t bind arguments, but instead uses rest arguments… So the arrow function doesn’t have arguments.callee and arguments.caller.</li>
</ul>
<p>The arguments object cannot be used; it does not exist within the function. If you want to use it, you can use the rest parameter instead. To get all arguments in an array-like form in an arrow function, you can use expansion operators to receive arguments, such as:</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> testFunc = <span class="function">(<span class="params">...args</span>)=&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(args) <span class="comment">//Arrays of output parameters</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>In function declarations before ECMAScript 6, their arguments were of the “simple parameter type”. After ECMAScript 6, any parameter declaration that uses one of the default, remaining, or template parameters is no longer “non-simple parameters”.<br>With traditional simple parameters, you simply bind the actual parameter passed when the parameter is called to the argument object; with “non-simple parameters”, you bind the name to the value via an “initializer assignment”.</p>
<p>The difference between the two binding modes is that usually when you bind an actual parameter to a parameter object, you only need to map the subscripts of the two arrays, whereas the initializer assignment needs to be indexed by name (for binding), so if there is a “renamed parameter” it can’t be processed.</p>
<p>Calling with call(), apply() and bind() has no effect on this</p>
<p>Since this is already bound at the lexical level, calling a function with the call(), apply(), or bind() methods passes only one argument and has no effect on this.</p>
<p>The arrow function has no prototype property prototype.<br>You cannot simply return the object literal.</p>
<p>You need to wrap the object in parentheses if you want to return it directly, because the curly brackets are taken up and interpreted as a block of code.<br>The yield keyword cannot be used, so the arrow function cannot be used as a Generator function.<br>The arrow function cannot be followed by a line break after the parenthesis.<br>The arrow function does not have super.</p>
<h1 id="ES5-implementation-of-the-arrow-function-Babel-conversion"><a href="#ES5-implementation-of-the-arrow-function-Babel-conversion" class="headerlink" title="ES5 implementation of the arrow function (Babel conversion)"></a>ES5 implementation of the arrow function (Babel conversion)</h1><p>Before:</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ES6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span>  obj = &#123;</span><br><span class="line"></span><br><span class="line">    getArrow() &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="built_in">console</span>.log(<span class="keyword">this</span> === obj);</span><br><span class="line"></span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>After:</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ES5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span>  obj = &#123;</span><br><span class="line">    getArrow:  <span class="function"><span class="keyword">function</span>  <span class="title">getArrow</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span>  _this = <span class="keyword">this</span>;</span><br><span class="line">        <span class="keyword">return</span>  <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">            <span class="built_in">console</span>.log(_this === obj);</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h1><ol>
<li>The this of an arrow function always points to this in its context, and no method can change its pointing, such as call() , bind() , apply() , etc. It can be said that it is precisely because there is no this of its own that it has most of the features described above.</li>
<li>The this of a normal function points to the object that called it.</li>
</ol>
]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title>Thinking about SQL in a database way</title>
    <url>/2020/09/27/Thinking%20about%20SQL%20in%20a%20database%20way/</url>
    <content><![CDATA[<h2 id="How-SQL-is-executed-in-Oracle"><a href="#How-SQL-is-executed-in-Oracle" class="headerlink" title="How SQL is executed in Oracle"></a>How SQL is executed in Oracle</h2><p><img src="http://56ssc.cc/2020/09/27/Thinking about SQL in a database way/1.jpg" alt></p>
<ol>
<li><p>Syntax Check: Check if the SQL spelling is correct, if not, Oracle will report syntax error.</p>
</li>
<li><p>Semantic check：Check if the access object in SQL exists. For example, when we write SELECT statement, the column name is written wrongly, the system will report an error. The purpose of syntax check and semantic check is to make sure there are no errors in the SQL statement.</p>
</li>
<li><p>Permissions check: to see if the user has permission to access the data.</p>
</li>
<li><p>The shared pool check: the shared pool is a memory pool, the main purpose is to cache the SQL statements and the execution plan of the statements, Oracle by checking whether the shared pool exists the execution plan of the SQL statements, to determine the soft or hard parsing. What do you mean by soft parsing and hard parsing?</p>
</li>
</ol>
<p>In the shared pool, Oracle first performs a Hash operation on the SQL statements, and then looks for them in the library cache according to the Hash value, if there is an execution plan for the SQL statements, it will be used to execute them directly, and directly enter the “executor” link, which is soft parsing.</p>
<p>If no SQL statement and execution plan are found, Oracle needs to create a parse tree for parsing, generate an execution plan, and enter the “optimizer” step, which is hard parsing.</p>
<ol start="4">
<li><p>Optimizer: the optimizer is to do hard parsing, that is, to decide what to do, such as creating the parse tree and generating the execution plan.</p>
</li>
<li><p>Executor: When you have a parse tree and execution plan, you know how the SQL should be executed, so you can execute statements in the executor.</p>
</li>
</ol>
<p>Shared pool is a term used in Oracle that includes library cache, data dictionary buffer, etc. We have already talked about the library buffer above, which mainly caches SQL statements and execution plans. And the data dictionary buffer stores object definitions in Oracle, such as tables, views, indexes, and other objects. When parsing SQL statements, if relevant data is needed, it is fetched from the data dictionary buffer.</p>
<p>This one step, the library cache, determines whether the SQL statement needs to be hard parsed or not. To improve the efficiency of SQL execution, we should avoid hard parsing as much as possible, because creating parse trees and generating execution plans during SQL execution is resource consuming.</p>
<p>You may ask, how to avoid hard parsing and use soft parsing as much as possible? In Oracle, bind variables are one of its major features. Binding variables is the use of variables in a SQL statement to change the result of SQL execution by taking different values of the variable. The advantage of this is that it improves the possibility of soft parsing, but the disadvantage is that it may result in an unoptimized execution plan, so the need to bind variables depends on the situation.</p>
<p>As an example, we could use the following query statement.</p>
<p><code>SQL&gt; select * from player where player_id = 10001;</code></p>
<p>You can also use binding variables such as.</p>
<p><code>SQL&gt; select * from player where player_id = :player_id;</code></p>
<p>The efficiency of these two query statements is completely different in Oracle. If you also query 10002, 10003, and so on after querying player_id = 10001, then a new query parse is created for each query. Whereas the second way uses bind variables, then after the first query, the execution plan for this type of query will exist in the shared pool, i.e., soft parsing.</p>
<p>So we can reduce the hard parsing and Oracle’s parsing workload by using bind variables. However, there are disadvantages to this approach, using dynamic SQL will lead to different SQL execution efficiency and also SQL optimization will be difficult because of different parameters.</p>
<h2 id="How-SQL-is-executed-in-MySQL"><a href="#How-SQL-is-executed-in-MySQL" class="headerlink" title="How SQL is executed in MySQL"></a>How SQL is executed in MySQL</h2><p><img src="http://56ssc.cc/2020/09/27/Thinking about SQL in a database way/573.jpg" alt></p>
<p>Oracle uses a shared pool to determine if the SQL statement has a cache and execution plan, and we can tell whether we should use hard or soft parsing from this step. So how does SQL get executed in MySQL?</p>
<p>First of all, MySQL is a typical C/S architecture, i.e. Client/Server architecture, and server-side programs use mysqld.</p>
<p>MySQL consists of three layers.</p>
<ol>
<li>Connection layer: client and server side establish connection, client sends SQL to server side.</li>
<li>SQL layer: query processing of SQL statements.</li>
<li>Storage engine layer: deals with database files and is responsible for storing and reading data.</li>
</ol>
<p>The flow of SQL statements in MySQL is: SQL statement → cache query → parser → optimizer → executor. In part, MySQL and Oracle execute SQL in the same way.</p>
<ol>
<li>Query Cache: If the Server finds this SQL statement in the query cache, it will return the result directly to the client; if not, it will go to the parser stage. It should be noted that this feature has been abandoned after MySQL 8.0 because the query cache is often inefficient.</li>
<li>Parser: The parser performs syntactic analysis and semantic analysis on SQL statements.</li>
<li>Optimizer: In the optimizer, the execution path of SQL statements is determined, such as whether to search according to the whole table or index.</li>
<li>Executor: Determine whether the user has privileges before execution, and if so, execute the SQL query and return the result. In MySQL versions below 8.0, if the query cache is set, the query result will be cached.</li>
</ol>
<p>Unlike Oracle, MySQL’s storage engines are in the form of plug-ins, each of which is geared towards a specific database application environment. The open source MySQL also allows developers to set up their own storage engines, and the following are some common ones.</p>
<ol>
<li>InnoDB Storage Engine: It is the default storage engine after MySQL 5.5 version, the biggest feature is that it supports transaction, row-level locking, foreign key constraint, and so on.</li>
<li>MyISAM Storage Engine: It is the default storage engine before MySQL 5.5, and does not support transactions or foreign keys.</li>
<li>Memory storage engine: It uses system memory as the storage medium to get faster response speed. However, if mysqld process crashes, all the data will be lost, so we only use the Memory storage engine when the data is temporary.</li>
<li>NDB Storage Engine: Also called NDB Cluster Storage Engine, mainly used for MySQL Cluster distributed cluster environment, similar to Oracle’s RAC cluster.</li>
<li>Archive storage engine: It has a good compression mechanism for archiving files and compresses them when a write request is made, so it is also often used as a repository.</li>
</ol>
<p>It is important to note that the design of the database lies in the design of the tables, and each table in MySQL can be designed with a different storage engine, and we can choose the storage engine based on the actual data processing needs, which is what makes MySQL so powerful.</p>
<h2 id="A-database-management-system-is-also-a-type-of-software"><a href="#A-database-management-system-is-also-a-type-of-software" class="headerlink" title="A database management system is also a type of software"></a>A database management system is also a type of software</h2><p>Since a single SQL statement goes through different modules, let’s look at the resources (time) used for SQL execution in the different modules. I’ll show you how to analyze the execution time of a SQL statement in MySQL.</p>
<p>First we need to see if profiling is enabled, enabling it allows MySQL to collect information about the resources used during SQL execution, with the following command.</p>
<p><code>mysql&gt; select @@profiling;</code></p>
<p><img src="http://56ssc.cc/2020/09/27/Thinking about SQL in a database way/11.jpg" alt></p>
<p><code>profiling=0</code> means off, we need to turn profiling on, i.e. set to 1.</p>
<p><code>mysql&gt; set profiling=1;</code></p>
<p>Then we execute a SQL query</p>
<p><code>mysql&gt; select * from wucai.heros;</code></p>
<p>View all profiles generated by the current session.</p>
<p><img src="http://56ssc.cc/2020/09/27/Thinking about SQL in a database way/22.jpg" alt></p>
<p>You will notice that we have just executed two queries with Query IDs of 1 and 2. If we want to get the execution time of the last query, we can use.</p>
<p><code>mysql&gt; show profile;</code></p>
<p><img src="http://56ssc.cc/2020/09/27/Thinking about SQL in a database way/33.jpg" alt></p>
<p>Of course you can also query for a specific Query ID, such as.</p>
<p><code>mysql&gt; show profile for query 2;</code></p>
<p>The result of querying SQL execution time is the same as above.</p>
<p>After version 8.0, MySQL no longer supports cached queries for the reasons I mentioned above. The cache is emptied as soon as a table is updated, so it’s only worth using cached queries if the table is static, or if the table has changed very little, otherwise it will increase the query time for SQL if the table is updated frequently.</p>
<p>You can use select version() to see the version of MySQL.</p>
<p><img src="http://56ssc.cc/2020/09/27/Thinking about SQL in a database way/44.jpg" alt></p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>We tend to see the forest for the trees when working with SQL and don’t notice how it performs in the various database software, but today we’re going to understand this from a full picture perspective. You can see the similarities and differences between the different RDBMSs.</p>
<p>The similarity is that both Oracle and MySQL execute SQL through the process of Parser→Optimizer→Executor.</p>
<p>Oracle proposes the concept of shared pool to determine whether to do soft parsing or hard parsing. In MySQL, version 8.0 onwards no longer supports query caching, but directly executes the process of parser→optimizer→executor, which can also be seen in the show profile of MySQL. In addition, MySQL provides a variety of storage engines to choose from, and different storage engines have their own usage scenarios.</p>
]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding SQL</title>
    <url>/2020/09/13/Understanding%20SQL/</url>
    <content><![CDATA[<p>In our daily work, we use database management systems like MySQL and Oracle, which actually follow the SQL language, which means that we deal with these databases through the SQL language. So for people who are in the programming or Internet industry, the most powerful language in the middle is SQL. Ever since SQL joined the TIOBE programming language rankings, it has remained in the top 10.</p>
<p>Arguably the most important and versatile meta-foundation in the entire digital world is data, and the language that works directly with data is SQL. Many people overlook the importance of the SQL language, thinking that it’s just the SELECT statement and that mastering it should be the business of the data analyst. In fact, you shouldn’t underestimate the role of SQL in practice. Many business processes on the Internet today cannot be done without SQL, because they all deal with data.</p>
<p>What about the ubiquity of SQL in all kinds of technologies and businesses, and its ubiquity 45 years ago, in 1974, when IBM researchers published a paper unveiling database technology, SEQUEL: A Structured English Query Language, a structured query language that hasn’t changed much to this day, with SQL’s half-life compared to other languages! That’s a very long time to say the least.</p>
<p>SQL has two important standards, SQL92 and SQL99, which represent the SQL standards that were released in ‘92 and ‘99 respectively, and the SQL language we use today still follows these standards. You know that ‘92 was when Windows 3.1 was released, and how many people still remember it today, but if you do data analysis, or work with data, you still use the SQL language.</p>
<p>As people in the technology and internet industries, we are always looking for a language that is versatile, relatively few changes, and relatively easy to learn, and SQL is one of the few languages that meets all three criteria.</p>
<p>SQL is so powerful, so will it be hard to learn? Not at all.SQL doesn’t require as much programming language base to learn as other languages do!</p>
<p>The SQL language can be divided into the following four parts by function.</p>
<ol>
<li><p>DDL, the Data Definition Language, is used to define our database objects, including databases, tables and columns. By using DDL, we can create, delete and modify databases and table structures.</p>
</li>
<li><p>DML, Data Manipulation Language, which we use to manipulate database related records, such as adding, deleting, and modifying records in data tables.</p>
</li>
<li><p>DCL, Data Control Language, which we use to define access rights and security levels.</p>
</li>
<li><p>DQL, the Data Query Language, which we use to query for the desired records, is the most important aspect of the SQL language. In real business, we deal with queries most of the time, so learning how to write correct and efficient queries is a key learning point.</p>
</li>
</ol>
<p>SQL is one of the few declarative languages, and the thing about this language is that you just need to tell the computer what kind of results you want from the raw data. For example, if I want to find heroes whose main character position is a warrior, as well as their hero name and maximum lifespan, I can enter the following language.</p>
<p><code>SELECT name, hp_max FROM heros WHERE role_main = &#39;hero&#39;</code></p>
<p>Here I’ve defined the heros table, which includes the fields name, hp_max, role_main, and so on.</p>
<p>As you can see from this code, I’m not telling the computer what to do to get the result, which is one of the greatest conveniences of declarative languages. We don’t need to specify specific execution steps, such as which step to execute first, which step to execute second, whether to check that condition A is met before executing, and other such conventional programming thinking.</p>
<p>Isn’t it great that the SQL language defines our requirements and a different DBMS extracts the desired results for us according to the specified SQL!</p>
<p>SQL is the language in which we communicate with the DBMS and we need to design the DBMS before we can create it, which in the case of the RDBMS is in the form of an ER diagram, or entity-relationship diagram.</p>
<p>After the ER diagram review is approved, we create the data table with SQL statements or visual management tools.</p>
<p>What is the use of Entity - Relationship Diagram? It is the conceptual model we use to describe the real world, and in this model there are 3 elements: entities, attributes, and relationships.</p>
<p>Entities are the objects we want to manage, attributes are the properties that identify each entity, and relationships are the relationships between objects.</p>
<p>Once we’ve created the table, we’re ready to operate with SQL. You can see that a lot of SQL statements are not uniformly case sensitive, and while the case does not affect the execution of SQL, I would recommend that you use a uniform writing convention, as good code conventions are the key to efficiency.</p>
<p>On the subject of SQL case-by-case, I have summarized the following two points.</p>
<ol>
<li>Table names, table aliases, field names, field aliases, etc. are all lowercase.</li>
<li>SQL Reserved words, function names, bind variables, etc. are all capitalized.</li>
</ol>
<p>For example, the following SQL statement.</p>
<p><code>SELECT name, hp_max FROM heros WHERE role_main = &#39;hero&#39;</code></p>
<p>You can see that SELECT, FROM and WHERE are all uppercase for common SQL reservations, while name, hp_max and role_main are all lowercase for table names. It is also recommended to use underscores for the field names in the table, such as role_main.</p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>Today I’ve taken you through a preliminary understanding of the SQL language, but of course, no matter how simple SQL is, you still need to do it step by step, starting from a little bit, first master the basic DDL, DML, DCL and DQL syntax, then understand the differences in the SQL syntax of different DBMS, and then look at how to optimize and improve the efficiency of SQL. To write high performance SQL, you first need to understand how it works, followed by a lot of practice.</p>
]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Using Redis for multi-user ticketing problems</title>
    <url>/2020/11/07/Using%20Redis%20for%20multi-user%20ticketing%20problems/</url>
    <content><![CDATA[<h2 id="Redis-Transaction-Processing-Mechanisms"><a href="#Redis-Transaction-Processing-Mechanisms" class="headerlink" title="Redis Transaction Processing Mechanisms"></a>Redis Transaction Processing Mechanisms</h2><p>There are four ACIDs that transactions in an RDBMS satisfy, representing atomicity, consistency, isolation, and persistence.</p>
<p>There are a few differences between Redis transactions and RDBMS transactions.</p>
<p>First of all, Redis does not support a transaction rollback mechanism, which means that when an error occurs in a transaction, the entire transaction continues to execute until all commands in the transaction queue have been executed. The official Redis documentation explains why Redis does not support transaction rollbacks.</p>
<p>Redis command execution fails only if there is a programming syntax error. This error usually occurs in development environments, but rarely in production environments, and there is no need to develop a transaction rollback feature.</p>
<p>In addition, Redis is an in-memory database and, unlike file-based RDBMSs, typically only performs in-memory calculations and operations and does not guarantee persistence. However, Redis also provides two persistence modes, the RDB and AOF modes.</p>
<p>RDB persistence generates a snapshot of the current process data and saves it to disk, and RDB persistence can be triggered manually or automatically. Because the persistence operation is not synchronized with the command operation, the persistence of the transaction is not guaranteed.</p>
<p>AOF persistence uses logging of each write operation, which makes up for the lack of data consistency in RDB, but using AOF mode means that each executed command needs to be written to a file, which will greatly reduce Redis access performance. There are three different ways to enable AOF mode, and the default is everysec, which is synchronized once per second. This is followed by always and no modes, which mean that data is written to the AOF file whenever it is modified, and the operating system decides when it is logged to the AOF file.</p>
<p>Although Redis provides two persistence mechanisms, persistence is not its strong suit as an in-memory database.</p>
<p>Redis is a single-threaded program that does not interrupt transactions while they are executing, and various operations submitted by other clients cannot be performed, so you can understand that Redis transaction processing is serialized in a way that is always isolated.</p>
<h2 id="Transactional-Commands-for-Redis"><a href="#Transactional-Commands-for-Redis" class="headerlink" title="Transactional Commands for Redis"></a>Transactional Commands for Redis</h2><p>Once we understand Redis’ transaction processing mechanism, let’s look at what commands are included in Redis’ transactions.</p>
<p>MULTI: Open a transaction.<br>EXEC: transaction execution, which will execute all commands within the transaction at once.<br>DISCARD: Cancellation of a transaction.<br>WATCH: monitors one or more keys and also interrupts a transaction if a key is changed before the transaction is executed.<br>UNWATCH: cancels the WATCH command’s monitoring of all keys.<br>It should be noted that Redis implements transactions based on the COMMAND queue. If Redis does not open a transaction, any COMMAND will be executed immediately and return the result. If Redis opens a transaction, the COMMAND command is placed in the queue and returns the queued state QUEUED, and commands in the COMMAND queue are executed only when EXEC is called.</p>
<p>For example, we use a transaction to store information about the heroes selected by 5 players, with the following code.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">MULTI</span><br><span class="line">hmset user:001 hero 'zhangfei' hp_max 8341 mp_max 100</span><br><span class="line">hmset user:002 hero 'guanyu' hp_max 7107 mp_max 10</span><br><span class="line">hmset user:003 hero 'liubei' hp_max 6900 mp_max 1742</span><br><span class="line">hmset user:004 hero 'dianwei' hp_max 7516 mp_max 1774</span><br><span class="line">hmset user:005 hero 'diaochan' hp_max 5611 mp_max 1960</span><br><span class="line">EXEC</span><br></pre></td></tr></table></figure>

<p>You can see that all COMMAND commands between MULTI and EXEC are placed in the COMMAND queue and returned to the queued state, only to be executed all at once when called by EXEC.</p>
<p><img src="http://56ssc.cc/2020/11/07/Using Redis for multi-user ticketing problems/1.jpg" alt></p>
<p>We often use Redis’ WATCH and MULTI commands to handle concurrent operations on shared resources, such as spikes, ticket grabbing, etc. What WATCH+MULTI actually implements is optimistic locking. Let’s use two Redis clients to simulate the ticket grabbing process.</p>
<p><img src="http://56ssc.cc/2020/11/07/Using Redis for multi-user ticketing problems/2.jpg" alt></p>
<p>We start Redis client 1, execute the above statement, and then wait for client 2 to complete the above execution before executing EXEC, with the following result for client 2.<br><img src="http://56ssc.cc/2020/11/07/Using Redis for multi-user ticketing problems/3.jpg" alt><br>Client 1 then executes EXEC with the following result.</p>
<p><img src="http://56ssc.cc/2020/11/07/Using Redis for multi-user ticketing problems/4.jpg" alt></p>
<p>You can see that the last ticket was actually grabbed by client 2, because the variable for client 1WATCH’s ticket changed before EXEC, and the entire transaction was interrupted, returning a null reply (nil).</p>
<p>It should be noted that the WATCH command can no longer be executed after MULTI, otherwise the WATCH inside MULTI is not allowed error is returned (because WATCH stands for observe the variable to see if it has changed before executing the transaction, and does not interrupt the transaction if the variable has changed, so the transaction is not interrupted before the transaction is also executed). (that is, before MULTI, using WATCH). Also, if there is a syntax error during the execution of a command, Redis will report the error and the entire transaction will not be executed, and Redis will ignore the error at runtime without affecting the execution later.</p>
<h2 id="Multi-user-ticket-grabbing-simulation"><a href="#Multi-user-ticket-grabbing-simulation" class="headerlink" title="Multi-user ticket grabbing simulation"></a>Multi-user ticket grabbing simulation</h2><p>We’ve just explained the Redis transaction command and simulated the process of two users grabbing tickets using the Redis client. Let’s continue the simulation in Python, with three caveats.</p>
<p>In Python, Redis transactions are wrapped in pipeline, so after you create a Redis connection, you need to get the pipeline, and then use the WATCH, MULTI, and EXEC commands through the pipeline.</p>
<p>Secondly, the user is concurrent, so you need to use Python’s multithreading, which you create using the threading library.</p>
<p>For the user’s ticket grab, you set up the sell function to simulate the user i’s ticket grab. Before executing MULTI, you need to use pipe.watch(KEY) to monitor the number of tickets, if the number of tickets is not greater than 0, it means the tickets are sold out and the user has failed to grab a ticket; if the number of tickets is greater than 0, it means the ticket can be grabbed, then execute MULTI again, subtract the number of tickets by 1 and commit. If the number of tickets is greater than 0, it proves that we can snatch the tickets, and then execute MULTI again to subtract the number of tickets by 1 and submit. However, the transaction may fail when it is submitted because an exception will be generated if the monitored KEY is changed. If the transaction is executed successfully, the user is prompted for a successful ticket grab and the current number of remaining tickets is displayed.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line">pool = redis.ConnectionPool(host = <span class="string">'127.0.0.1'</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>)</span><br><span class="line">r = redis.StrictRedis(connection_pool = pool)</span><br><span class="line"> </span><br><span class="line">KEY=<span class="string">"ticket_count"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sell</span><span class="params">(i)</span>:</span></span><br><span class="line">    pipe = r.pipeline()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pipe.watch(KEY)</span><br><span class="line">            c = int(pipe.get(KEY))      </span><br><span class="line">            <span class="keyword">if</span> c &gt; <span class="number">0</span>:</span><br><span class="line">                pipe.multi()            </span><br><span class="line">                c = c - <span class="number">1</span></span><br><span class="line">                pipe.set(KEY, c)        </span><br><span class="line">                pipe.execute()</span><br><span class="line">                print(<span class="string">'User &#123;&#125; Successful snatching, current number of votes &#123;&#125;'</span>.format(i, c))</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'User &#123;&#125; Failed to grab a ticket, tickets are sold out!'</span>.format(i))</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">'User &#123;&#125; failed to grab a ticket, try again'</span>.format(i))</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            pipe.unwatch()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    r.set(KEY, <span class="number">5</span>)  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        t = threading.Thread(target=sell, args=(i,))</span><br><span class="line">        t.start()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">User 0 Successful snatching, current number of votes 4</span><br><span class="line">User 4 failed to grab a ticket, try again.</span><br><span class="line">User 1 has successfully snatched tickets, current number of tickets 3</span><br><span class="line">User 2 successfully snatched tickets, current number of tickets 2</span><br><span class="line">User 4 failed to grab a ticket, try again.</span><br><span class="line">User 5 failed to grab a ticket, try again.</span><br><span class="line">User 6 successfully snatched tickets, current number of tickets 1</span><br><span class="line">User 4 snatched the ticket successfully, current number of votes 0</span><br><span class="line">User 5 failed to grab a ticket, try again.</span><br><span class="line">User 3 failed to grab a ticket, try again.</span><br><span class="line">User 7 Failed to grab a ticket, tickets are sold out!</span><br><span class="line">User 5 failed to snag a ticket, tickets are sold out!</span><br><span class="line">User 3 failed to grab a ticket, the ticket is sold out!</span><br></pre></td></tr></table></figure>

<p>There is no pessimistic locking in Redis, transaction processing should take into account concurrent requests, we need to implement optimistic locking by WATCH+MULTI, if the monitored KEY does not change then the transaction can be executed smoothly, otherwise it means that the security of the transaction has been compromised, the server will give up on executing the transaction and directly return a null reply (nil) to the client, after the transaction execution fails, we can try again.</p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>A Redis transaction is a collection of Redis commands, and all commands in the transaction are executed in order and without interference from other clients during the execution. However, during the execution of a transaction, Redis may encounter the following two types of errors.</p>
<p>The first is a syntax error, which is the one that occurs when a Redis command is queued.Redis does not allow syntax errors before transaction execution, and if they occur, they will cause transaction execution to fail. As the official documentation says, this usually happens very rarely in production environments, usually in development environments, and if this syntax error is encountered, it is up to the developer to correct the error.</p>
<p>The second is an execution time error, which is an error that occurs during transaction execution, such as handling the wrong type of key, etc. This error is not a syntax error and Redis can only determine it during actual execution. However, Redis does not provide a rollback mechanism, so when this type of error occurs, Redis will continue to execute to ensure that other commands are executed properly.</p>
<p>In transaction processing, we need a locking mechanism to resolve the situation of concurrent access to shared resources. The WATCH+MULTI approach to optimistic locking is provided in Redis. We have previously understood that optimistic locking is the idea that a locking mechanism is implemented by a program to determine when data updates are made, execute if it succeeds, fail if it doesn’t, and there is no need to wait for another transaction to release the lock. In fact, this optimistic, simple design philosophy is reflected everywhere in the design of Redis.</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Webpack5</title>
    <url>/2020/06/01/Webpack5/</url>
    <content><![CDATA[<p>Webpack5 is expected to be released in early 2020 and has been in the limelight since the alpha version, but this update focuses on long term caching, tree shakking and es6 packaging.</p>
<p>Webpack is the hottest module packaging tool in modern front-end development, only through a simple configuration, you can complete the module loading and packaging. How does it do that by configuring some plug-ins, you can easily achieve the construction of the code?</p>
<p>This article won’t discuss the content to be updated in webpack5, I believe most front-end students will just have a simple configuration for webpack, and there are good packages for webpack like vue-cli, umi, etc., but it’s not good for us. Especially when you want to do some personalized customization for business scenarios. In this article, we will take you step by step to build the ultimate front-end development environment from the existing major version of webpack, webpack4.</p>
<h1 id="Several-ways-to-install-webpack"><a href="#Several-ways-to-install-webpack" class="headerlink" title="Several ways to install webpack"></a>Several ways to install webpack</h1><ul>
<li>global: run the <code>webpack index.js</code></li>
<li>local: run <code>npx webpack index.js</code></li>
</ul>
<p>Avoid global installation of webpack (scenario where multiple projects are packaged with different versions of webpack), use <code>npx</code>.</p>
<h1 id="Entry"><a href="#Entry" class="headerlink" title="Entry"></a>Entry</h1><h2 id="Single-entry"><a href="#Single-entry" class="headerlink" title="Single entry"></a>Single entry</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// webpack.config.js</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> config = &#123;</span><br><span class="line">  entry: &#123;</span><br><span class="line">    main: <span class="string">"./src/index.js"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h2 id="Multiple-entry"><a href="#Multiple-entry" class="headerlink" title="Multiple entry"></a>Multiple entry</h2><p>// webpack.config.js</p>
<p>const config = {<br>  entry: {<br>    main: “./src/index.js”,<br>    sub: “./src/sub.js”<br>  }<br>};</p>
<h1 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h1><h2 id="Default-configuration"><a href="#Default-configuration" class="headerlink" title="Default configuration"></a>Default configuration</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// webpack.config.js</span></span><br><span class="line"><span class="keyword">const</span> path = <span class="built_in">require</span>(<span class="string">'path'</span>);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> config = &#123;</span><br><span class="line">  output: &#123;</span><br><span class="line">    filename: <span class="string">'bundle.js'</span>,</span><br><span class="line">    path: path.resolve(__dirname, <span class="string">'dist'</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">module</span>.exports = config;</span><br></pre></td></tr></table></figure>

<h2 id="Multiple-entry-1"><a href="#Multiple-entry-1" class="headerlink" title="Multiple entry"></a>Multiple entry</h2><p>If the configuration creates multiple individual “chunks” (e.g., using multiple entry points or using a plugin like CommonsChunkPlugin), placeholders (substitutions) should be used to ensure that each file has a unique name.</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// webpack.config.js</span></span><br><span class="line"><span class="keyword">const</span> path = <span class="built_in">require</span>(<span class="string">'path'</span>);</span><br><span class="line">&#123;</span><br><span class="line">  entry: &#123;</span><br><span class="line">    main: <span class="string">'./src/index.js'</span>,</span><br><span class="line">    sub: <span class="string">'./src/sub.js'</span></span><br><span class="line">  &#125;,</span><br><span class="line">  output: &#123;</span><br><span class="line">    filename: <span class="string">'[name].js'</span>,</span><br><span class="line">    path: path.resolve(__dirname, <span class="string">'dist'</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Write to hard disks：./dist/main.js, ./dist/sub.js</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Webpack</category>
      </categories>
      <tags>
        <tag>Webpack</tag>
      </tags>
  </entry>
  <entry>
    <title>What to look for when creating databases &amp; tables</title>
    <url>/2020/10/10/What%20to%20look%20for%20when%20creating%20databases%20&amp;%20tables/</url>
    <content><![CDATA[<p>DDL is a core component of DBMS and an important part of SQL, and the correctness and stability of DDL is an important foundation for the entire SQL operation. Different developers may create very different databases and tables for the same requirement, so what are the good principles when designing a database?</p>
<h2 id="Basic-syntax-and-design-tools-of-the-DDL"><a href="#Basic-syntax-and-design-tools-of-the-DDL" class="headerlink" title="Basic syntax and design tools of the DDL"></a>Basic syntax and design tools of the DDL</h2><p>In DDL, we often use the commands CREATE, DROP and ALTER to add, delete and modify, respectively.</p>
<ol>
<li>Definition of the database</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> nba; </span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> nba;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Definition of data tables</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_name</span><br></pre></td></tr></table></figure>

<h3 id="Creating-a-table-structure"><a href="#Creating-a-table-structure" class="headerlink" title="Creating a table structure"></a>Creating a table structure</h3><p>For example, we want to create a player table with the name player and two fields, one is player_id, which is of type int, and the other is player_name, which is of type varchar(255). Neither of these fields is empty, and player_id is incremental.</p>
<p>Then when created it could be written as.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> player  (</span><br><span class="line">  player_id <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  player_name <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>Note that statements are terminated by a semicolon (;) and that there is no comma at the end of the definition of the last field. Data type int(11) represents an integer type with a display length of 11 bits, and the parameter 11 in parentheses represents the maximum valid display length, regardless of the size of the range of values contained in the type. varchar(255) represents a variable string type with a maximum length of 255. not NULL indicates that the entire field cannot be empty, and is a data Constraints.AUTO_INCREMENT stands for automatic growth of the primary key.</p>
<p>In fact, we seldom write DDL statements ourselves and can use some visual tools to create and manipulate databases and tables. I would like to recommend Navicat, which is a database management and design tool that is cross-platform and supports many kinds of database management software, such as MySQL, Oracle, MariaDB, etc. In this case, I would like to recommend Navicat, which is a database management and design tool that is cross-platform and supports many kinds of database management software. </p>
<p>So a player table is designed. The code is as follows.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`player`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`player`</span>  (</span><br><span class="line">  <span class="string">`player_id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`team_id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`player_name`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`height`</span> <span class="built_in">float</span>(<span class="number">3</span>, <span class="number">2</span>) <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="number">0.00</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`player_id`</span>) <span class="keyword">USING</span> BTREE,</span><br><span class="line">  <span class="keyword">UNIQUE</span> <span class="keyword">INDEX</span> <span class="string">`player_name`</span>(<span class="string">`player_name`</span>) <span class="keyword">USING</span> BTREE</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">InnoDB</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> = utf8 <span class="keyword">COLLATE</span> = utf8_general_ci ROW_FORMAT = Dynamic;</span><br></pre></td></tr></table></figure>

<p>You can see the DDL processing throughout the SQL file, first deleting the player table first, then creating the player table, which uses inverted quotes for both the data table and the field names, this is to avoid them having the same names as the MySQL reserved fields.</p>
<p>The player_name field has utf8 as its character set, and the sorting rule is utf8_general_ci, which means that it is not case sensitive, and if it is set to utf8_bin, it means that it is case sensitive, and many other sorting rules are not described here.</p>
<p>Since player_id is set as the primary key, it is specified in the DDL using PRIMARY KEY, and indexed using BTREE.</p>
<p>The difference between UNIQUE INDEX and normal index is that it uses Uniqueness Constraints. You can choose between BTREE or HASH for the indexing method, which is used here. I will explain the difference between BTREE and HASH indexing method later.</p>
<p>The entire data table is stored using InnoDB, which is the default storage engine after MySQL version 5.5, as we have mentioned before. Also, by setting the character set to utf8, the sorting rule to utf8_general_ci, and the row format to Dynamic, we can define the final convention for the data table as follows.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ENGINE = InnoDB CHARACTER <span class="keyword">SET</span> = utf8 <span class="keyword">COLLATE</span> = utf8_general_ci ROW_FORMAT = Dynamic;</span><br></pre></td></tr></table></figure>

<h3 id="Modifying-the-table-structure"><a href="#Modifying-the-table-structure" class="headerlink" title="Modifying the table structure"></a>Modifying the table structure</h3><p>After creating the table structure, we can also modify the table structure, although using Navicat directly will ensure that the re-exported table is up-to-date, it is necessary to understand how to use the DDL command to modify the table structure.</p>
<ol>
<li>Add a field, e.g. I add an age field of type int(11) to the data table</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> player <span class="keyword">ADD</span> (age <span class="built_in">int</span>(<span class="number">11</span>));</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Modify the field name to change the age field to player_age</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> player <span class="keyword">RENAME</span> <span class="keyword">COLUMN</span> age <span class="keyword">to</span> player_age</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>Change the data type of the field, set the data type of player_age to float(3,1).</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> player <span class="keyword">MODIFY</span> (player_age <span class="built_in">float</span>(<span class="number">3</span>,<span class="number">1</span>));</span><br></pre></td></tr></table></figure>
</li>
<li><p>Delete the field, delete the player_age field you just added.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> player <span class="keyword">DROP</span> <span class="keyword">COLUMN</span> player_age;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="Common-constraints-on-data-tables"><a href="#Common-constraints-on-data-tables" class="headerlink" title="Common constraints on data tables"></a>Common constraints on data tables</h3><p>When we create a data table, we also impose constraints on the fields, which are designed to ensure the accuracy and consistency of the data within the RDBMS. Let’s take a look at what the common constraints are.</p>
<p>The first is the primary key constraint.</p>
<p>The primary key is used to uniquely identify a record and cannot be duplicated or null, i.e. UNIQUE+NOT NULL. A primary key can be a single field, or a composite of several fields. In the example above, we set player_id as the primary key.</p>
<p>This is followed by the foreign key constraint.</p>
<p>Foreign keys ensure the integrity of table-to-table references. A foreign key in one table corresponds to the primary key of another table. A foreign key can be duplicate or null. For example, player_id is the primary key in the player table. If you want to set a player score table, i.e. player_score, you can set player_id as a foreign key in player_score, which is associated with the player table.</p>
<p>In addition to constraints on keys, there are also field constraints.</p>
<p>Uniqueness constraint.</p>
<p>A uniqueness constraint indicates that a field has a unique value in the table, even if we already have a primary key, but we can also impose uniqueness constraints on other fields. For example, if we set a uniqueness constraint on player_name in the player table, it indicates that no two players can have the same name. It should be noted that there is a difference between a uniqueness constraint and a NORMAL INDEX. A uniqueness constraint creates a constraint and a normal index to ensure that the fields are correct, while a normal index only speeds up data retrieval and does not constrain the uniqueness of the fields.</p>
<p>NOT NULL Constraint. Defines NOT NULL for a field, which indicates that the field should not be empty and must have a fetch value.</p>
<p>DEFAULT, which indicates a default value for the field. If this field does not have a value when the data is inserted, it is set to the default value. For example, we set the height field to DEFAULT 0.00 by default.</p>
<p>CHECK constraint, used to check the validity of the value range of a specific field, the result of the CHECK constraint can not be FALSE, for example, we can CHECK the height of the value, must be ≥ 0, and &lt;3, that is, CHECK(height&gt;=0 AND height&lt;3).</p>
<h3 id="Principles-for-designing-data-tables"><a href="#Principles-for-designing-data-tables" class="headerlink" title="Principles for designing data tables"></a>Principles for designing data tables</h3><p>When we design data tables, we often consider various questions, such as: what data do users need? What data needs to be stored in a data table? What data is frequently accessed? How can you improve the efficiency of your search?</p>
<p>How do you ensure the correctness of data in a data table, and what kind of constraint checking should be done when inserting, deleting, or updating?</p>
<p>How do you reduce data redundancy in data tables to ensure that they don’t expand rapidly as the number of users grows?</p>
<p>How can we make the database more accessible to those responsible for its maintenance?</p>
<p>In addition to this, the scenarios in which we use databases vary, and it is fair to say that data tables may be designed very differently for different situations.</p>
<ol>
<li>The smaller the number of data tables, the better.</li>
</ol>
<p>The core of the RDBMS is the definition of entities and links, i.e., the E-R diagram. The fewer data tables there are, the more concise the design of the entities and links proves to be, both easy to understand and easy to operate.</p>
<ol start="2">
<li>The smaller the number of fields in the data table, the better.</li>
</ol>
<p>The greater the number of fields, the greater the potential for data redundancy. The premise of setting the number of fields low is that the fields are independent of each other, rather than that the value of one field can be calculated by the other fields. Of course, having a small number of fields is relative, and we usually balance data redundancy with search efficiency.</p>
<ol start="3">
<li>The fewer the number of fields in a data table that have a joint primary key, the better.</li>
</ol>
<p>The primary key is set to determine uniqueness, and when a field cannot be uniquely determined, a joint primary key is used (i.e., multiple fields are used to define a primary key). The more fields in the joint primary key, the more index space it takes up, which not only makes it more difficult to understand, but also increases the running time and index space, so the fewer fields in the joint primary key, the better.</p>
<ol start="4">
<li>The more primary keys and foreign keys you use, the better.</li>
</ol>
<p>The design of a database is really about defining various tables, and the relationships between various fields. The more of these relationships there are, the less redundancy and higher utilization these entities prove to be. This has the advantage of not only ensuring the independence of the data tables from each other, but also increasing the utilization of the correlations between them.</p>
<h2 id="Sum-up"><a href="#Sum-up" class="headerlink" title="Sum up"></a>Sum up</h2><p>Today we learned the basic syntax of DDL, such as how to define a database and database tables.</p>
<p>When creating a data table, apart from defining the field names and data types, the most common constraints we consider are the constraints on the fields.</p>
]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>linear algebra</title>
    <url>/2020/09/07/linear%20algebra/</url>
    <content><![CDATA[<p>Linear algebra is not only the basis of artificial intelligence, but of modern mathematics and the many disciplines that use modern mathematics as their primary method of analysis. From quantum mechanics to image processing, the use of vectors and matrices is essential. Behind vectors and matrices, the core meaning of linear algebra is to provide an abstract view of the world: everything can be abstracted as a combination of certain features and viewed in a static and dynamic way within a framework defined by pre-defined rules.</p>
<p>The most fundamental concept in linear algebra is the notion of a set. In mathematics, a set is defined as a collective of some particular objects aggregated together. The elements in a set will usually have certain commonalities, and thus can be represented by these commonalities. For the set {apples, oranges, pears }, the commonality is that they are all fruits; for the set {cows, horses, sheep }, the commonality is that they are all animals. Of course {apples, cows } can also form a set, but the two elements have no obvious commonality, and the usefulness of such a set in solving real-world problems is quite limited.</p>
<p>In linear algebra, an element consisting of a single number a is called a scalar: a scalar a can be an integer, a real number, or a complex number. If several scalars a1,a2,⋯,an form a sequence in a certain order, such elements are called vectors. Clearly, a vector can be seen as an extension of a scalar. The original number of a number is replaced by a set of numbers, thus bringing an increase in dimensionality, given the subscript that represents the index to uniquely identify the elements in the vector.</p>
<p>Each vector consists of a number of scalars, and if all the scalars of the vector are replaced with vectors of the same specification, the following matrix is obtained:</p>
<p><img src="http://56ssc.cc/2020/09/07/linear algebra/1.jpg" alt></p>
<p>The matrix also represents an increase in dimension relative to the vector, and each element in the matrix needs to be determined using two indexes (instead of one). Similarly, if each scalar element in the matrix is then replaced with a vector, what you get is a tensor. Intuitively, the tensor is the higher-order matrix.</p>
<p>If you think of each cube of a third-order Rubik’s cube as a number, it is a 3x3x3 tensor, and the 3x3 matrix is exactly one face of the cube, which is a slice of the tensor. The tensor is a more complex and less intuitive concept than the vector or matrix.</p>
<p>Vectors and matrices are not only theoretical tools of analysis, but also the basic conditions under which computers work. Humans are able to perceive a continuously changing world, but computers can only process binary information with discrete values, and thus signals from the analog world must be digitized in both the definition and value domains in order to be stored and processed by computers. From this perspective, linear algebra is a tool for representing the real physical world in a virtual digital world.</p>
<p>In computer storage, scalars occupy zero-dimensional arrays; vectors occupy one-dimensional arrays, such as speech signals; matrices occupy two-dimensional arrays, such as grayscale images; and tensors occupy three-dimensional and higher-dimensional arrays, such as RGB images and video.</p>
<p>Describing vectors as mathematical objects requires a specific mathematical language, represented by the norm and the inner product. The norm is a measure of the size of a single vector, describing the properties of the vector itself, which serves to map the vector to a non-negative value. A generic definition of Lp paradigms is as follows.</p>
<p><img src="http://56ssc.cc/2020/09/07/linear algebra/2.jpg" alt></p>
<p>For a given vector, the L1 paradigm calculates the sum of the absolute values of all the elements of the vector, the L2 paradigm calculates the length of the vector in the usual sense, and the L∞ paradigm calculates the value of the largest element in the vector.</p>
<p>The parametric calculates the scale of a single vector, and the inner product calculates the relationship between two vectors. The expression for the inner product of two vectors of the same dimension is</p>
<p><img src="http://56ssc.cc/2020/09/07/linear algebra/3.jpg" alt><br>That is, the summation of the product of the corresponding elements. The inner product can represent the relative position between two vectors, i.e. the angle between the vectors. A special case is where the inner product is 0, i.e., Ps_27E8x,y Pe_27E9=0. In two-dimensional space, this means that the angle between two vectors is 90 degrees, i.e., perpendicular to each other. Whereas, on higher dimensional space, this relationship is called orthogonality. If two vectors are orthogonal, it means that they are linearly independent of each other and have no influence on each other.</p>
<p>In practical problems, the meaning of a vector is not only a combination of some numbers, but also a characteristic of some object or some behavior. The paradigms and inner products can handle the mathematical model of these representations of features and thus extract the implied relationships in the original object or the original behavior.</p>
<p>If there is a set whose elements are vectors of the same dimension (either finite or infinite), and which defines structured operations such as addition and multiplication, such a set is called a linear space, and a linear space which defines inner product operations is called an inner product space. In a linear space, any vector represents a point in an n-dimensional space; conversely, any point in space can be uniquely represented by a vector. The two are equivalent to each other.</p>
<p>A key issue in mapping points and vectors to each other in linear space is the choice of reference system. In real life, given longitude, latitude and altitude, it is possible to uniquely determine any location on the Earth, and thus the three-dimensional vectors (x, y, h) consisting of longitude, latitude and altitude correspond to a point in three-dimensional physical space.</p>
<p>However, in high-dimensional space, where intuition cannot be felt, the definition of a coordinate system is not so intuitive. It should be noted that artificial neural networks usually deal with tens of thousands of features, corresponding to a complex space with tens of thousands of dimensions, and the concept of orthogonal bases is needed.</p>
<p>In the inner product space, a set of two orthogonal vectors form the orthogonal basis of this space, assuming that the L2 paradigms of the base vectors in the orthogonal basis are of unit length 1, this set of orthogonal bases is the standard orthonormal basis. The purpose of orthogonal bases is to define the latitude and longitude of the inner product space. Once the orthonormal basis describing the inner product space is determined, the correspondence between the vectors and the points will be determined as well.</p>
<p>It is important to note that the orthogonal bases describing the inner product space are not unique. For two-dimensional spaces, the plane Cartesian and polar coordinate systems correspond to two different sets of orthogonal bases, and represent two practical ways of describing them.</p>
<p>An important feature of linear space is its ability to carry change. When the standard orthogonal bases used as reference systems are determined, a point in space can be represented as a vector. As the point moves from one location to another, the vector describing it also changes. The change of a point corresponds to a linear transformation of a vector, and the mathematical language for describing object changes or vector transformations is the matrix.</p>
<p>In a linear space, a change can be achieved in two ways: either by a change in the point itself or by a change in the reference system. In the first way, the way to make a point change is to multiply the matrix representing the change by the vector representing the object. But on the other hand, if we keep the point unchanged, but change the angle of observation, we will get a different result, as the saying goes, “There are different heights for different directions.</p>
<p>In this case, the role of the matrix is to transform the orthogonal basis. Therefore, there are different ways of interpreting the multiplication of matrices and vectors.</p>
<p><img src="http://56ssc.cc/2020/09/07/linear algebra/4.jpg" alt></p>
<p>This expression can be interpreted either as a vector x that undergoes the transformation described by matrix A becomes a vector y, or as an object that results in a vector x under the measure of the coordinate system A and a vector y under the measure of the standard coordinate system I (the unit matrix: the main diagonal element is 1 and the remaining elements are 0).</p>
<p>This means that the matrix can describe not only the changes, but also the reference system itself. To use a good analogy from the web: the expression Ax is equivalent to a declaration of the environment for the vector x, and the reference system used to measure it is A. If you want to use another reference system to measure it, you have to redeclare it. A transformation is applied to the coordinate system by multiplying the matrix representing the original coordinate system with the matrix representing the transformation.</p>
<p>An important pair of parameters to describe the matrix are the eigenvalue and the eigenvector. For a given matrix A, assume that its eigenvalue is λ and its eigenvector is x. The relationship between them is as follows.</p>
<p><img src="http://56ssc.cc/2020/09/07/linear algebra/5.jpg" alt></p>
<p>As mentioned earlier, a matrix represents a transformation of a vector, the effect of which is usually to apply both a direction change and a scale change to the original vector. However, for some special vectors, the effect of the matrix is only a change of scale and not a change of direction, i.e., only the effect of telescoping and not the effect of rotation. For a given matrix, such special vectors are the eigenvectors of the matrix, and the scale change coefficient of the eigenvector is the eigenvalue.</p>
<p>The dynamic significance of the matrix eigenvalues and eigenvectors is that they represent the rate and direction of change. If you think of the change represented by the matrix as a running man, then the eigenvalue of the matrix represents the speed at which he runs, and the eigenvector represents the direction in which he runs. But the matrix is not an ordinary person, it is a three-headed, six-armed Nezha, running at different speeds (eigenvalues) and in different directions (eigenvectors), and all the movements of the different bodies are superimposed on the matrix.</p>
<p>The process of solving the eigenvalues and eigenvectors of a given matrix is called eigenvalue decomposition, but the matrix that can be eigenvalue decomposed must be an n-dimensional matrix. Extending the eigenvalue decomposition algorithm over all matrices is the more general singular value decomposition.</p>
<p>Today I share with you the essential linear algebra foundations of AI, focusing on the interpretation of abstract concepts rather than concrete mathematical formulas, the key points of which are as follows.</p>
<p>The essence of linear algebra lies in the abstraction of concrete things into mathematical objects and the description of their static and dynamic properties.<br>The substance of a vector is a stationary point in an n-dimensional linear space.<br>A linear transformation describes a change in a vector or in a coordinate system used as a reference system and can be expressed as a matrix.<br>The eigenvalues and eigenvectors of a matrix describe the rate and direction of change.<br>Linear algebra is to artificial intelligence what addition is to higher mathematics.</p>
]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>linear regression</title>
    <url>/2020/11/11/linear%20regression/</url>
    <content><![CDATA[<p>In mathematical statistics, regression analysis is a method of determining quantitative relationships among multiple variables that are interdependent. Linear regression assumes that the output variable is a linear combination of several input variables and solves for the optimal coefficient in the linear combination based on this relationship. Among the many methods of regression analysis, the linear regression model is the easiest to fit and the statistical properties of its estimated results are easier to determine, thus it is widely used. In machine learning, the regression problem implies the premise that both the input and output variables can be taken continuously, and thus the linear regression model can be used to give estimates of the output for any input.</p>
<p>In 1875, Francis Galton, a British statistician working on genetic problems, was looking for the relationship between parental and offspring height. After analyzing the height data of 1078 father-son pairs, he found that the scatter plot of these data was roughly linear, that is, the height of the father was positively correlated with the height of the son. Behind the positive correlation lies another phenomenon: sons of short fathers are more likely to be taller than their fathers, while sons of tall fathers are more likely to be shorter than their fathers.</p>
<p>Influenced by his cousin Charles Darwin, Galton called this phenomenon the “regression effect,” which means that nature constrains the distribution of human height to a relatively stable and non-polarizing overall level, and gave the first expression for linear regression in history: y = 0.516x + 33.73, in which y and x represent the height in inches of the offspring and the parent, respectively.</p>
<p>Galton’s ideas are still alive and well in today’s machine learning. Assuming that an instance can be represented by a column vector x=(x1;x2;⋯,xn), with each xi representing the instance’s value on the i-th attribute, linear regression works by acquiring a set of parameters wi,i=0,1,⋯,n so that the predicted output can be represented as a linear combination of the instance’s attributes with this set of parameters as weights. If the constant x0=1 is introduced, the model that linear regression attempts to learn is</p>
<p><img src="http://56ssc.cc/2020/11/11/linear regression/1.jpg" alt></p>
<p>When the instance has only one attribute, the relationship between input and output is a straight line on a two-dimensional plane; when the number of attributes of the instance is large, linear regression yields a hyperplane on an n-dimensional space, corresponding to a linear subspace of dimension equal to n - 1.</p>
<p>When determining the coefficient wi on the training set, the error between the predicted output f(x) and the true output y is the central metric of interest. In linear regression, this error is defined in terms of the mean squared error. When the linear regression model is a straight line on a two-dimensional plane, the mean square error is the Euclidean distance between the predicted output and the true output, which is the L2 parametrization of the vector between two points. The solution method of the model with the goal of minimizing the mean square error is the least squares method, which can be written as follows</p>
<p><img src="http://56ssc.cc/2020/11/11/linear regression/2.jpg" alt></p>
<p>where each xk represents a sample in the training set. In the univariate linear regression task, the role of least squares is to find a straight line that minimizes the sum of the European distances from all samples to the line.</p>
<p>At this point, the question arises: why should the parameter that minimizes the mean square error be the optimal model that matches the training samples?</p>
<p>This problem can be elucidated from a probabilistic point of view. Linear regression yields a statistically significant fit, and in the univariate case, it is possible that every sample point does not fall on the resulting line.</p>
<p>One explanation for this phenomenon is that the regression results can perfectly match the distribution of the ideal sample points, but the real sample points used in the training are the result of the superposition of the ideal sample points and the noise, thus creating a deviation from the regression model, and the value of the noise at each sample point is equal to yk - f(xk).</p>
<p>It is assumed that the noise affecting the sample points satisfies a normal distribution with parameter (0,σ2) (remember the probability density formula for a normal distribution?) This means that the probability density is greatest for noise equal to zero, and the larger the magnitude (whether positive or negative), the smaller the probability of noise occurring. In this case, the derivation of the parameter w can be done in a maximum likelihood manner, i.e., finding the assumptions that make the sample data appear with maximum probability, given that the sample data and its distribution are known.</p>
<p>The probability of a single sample xk is actually the probability that the noise is equal to yk - f(xk), while the probability of the simultaneous occurrence of all samples independent of each other is the product of the probabilities of each sample, which can be written as follows</p>
<p><img src="http://56ssc.cc/2020/11/11/linear regression/3.jpg" alt></p>
<p>The task of maximum likelihood estimation is to maximize the values of the above expressions. For computational simplicity, the product of the above expression can be transformed into a summation by taking the logarithm, and the logarithm operation does not affect its monotonicity. After some calculations, the maximization of the above formula is equivalent to the minimization of ∑k(yk-wTxk)2. Isn’t this the result of least squares?</p>
<p>Thus, for univariate linear regression, where the error function obeys a normal distribution, least squares from a geometric sense is equivalent to maximum likelihood estimation from a probabilistic sense.</p>
<p>Having determined the optimality of the least squares method, the next question is how to solve for the minimum value of the mean squared error. In univariate linear regression, the regression equation can be written as y=w1x+w0. According to the optimization theory, substitute this expression into the mean square error expression, and find the derivatives of w1 and w0 respectively, so that the value of both derivatives is equal to 0 is the optimal solution of the linear regression, and its analytical formula can be written as follows</p>
<p><img src="http://56ssc.cc/2020/11/11/linear regression/4.jpg" alt></p>
<p>Univariate linear regression is just the simplest of exceptions. The height of the offspring is not determined solely by the genetic inheritance of the parents; factors such as nutritional conditions and living environment can have an effect. When the description of the sample involves multiple attributes, this type of problem is called multiple linear regression.</p>
<p>The parameters w in multiple linear regression can also be estimated using least squares, and their optimal solutions are also determined using partial derivatives, but the elements involved in the operation change from vectors to matrices. In the ideal case, the optimal parameters of multiple linear regression are</p>
<p><img src="http://56ssc.cc/2020/11/11/linear regression/5.jpg" alt></p>
<p>where X is a matrix of transpositions of all samples x=(x0;x1;x2;⋯,xn) together. However, this expression holds only in the presence of the inverse of the matrix (XTX). In a large number of complex practical tasks, the number of attributes in each sample may even exceed the total number of samples in the training set, where the optimal solution w∗ will not be unique and the choice of solution will depend on the inductive preferences of the learning algorithm.</p>
<p>However, regardless of the selection criteria used, the fact that there are multiple optimal solutions cannot be changed, which implies the generation of overfitting. More importantly, in the case of overfitting, a small perturbation bringing a millisecond difference to the training data may cause the trained model to be false and the stability of the model cannot be guaranteed.</p>
<p>A common approach to solving the overfitting problem is regularization, i.e., adding an additional penalty term. In linear regression, there are two types of regularization methods according to the penalties used, namely “Ridge Regression” and “LASSO Regression”.</p>
<p>In machine learning, Ridge Regression, also known as Parameter Decay, was proposed by Andrei Tikhonov in the 1940’s. Of course, machine learning was not yet in existence at that time. The main purpose of this method was to solve the stability problem of matrix inverse, and its idea was later applied to regularization, which resulted in today’s Ridge Regression.</p>
<p>Ridge regression implements regularization by adding a two-paradigm term to the original mean square error term to be solved, i.e., the minimized object becomes ||yk-wTxk||2+||Γw||2, where Γ is called a Tikhonov matrix and can be reduced to a constant.</p>
<p>From an optimization point of view, the role of the two-paradigm penalty term is to preferentially select w with a smaller paradigm, which is equivalent to adding an extra layer of constraints on the properties of the optimal solution in addition to the minimum mean square error, restricting the optimal solution to a ball in a high-dimensional space. The Ridge regression is equivalent to scaling the original least-squares result, and although the contribution of each parameter in the optimal solution is weakened, the number of parameters does not become smaller.</p>
<p>The full name of LASSO regression is “Least Absolute Shrinkage and Selection Operator” (LASSO), which was proposed by Robert Tibshlani in 1996. In contrast to the Ridge regression, the LASSO regression selects a paradigm term of the parameter to be solved as the penalty term, i.e., the minimization object becomes ||yk-wTxk||2+λ||w|||1, where λ is a constant.</p>
<p>In contrast to the Ridge regression, the LASSO regression is characterized by the introduction of sparsity. It reduces the dimensionality of the optimal solution w, i.e., it weakens the contribution of some of the parameters to zero, which makes the number of elements in w much smaller than the number of original features.</p>
<p>This can be seen more or less as an implementation of Occam’s razor principle: when both primary and secondary contradictions exist, the priority must be the primary contradiction. While factors such as diet, environment, and exercise all influence variation in height, the determining factor is clearly present only in the chromosomes. It is worth mentioning that the introduction of sparsity is a common method for simplifying complex problems and is widely used in other fields such as data compression and signal processing.</p>
<p>From a probabilistic point of view, the analytical solution of least squares can be obtained using normal distribution as well as maximum likelihood estimation, as explained previously. Ridge regression and LASSO regression can also be interpreted from the probabilistic point of view: Ridge regression is estimated with the maximum a posteriori probability if wi satisfies the normal prior distribution; LASSO regression is estimated with the maximum a posteriori probability if wi satisfies the Laplace prior distribution.</p>
<p>However, both the Ridge regression and LASSO regression suppress the overfitting phenomenon by introducing a penalty term, at the expense of increasing the training error and decreasing the test error. Combining the ideas of these two methods can lead to new optimization methods, which I won’t go into here.</p>
<p>Today I have shared with you the basic principles of linear regression, one of the basic machine learning algorithms, the main points of which are as follows.</p>
<p>Linear regression assumes that the output variable is a linear combination of several input variables and solves for the optimal coefficient in the linear combination based on this relationship.<br>The least squares method can be used to solve univariate linear regression problems and is equivalent to maximum likelihood estimation when the error function obeys a normal distribution.<br>Multiple linear regression problems can also be solved by least squares, but are highly susceptible to overfitting.<br>Ridge regression and LASSO regression suppress overfitting by introducing a two-paradigm penalty term and a one-paradigm penalty term, respectively.</p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
</search>
